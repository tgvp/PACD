{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "981f94d4",
   "metadata": {},
   "source": [
    "# Week 3 - Data Wrangling\n",
    "\n",
    "In this notebook we will practice data cleaning and wrangling using a *messy* version of the Titanic dataset.\n",
    "\n",
    "Topics covered:\n",
    "- Inspecting data\n",
    "- Preprocessing\n",
    "    - Categorical x Numerical Data\n",
    "    - Fixing column types\n",
    "    - Standardizing Categorical Values\n",
    "    - Missing values (Identifying and Imputation)\n",
    "- Feature Engineering\n",
    "- Exporting and Loading Cleaned Data\n",
    "\n",
    "### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2d8e456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you any of those\n",
    "#!pip install pandas numpy seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "09c21761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887cd0a9",
   "metadata": {},
   "source": [
    "##  Dataset\n",
    "\n",
    "In this section, we will work with real-world datasets to apply the concepts we have learned so far.\n",
    "\n",
    "Let's load a sample dataset using Pandas.\n",
    "\n",
    "We are going to use the popular `Titanic` dataset, which contains information about passengers on the Titanic, including whether they survived the disaster.\n",
    "\n",
    "**Data dictionary**\n",
    "\n",
    "| Column | Description |\n",
    "|---|---|\n",
    "| PassengerId | Unique ID : T (Titanic) + ticket class + year + port + incremental ID per class.\n",
    "| Survived | 0 = No, 1 = Yes |\n",
    "| Pclass | Ticket class (1, 2, 3) |\n",
    "| Name | Passenger name |\n",
    "| Sex | Gender |\n",
    "| Age | Age in years |\n",
    "| SibSp | Siblings/spouses aboard |\n",
    "| Parch | Parents/children aboard |\n",
    "| Fare | Ticket fare |\n",
    "| Deck | Deck number |\n",
    "| Embarked | Port of embarkation (C/Q/S)|\n",
    "\n",
    "\n",
    "### Load the Titanic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3803f08b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>;survived;pclass;sex;age;sibsp;parch;fare;adult_male;deck;embark_town;passengerId;name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0;0;3;Male;22.0;1;0;7.25;True;;Southampton;T3-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1;1;1;female;38.0;1;0;71.2833;False;C;Cherbour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2;1;3;female;26.0;0;0;7.925;False;;Southampton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3;1;1;female;35.0;1;0;53.1;False;C;Southampton...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4;0;3;Male;35.0;0;0;8.05;True;;Southampton;T3-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>886;0;2;Male;27.0;0;0;13.0;True;;Southampton;T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>887;1;1;female;19.0;0;0;30.0;False;B;Southampt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>888;0;3;female;nan;1;2;23.45;False;;Southampto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>889;1;1;Male;26.0;0;0;;True;C;Cherbourg;T1-191...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>890;0;3;Male;32.0;0;0;7.75;True;;Queenstown;T3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ;survived;pclass;sex;age;sibsp;parch;fare;adult_male;deck;embark_town;passengerId;name\n",
       "0    0;0;3;Male;22.0;1;0;7.25;True;;Southampton;T3-...                                    \n",
       "1    1;1;1;female;38.0;1;0;71.2833;False;C;Cherbour...                                    \n",
       "2    2;1;3;female;26.0;0;0;7.925;False;;Southampton...                                    \n",
       "3    3;1;1;female;35.0;1;0;53.1;False;C;Southampton...                                    \n",
       "4    4;0;3;Male;35.0;0;0;8.05;True;;Southampton;T3-...                                    \n",
       "..                                                 ...                                    \n",
       "886  886;0;2;Male;27.0;0;0;13.0;True;;Southampton;T...                                    \n",
       "887  887;1;1;female;19.0;0;0;30.0;False;B;Southampt...                                    \n",
       "888  888;0;3;female;nan;1;2;23.45;False;;Southampto...                                    \n",
       "889  889;1;1;Male;26.0;0;0;;True;C;Cherbourg;T1-191...                                    \n",
       "890  890;0;3;Male;32.0;0;0;7.75;True;;Queenstown;T3...                                    \n",
       "\n",
       "[891 rows x 1 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('messy_titanic.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a383bc",
   "metadata": {},
   "source": [
    "Before anything, we must load the dataset correctly\n",
    "\n",
    "Manually inspect the dataset file or run the command `!head file_name.csv` or `!cat file_name.csv` - **head** might not work in windows OS ðŸ˜Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6bf27133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ";survived;pclass;sex;age;sibsp;parch;fare;adult_male;deck;embark_town;passengerId;name\n",
      "0;0;3;Male;22.0;1;0;7.25;True;;Southampton;T3-1912-S-P0001;Dustin Flores\n",
      "1;1;1;female;38.0;1;0;71.2833;False;C;Cherbourg;T1-1912-C-P0001;Mary Tyler\n",
      "2;1;3;female;26.0;0;0;7.925;False;;Southampton;T3-1912-S-P0002;Anna Mendoza\n",
      "3;1;1;female;35.0;1;0;53.1;False;C;Southampton;T1-1912-S-P0001;Debra Pearson\n",
      "4;0;3;Male;35.0;0;0;8.05;True;;Southampton;T3-1912-S-P0003;Anthony Taylor\n",
      "5;0;3;Male;nan;0;0;8.4583;True;;Queenstown;T3-1912-Q-P0001;Eric Mayo\n",
      "6;0;1;Male;54.0;0;0;51.8625;True;E;Southampton;T1-1912-S-P0002;Brian James\n",
      "7;0;3;Male;2.0;3;1;21.075 USD;False;;Southampton;T3-1912-S-P0004;Tyler Smith\n",
      "8;1;3;female;27.0;0;2;11.1333 USD;False;;Southampton;T3-1912-S-P0005;Robin Dodson\n",
      "9;1;2;female;14.0;1;0;30.0708;False;;Cherbourg;T2-1912-C-P0001;Sandra Ward\n",
      "10;1;3;female;4.0;1;1;16.7 USD;False;G;Southampton;T3-1912-S-P0006;Kimberly Stevenson DDS\n",
      "11;1;1;female;58.0;0;0;26.55 USD;False;C;Southampton;T1-1912-S-P0003;Michelle Wood\n",
      "12;0;3;Male;20.0;0;0;8.05;True;;Southampton;T3-1912-S-P0007;Russell Carey\n",
      "13;0;3;Male;39.0;1;5;31.275;True;;Southampton;T3-1912-S-P0008;Ryan Parker\n",
      "14;0;3;female;14.0;0;0;7.8542;False;;Southampton;T3-1912-S-P0009;Whitney Wong\n",
      "15;1;2;female;55.0;0;0;16.0;False;;Southampton;T2-1912-S-P0001;Monica Park\n",
      "16;0;3;Male;2.0;4;1;29.125;False;;Queenstown;T3-1912-Q-P0002;Mr. Ian Wolfe\n",
      "17;1;2;Male;;0;0;13.0;True;;Southampton;T2-1912-S-P0002;David Lester\n",
      "18;0;3;female;31.0;1;0;18.0;False;;Southampton;T3-1912-S-P0010;Kimberly Lin\n",
      "19;1;3;female;nan;0;0;7.225;False;;Cherbourg;T3-1912-C-P0001;Wendy Taylor\n",
      "20;0;2;Male;35.0;0;0;;True;;Southampton;T2-1912-S-P0003;Andrew Wright\n",
      "21;1;2;Male;34.0;0;0;13.0;True;D;Southampton;T2-1912-S-P0004;Bruce Young\n",
      "22;1;3;female;15.0;0;0;8.0292;False;;Queenstown;T3-1912-Q-P0003;Maria Stephens\n",
      "23;1;1;Male;28.0;0;0;35.5;True;A;Southampton;T1-1912-S-P0004;Robert Harrell\n",
      "24;0;3;female;8.0;3;1;21.075;False;;Southampton;T3-1912-S-P0011;Wanda Coffey\n",
      "25;1;3;female;38.0;1;5;31.3875;False;;Southampton;T3-1912-S-P0012;Amy Cole\n",
      "26;0;3;Male;nan;0;0;7.225;True;;Cherbourg;T3-1912-C-P0002;Mark Black\n",
      "27;0;1;Male;19.0;3;2;263.0;True;C;Southampton;T1-1912-S-P0005;Nathan Cisneros\n",
      "28;1;3;female;;0;0;7.8792 USD;False;;Queenstown;T3-1912-Q-P0004;Cynthia Flores\n",
      "29;0;3;Male;nan;0;0;7.8958;True;;Southampton;T3-1912-S-P0013;Philip Drake\n",
      "30;0;1;Male;40.0;0;0;27.7208;True;;Cherbourg;T1-1912-C-P0002;Francisco Joseph\n",
      "31;1;1;female;nan;1;0;146.5208;False;B;Cherbourg;T1-1912-C-P0003;Mckenzie Cisneros\n",
      "32;1;3;female;nan;0;0;7.75;False;;Queenstown;T3-1912-Q-P0005;Taylor Smith\n",
      "33;0;2;Male;66.0;0;0;10.5;True;;Southampton;T2-1912-S-P0005;Jonathan Hart\n",
      "34;0;1;Male;28.0;1;0;;True;;Cherbourg;T1-1912-C-P0004;Robert Mendez\n",
      "35;0;1;Male;42.0;1;0;52.0;True;;Southampton;T1-1912-S-P0006;Alan Mills\n",
      "36;1;3;Male;;0;0;;True;;Cherbourg;T3-1912-C-P0003;Christian Henderson\n",
      "37;0;3;Male;21.0;0;0;8.05;True;;Southampton;T3-1912-S-P0014;Paul Shaw\n",
      "38;0;3;female;18.0;2;0;18.0;False;;Southampton;T3-1912-S-P0015;Tracy Harper\n",
      "39;1;3;female;14.0;1;0;11.2417 USD;False;;Cherbourg;T3-1912-C-P0004;Karen Lopez\n",
      "40;0;3;female;40.0;1;0;9.475;False;;Southampton;T3-1912-S-P0016;Kimberly Johnson\n",
      "41;0;2;female;27.0;1;0;;False;;Southampton;T2-1912-S-P0006;Natalie Nichols\n",
      "42;0;3;Male;nan;0;0;7.8958;True;;Cherbourg;T3-1912-C-P0005;Francisco Gilmore\n",
      "43;1;2;female;3.0;1;2;41.5792 USD;False;;Cherbourg;T2-1912-C-P0002;Kathryn Smith\n",
      "44;1;3;female;19.0;0;0;7.8792;False;;Queenstown;T3-1912-Q-P0006;Tamara Castaneda\n",
      "45;0;3;Male;;0;0;8.05 USD;True;;Southampton;T3-1912-S-P0017;Brandon Holt\n",
      "46;0;3;Male;;1;0;15.5;True;;Queenstown;T3-1912-Q-P0007;Frank Richardson\n",
      "47;1;3;female;;0;0;7.75 USD;False;;Queenstown;T3-1912-Q-P0008;Kathryn Perry\n",
      "48;0;3;Male;;2;0;21.6792;True;;Cherbourg;T3-1912-C-P0006;Charles Anderson\n",
      "49;0;3;female;18.0;1;0;17.8;False;;Southampton;T3-1912-S-P0018;Amy Jenkins\n",
      "50;0;3;Male;7.0;4;1;39.6875;False;;Southampton;T3-1912-S-P0019;Lawrence Sherman\n",
      "51;0;3;Male;21.0;0;0;7.8;True;;Southampton;T3-1912-S-P0020;Tony Mathews\n",
      "52;1;1;female;49.0;1;0;76.7292;False;D;Cherbourg;T1-1912-C-P0005;Brittany Parrish\n",
      "53;1;2;female;29.0;1;0;26.0;False;;Southampton;T2-1912-S-P0007;Kimberly Ellis\n",
      "54;0;1;Male;65.0;0;1;61.9792 USD;True;B;Cherbourg;T1-1912-C-P0006;Harry Williams\n",
      "55;1;1;Male;;0;0;35.5 USD;True;C;Southampton;T1-1912-S-P0007;Dale Pope\n",
      "56;1;2;female;21.0;0;0;10.5;False;;Southampton;T2-1912-S-P0008;Regina Ryan\n",
      "57;0;3;Male;28.5;0;0;7.2292;True;;Cherbourg;T3-1912-C-P0007;Shannon Perez\n",
      "58;1;2;female;5.0;1;2;27.75;False;;Southampton;T2-1912-S-P0009;Ann Ramos\n",
      "59;0;3;Male;11.0;5;2;;False;;Southampton;T3-1912-S-P0021;Timothy Mercado\n",
      "60;0;3;Male;22.0;0;0;7.2292;True;;Cherbourg;T3-1912-C-P0008;Aaron Gonzalez\n",
      "61;1;1;female;38.0;0;0;80.0 USD;False;B;;T1-1912-U-P0001;Kelly Smith\n",
      "62;0;1;Male;45.0;1;0;83.475;True;C;Southampton;T1-1912-S-P0008;Jerry Little\n",
      "63;0;3;Male;4.0;3;2;27.9;False;;Southampton;T3-1912-S-P0022;Jeremy Olson\n",
      "64;0;1;Male;nan;0;0;27.7208;True;;Cherbourg;T1-1912-C-P0007;Anthony Gonzalez\n",
      "65;1;3;Male;;1;1;15.2458;True;;Cherbourg;T3-1912-C-P0009;Christopher Decker\n",
      "66;1;2;female;29.0;0;0;10.5 USD;False;F;Southampton;T2-1912-S-P0010;Sherry Hill\n",
      "67;0;3;Male;19.0;0;0;8.1583 USD;True;;Southampton;T3-1912-S-P0023;David Ware\n",
      "68;1;3;female;17.0;4;2;7.925;False;;Southampton;T3-1912-S-P0024;Jessica Copeland\n",
      "69;0;3;Male;26.0;2;0;8.6625 USD;True;;Southampton;T3-1912-S-P0025;Aaron Smith\n",
      "70;0;2;Male;32.0;0;0;10.5;True;;Southampton;T2-1912-S-P0011;Gregory Hernandez\n",
      "71;0;3;female;16.0;5;2;46.9;False;;Southampton;T3-1912-S-P0026;Christina Warner\n",
      "72;0;2;Male;21.0;0;0;73.5;True;;Southampton;T2-1912-S-P0012;Jason Baker\n",
      "73;0;3;Male;26.0;1;0;14.4542;True;;Cherbourg;T3-1912-C-P0010;Anthony Parker\n",
      "74;1;3;Male;32.0;0;0;56.4958;True;;Southampton;T3-1912-S-P0027;John Shannon\n",
      "75;0;3;Male;25.0;0;0;7.65;True;F;Southampton;T3-1912-S-P0028;Clayton Nash\n",
      "76;0;3;Male;;0;0;7.8958;True;;Southampton;T3-1912-S-P0029;Chris Brown\n",
      "77;0;3;Male;nan;0;0;8.05;True;;Southampton;T3-1912-S-P0030;Stephen Ramsey\n",
      "78;1;2;Male;0.83;0;2;29.0;False;;Southampton;T2-1912-S-P0013;Daniel Montoya\n",
      "79;1;3;female;30.0;0;0;12.475;False;;Southampton;T3-1912-S-P0031;Mary Saunders\n",
      "80;0;3;Male;22.0;0;0;9.0;True;;Southampton;T3-1912-S-P0032;Tom Boyle\n",
      "81;1;3;Male;29.0;0;0;9.5;True;;Southampton;T3-1912-S-P0033;Roger Valdez\n",
      "82;1;3;female;;0;0;7.7875;False;;Queenstown;T3-1912-Q-P0009;Kaitlyn Guzman\n",
      "83;0;1;Male;28.0;0;0;47.1 USD;True;;Southampton;T1-1912-S-P0009;Brett Ruiz\n",
      "84;1;2;female;17.0;0;0;10.5;False;;Southampton;T2-1912-S-P0014;Jennifer Rodriguez\n",
      "85;1;3;female;33.0;3;0;15.85;False;;Southampton;T3-1912-S-P0034;Teresa Smith\n",
      "86;0;3;Male;16.0;1;3;34.375 USD;True;;Southampton;T3-1912-S-P0035;Andrew Griffith\n",
      "87;0;3;Male;;0;0;8.05;True;;Southampton;T3-1912-S-P0036;Nathaniel Avila\n",
      "88;1;1;female;23.0;3;2;263.0;False;C;Southampton;T1-1912-S-P0010;Elizabeth Preston\n",
      "89;0;3;Male;24.0;0;0;8.05;True;;Southampton;T3-1912-S-P0037;John Wilson\n",
      "90;0;3;Male;29.0;0;0;8.05;True;;Southampton;T3-1912-S-P0038;Brian Wright\n",
      "91;0;3;Male;20.0;0;0;7.8542 USD;True;;Southampton;T3-1912-S-P0039;Jesse Taylor\n",
      "92;0;1;Male;46.0;1;0;61.175 USD;True;E;Southampton;T1-1912-S-P0011;Douglas Sanders DDS\n",
      "93;0;3;Male;26.0;1;2;20.575;True;;Southampton;T3-1912-S-P0040;Donald Johnson\n",
      "94;0;3;Male;59.0;0;0;7.25;True;;Southampton;T3-1912-S-P0041;Cesar Walters\n",
      "95;0;3;Male;;0;0;8.05;True;;Southampton;T3-1912-S-P0042;William Kennedy\n",
      "96;0;1;Male;71.0;0;0;34.6542;True;A;Cherbourg;T1-1912-C-P0008;Cole Hurley\n",
      "97;1;1;Male;23.0;0;1;63.3583;True;D;Cherbourg;T1-1912-C-P0009;Ryan Robbins\n",
      "98;1;2;female;34.0;0;1;23.0;False;;Southampton;T2-1912-S-P0015;Whitney Adams\n",
      "99;0;2;Male;34.0;1;0;26.0;True;;Southampton;T2-1912-S-P0016;Jeffrey Martinez\n",
      "100;0;3;female;28.0;0;0;7.8958;False;;Southampton;T3-1912-S-P0043;Nicole Tate\n",
      "101;0;3;Male;;0;0;7.8958;True;;Southampton;T3-1912-S-P0044;William Roberts\n",
      "102;0;1;Male;21.0;0;1;77.2875;True;D;Southampton;T1-1912-S-P0012;John Parks\n",
      "103;0;3;Male;33.0;0;0;8.6542;True;;Southampton;T3-1912-S-P0045;Michael Shea\n",
      "104;0;3;Male;37.0;2;0;7.925 USD;True;;Southampton;T3-1912-S-P0046;William Griffin\n",
      "105;0;3;Male;28.0;0;0;;True;;Southampton;T3-1912-S-P0047;Dr. Kevin Turner\n",
      "106;1;3;female;21.0;0;0;7.65 USD;False;;Southampton;T3-1912-S-P0048;Vanessa Ford\n",
      "107;1;3;Male;;0;0;7.775;True;;Southampton;T3-1912-S-P0049;Robert Murphy\n",
      "108;0;3;Male;38.0;0;0;7.8958;True;;Southampton;T3-1912-S-P0050;Matthew Hopkins\n",
      "109;1;3;female;nan;1;0;;False;;Queenstown;T3-1912-Q-P0010;Nichole Nguyen\n",
      "110;0;1;Male;47.0;0;0;52.0;True;C;Southampton;T1-1912-S-P0013;William Reyes\n",
      "111;0;3;female;14.5;1;0;14.4542;False;;Cherbourg;T3-1912-C-P0011;Yvonne Marshall\n",
      "112;0;3;Male;22.0;0;0;8.05;True;;Southampton;T3-1912-S-P0051;Oscar Wong\n",
      "113;0;3;female;20.0;1;0;9.825;False;;Southampton;T3-1912-S-P0052;Sabrina Sosa\n",
      "114;0;3;female;17.0;0;0;14.4583;False;;Cherbourg;T3-1912-C-P0012;Valerie Wright\n",
      "115;0;3;Male;21.0;0;0;7.925;True;;Southampton;T3-1912-S-P0053;Benjamin Marks\n",
      "116;0;3;Male;70.5;0;0;7.75;True;;Queenstown;T3-1912-Q-P0011;George Vincent\n",
      "117;0;2;Male;29.0;1;0;21.0;True;;Southampton;T2-1912-S-P0017;Scott Chase\n",
      "118;0;1;Male;24.0;0;1;247.5208;True;B;Cherbourg;T1-1912-C-P0010;Kenneth Farrell\n",
      "119;0;3;female;2.0;4;2;;False;;Southampton;T3-1912-S-P0054;Jaclyn Williams\n",
      "120;0;2;Male;21.0;2;0;73.5;True;;Southampton;T2-1912-S-P0018;Gregory Edwards\n",
      "121;0;3;Male;;0;0;8.05;True;;Southampton;T3-1912-S-P0055;James Ellis\n",
      "122;0;2;Male;32.5;1;0;30.0708 USD;True;;Cherbourg;T2-1912-C-P0003;Matthew Jones\n",
      "123;1;2;female;32.5;0;0;13.0;False;E;Southampton;T2-1912-S-P0019;Christie Morgan\n",
      "124;0;1;Male;54.0;0;1;77.2875;True;D;Southampton;T1-1912-S-P0014;Robert Soto\n",
      "125;1;3;Male;12.0;1;0;11.2417 USD;False;;Cherbourg;T3-1912-C-P0013;Jason Johnson\n",
      "126;0;3;Male;;0;0;7.75;True;;Queenstown;T3-1912-Q-P0012;Brett Henry\n",
      "127;1;3;Male;24.0;0;0;7.1417;True;;Southampton;T3-1912-S-P0056;Thomas Coleman\n",
      "128;1;3;female;nan;1;1;22.3583 USD;False;F;Cherbourg;T3-1912-C-P0014;Diana Ritter\n",
      "129;0;3;Male;45.0;0;0;;True;;Southampton;T3-1912-S-P0057;Gregory Thomas\n",
      "130;0;3;Male;33.0;0;0;7.8958;True;;Cherbourg;T3-1912-C-P0015;Chad Ross\n",
      "131;0;3;Male;20.0;0;0;7.05;True;;Southampton;T3-1912-S-P0058;Richard Wilson\n",
      "132;0;3;female;47.0;1;0;14.5;False;;Southampton;T3-1912-S-P0059;Sonia Wood\n",
      "133;1;2;female;29.0;1;0;26.0;False;;Southampton;T2-1912-S-P0020;Angela Santos\n",
      "134;0;2;Male;25.0;0;0;13.0;True;;Southampton;T2-1912-S-P0021;Keith Thomas\n",
      "135;0;2;Male;23.0;0;0;15.0458;True;;Cherbourg;T2-1912-C-P0004;Mr. Bryan Pittman\n",
      "136;1;1;female;19.0;0;2;26.2833;False;D;Southampton;T1-1912-S-P0015;Margaret Brown\n",
      "137;0;1;Male;37.0;1;0;53.1;True;C;Southampton;T1-1912-S-P0016;Michael Hart\n",
      "138;0;3;Male;16.0;0;0;9.2167;True;;Southampton;T3-1912-S-P0060;Cody Phillips\n",
      "139;0;1;Male;24.0;0;0;79.2 USD;True;B;Cherbourg;T1-1912-C-P0011;George Bell\n",
      "140;0;3;female;;0;2;15.2458;False;;Cherbourg;T3-1912-C-P0016;Kelsey Torres\n",
      "141;1;3;female;22.0;0;0;7.75;False;;Southampton;T3-1912-S-P0061;Lisa Stephens\n",
      "142;1;3;female;24.0;1;0;15.85 USD;False;;Southampton;T3-1912-S-P0062;Jessica Lewis\n",
      "143;0;3;Male;19.0;0;0;6.75 USD;True;;Queenstown;T3-1912-Q-P0013;Jorge Boyer\n",
      "144;0;2;Male;18.0;0;0;11.5;True;;Southampton;T2-1912-S-P0022;Reginald Garcia\n",
      "145;0;2;Male;19.0;1;1;36.75;True;;Southampton;T2-1912-S-P0023;Marcus Gill\n",
      "146;1;3;Male;27.0;0;0;7.7958;True;;Southampton;T3-1912-S-P0063;Adam Huff\n",
      "147;0;3;female;9.0;2;2;34.375 USD;False;;Southampton;T3-1912-S-P0064;Tracy Rodriguez\n",
      "148;0;2;Male;36.5;0;2;26.0;True;F;Southampton;T2-1912-S-P0024;Nathan Bowman\n",
      "149;0;2;Male;42.0;0;0;13.0;True;;Southampton;T2-1912-S-P0025;Daniel Ross\n",
      "150;0;2;Male;51.0;0;0;12.525;True;;Southampton;T2-1912-S-P0026;Michael Young\n",
      "151;1;1;female;22.0;1;0;;False;C;Southampton;T1-1912-S-P0017;Kristin Patterson\n",
      "152;0;3;Male;55.5;0;0;8.05;True;;Southampton;T3-1912-S-P0065;Gregory Duncan\n",
      "153;0;3;Male;40.5;0;2;14.5;True;;Southampton;T3-1912-S-P0066;Michael Smith\n",
      "154;0;3;Male;;0;0;7.3125;True;;Southampton;T3-1912-S-P0067;William Wilson MD\n",
      "155;0;1;Male;51.0;0;1;61.3792;True;;Cherbourg;T1-1912-C-P0012;Erik Richards\n",
      "156;1;3;female;16.0;0;0;7.7333 USD;False;;Queenstown;T3-1912-Q-P0014;Amanda Woods\n",
      "157;0;3;Male;30.0;0;0;8.05;True;;Southampton;T3-1912-S-P0068;James Cook\n",
      "158;0;3;Male;nan;0;0;8.6625 USD;True;;Southampton;T3-1912-S-P0069;Jimmy Patterson\n",
      "159;0;3;Male;;8;2;69.55;True;;Southampton;T3-1912-S-P0070;Christopher Garcia\n",
      "160;0;3;Male;44.0;0;1;16.1;True;;Southampton;T3-1912-S-P0071;Joshua Neal\n",
      "161;1;2;female;40.0;0;0;15.75;False;;Southampton;T2-1912-S-P0027;Stacey Rocha\n",
      "162;0;3;Male;26.0;0;0;7.775;True;;Southampton;T3-1912-S-P0072;Preston Kennedy\n",
      "163;0;3;Male;17.0;0;0;8.6625;True;;Southampton;T3-1912-S-P0073;Christopher Barajas Jr.\n",
      "164;0;3;Male;1.0;4;1;39.6875;False;;Southampton;T3-1912-S-P0074;Michael Reid\n",
      "165;1;3;Male;9.0;0;2;20.525;False;;Southampton;T3-1912-S-P0075;Travis Yang\n",
      "166;1;1;female;;0;1;55.0;False;E;Southampton;T1-1912-S-P0018;Jasmine Spears\n",
      "167;0;3;female;45.0;1;4;27.9;False;;Southampton;T3-1912-S-P0076;Samantha Howard\n",
      "168;0;1;Male;nan;0;0;25.925;True;;Southampton;T1-1912-S-P0019;Joseph Hayes\n",
      "169;0;3;Male;28.0;0;0;56.4958;True;;Southampton;T3-1912-S-P0077;Dr. Jared Hayden II\n",
      "170;0;1;Male;61.0;0;0;33.5;True;B;Southampton;T1-1912-S-P0020;Michael Henderson\n",
      "171;0;3;Male;4.0;4;1;;False;;Queenstown;T3-1912-Q-P0015;Richard Clark\n",
      "172;1;3;female;1.0;1;1;11.1333;False;;Southampton;T3-1912-S-P0078;Bethany Collins\n",
      "173;0;3;Male;21.0;0;0;7.925;True;;Southampton;T3-1912-S-P0079;Steven Burgess\n",
      "174;0;1;Male;56.0;0;0;30.6958;True;A;Cherbourg;T1-1912-C-P0013;Jeffrey Boone\n",
      "175;0;3;Male;18.0;1;1;;True;;Southampton;T3-1912-S-P0080;Timothy Walker\n",
      "176;0;3;Male;;3;1;25.4667 USD;True;;Southampton;T3-1912-S-P0081;Michael Coffey\n",
      "177;0;1;female;50.0;0;0;28.7125;False;C;Cherbourg;T1-1912-C-P0014;Kimberly Mckinney\n",
      "178;0;2;Male;30.0;0;0;13.0;True;;Southampton;T2-1912-S-P0028;Brian Garcia\n",
      "179;0;3;Male;36.0;0;0;0.0;True;;Southampton;T3-1912-S-P0082;James Marshall\n",
      "180;0;3;female;;8;2;69.55;False;;Southampton;T3-1912-S-P0083;Angelica Floyd\n",
      "181;0;2;Male;nan;0;0;15.05 USD;True;;Cherbourg;T2-1912-C-P0005;Robert Kidd\n",
      "182;0;3;Male;9.0;4;2;31.3875 USD;False;;Southampton;T3-1912-S-P0084;Steven Reynolds\n",
      "183;1;2;Male;1.0;2;1;39.0;False;F;Southampton;T2-1912-S-P0029;Richard Carpenter\n",
      "184;1;3;female;4.0;0;2;22.025;False;;Southampton;T3-1912-S-P0085;Nicole Castro\n",
      "185;0;1;Male;;0;0;50.0;True;A;Southampton;T1-1912-S-P0021;Taylor Yoder\n",
      "186;1;3;female;;1;0;15.5;False;;Queenstown;T3-1912-Q-P0016;Daisy Adams\n",
      "187;1;1;Male;45.0;0;0;26.55;True;;Southampton;T1-1912-S-P0022;Anthony Scott\n",
      "188;0;3;Male;40.0;1;1;15.5;True;;Queenstown;T3-1912-Q-P0017;Jonathon Vasquez\n",
      "189;0;3;Male;36.0;0;0;7.8958;True;;Southampton;T3-1912-S-P0086;Ryan Roberts\n",
      "190;1;2;female;32.0;0;0;13.0 USD;False;;Southampton;T2-1912-S-P0030;Danielle Marshall\n",
      "191;0;2;Male;19.0;0;0;;True;;Southampton;T2-1912-S-P0031;Aaron Cantu\n",
      "192;1;3;female;19.0;1;0;7.8542;False;;Southampton;T3-1912-S-P0087;Lori Alvarez\n",
      "193;1;2;Male;3.0;1;1;26.0;False;F;Southampton;T2-1912-S-P0032;John Flores\n",
      "194;1;1;female;44.0;0;0;27.7208;False;B;Cherbourg;T1-1912-C-P0015;Alexis Aguirre\n",
      "195;1;1;female;58.0;0;0;146.5208;False;B;Cherbourg;T1-1912-C-P0016;Maria York\n",
      "196;0;3;Male;;0;0;7.75 USD;True;;Queenstown;T3-1912-Q-P0018;James Pratt\n",
      "197;0;3;Male;42.0;0;1;8.4042;True;;Southampton;T3-1912-S-P0088;Brian Cline\n",
      "198;1;3;female;;0;0;7.75;False;;Queenstown;T3-1912-Q-P0019;Marisa Lewis\n",
      "199;0;2;female;24.0;0;0;13.0;False;;Southampton;T2-1912-S-P0033;Carrie Brown\n",
      "200;0;3;Male;28.0;0;0;9.5;True;;Southampton;T3-1912-S-P0089;Shane Moore\n",
      "201;0;3;Male;nan;8;2;69.55;True;;Southampton;T3-1912-S-P0090;Harold Taylor\n",
      "202;0;3;Male;34.0;0;0;6.4958;True;;Southampton;T3-1912-S-P0091;James Hernandez\n",
      "203;0;3;Male;45.5;0;0;7.225;True;;Cherbourg;T3-1912-C-P0017;Brian Thomas\n",
      "204;1;3;Male;18.0;0;0;8.05 USD;True;;Southampton;T3-1912-S-P0092;John Randall Jr.\n",
      "205;0;3;female;2.0;0;1;10.4625;False;G;Southampton;T3-1912-S-P0093;Ashley Smith\n",
      "206;0;3;Male;32.0;1;0;15.85 USD;True;;Southampton;T3-1912-S-P0094;David Henry\n",
      "207;1;3;Male;26.0;0;0;18.7875;True;;Cherbourg;T3-1912-C-P0018;Corey Hill\n",
      "208;1;3;female;16.0;0;0;7.75;False;;Queenstown;T3-1912-Q-P0020;Karen Jackson\n",
      "209;1;1;Male;40.0;0;0;31.0 USD;True;A;Cherbourg;T1-1912-C-P0017;Robert May DDS\n",
      "210;0;3;Male;24.0;0;0;7.05;True;;Southampton;T3-1912-S-P0095;Wayne Brown III\n",
      "211;1;2;female;35.0;0;0;21.0;False;;Southampton;T2-1912-S-P0034;Sandy Lozano\n",
      "212;0;3;Male;22.0;0;0;7.25;True;;Southampton;T3-1912-S-P0096;Ivan Tran\n",
      "213;0;2;Male;30.0;0;0;13.0;True;;Southampton;T2-1912-S-P0035;Matthew Hutchinson\n",
      "214;0;3;Male;;1;0;7.75 USD;True;;Queenstown;T3-1912-Q-P0021;Jeffrey Mclaughlin\n",
      "215;1;1;female;31.0;1;0;113.275;False;D;Cherbourg;T1-1912-C-P0018;Donna Nguyen\n",
      "216;1;3;female;27.0;0;0;7.925;False;;Southampton;T3-1912-S-P0097;Emily Lynch\n",
      "217;0;2;Male;42.0;1;0;27.0;True;;Southampton;T2-1912-S-P0036;Justin Powers\n",
      "218;1;1;female;32.0;0;0;;False;D;Cherbourg;T1-1912-C-P0019;Taylor Brooks\n",
      "219;0;2;Male;30.0;0;0;10.5;True;;Southampton;T2-1912-S-P0037;Terry Valdez\n",
      "220;1;3;Male;16.0;0;0;8.05;True;;Southampton;T3-1912-S-P0098;Cory Cunningham\n",
      "221;0;2;Male;27.0;0;0;13.0;True;;Southampton;T2-1912-S-P0038;Alfred Burke\n",
      "222;0;3;Male;51.0;0;0;8.05;True;;Southampton;T3-1912-S-P0099;Michael Smith\n",
      "223;0;3;Male;;0;0;7.8958;True;;Southampton;T3-1912-S-P0100;David Salazar\n",
      "224;1;1;Male;38.0;1;0;90.0;True;C;Southampton;T1-1912-S-P0023;Joseph Delacruz\n",
      "225;0;3;Male;22.0;0;0;9.35 USD;True;;Southampton;T3-1912-S-P0101;Andrew Mckinney\n",
      "226;1;2;Male;19.0;0;0;10.5;True;;Southampton;T2-1912-S-P0039;Luis Moss\n",
      "227;0;3;Male;20.5;0;0;7.25;True;;Southampton;T3-1912-S-P0102;Shawn Stein\n",
      "228;0;2;Male;18.0;0;0;13.0 USD;True;;Southampton;T2-1912-S-P0040;William Nguyen\n",
      "229;0;3;female;;3;1;25.4667;False;;Southampton;T3-1912-S-P0103;Kara Butler\n",
      "230;1;1;female;35.0;1;0;83.475;False;C;Southampton;T1-1912-S-P0024;Teresa Lane\n",
      "231;0;3;Male;29.0;0;0;7.775;True;;Southampton;T3-1912-S-P0104;Edward Whitaker\n",
      "232;0;2;Male;59.0;0;0;13.5;True;;Southampton;T2-1912-S-P0041;Rick Farmer\n",
      "233;1;3;female;5.0;4;2;;False;;Southampton;T3-1912-S-P0105;Erica Gomez\n",
      "234;0;2;Male;24.0;0;0;10.5;True;;Southampton;T2-1912-S-P0042;William Wiley\n",
      "235;0;3;female;;0;0;7.55 USD;False;;Southampton;T3-1912-S-P0106;Nicole White\n",
      "236;0;2;Male;44.0;1;0;26.0;True;;Southampton;T2-1912-S-P0043;Christopher Booth\n",
      "237;1;2;female;8.0;0;2;26.25;False;;Southampton;T2-1912-S-P0044;Ashley Taylor\n",
      "238;0;2;Male;19.0;0;0;10.5;True;;Southampton;T2-1912-S-P0045;Dustin Hicks\n",
      "239;0;2;Male;33.0;0;0;12.275;True;;Southampton;T2-1912-S-P0046;Matthew Garcia\n",
      "240;0;3;female;;1;0;14.4542;False;;Cherbourg;T3-1912-C-P0019;Erin Matthews\n",
      "241;1;3;female;;1;0;15.5 USD;False;;Queenstown;T3-1912-Q-P0022;Brandy Bailey\n",
      "242;0;2;Male;29.0;0;0;10.5;True;;Southampton;T2-1912-S-P0047;Tyler Smith\n",
      "243;0;3;Male;22.0;0;0;7.125;True;;Southampton;T3-1912-S-P0107;Matthew Graham\n",
      "244;0;3;Male;30.0;0;0;7.225 USD;True;;Cherbourg;T3-1912-C-P0020;Richard Green\n",
      "245;0;1;Male;44.0;2;0;90.0;True;C;Queenstown;T1-1912-Q-P0001;Dean Ford\n",
      "246;0;3;female;25.0;0;0;7.775;False;;Southampton;T3-1912-S-P0108;Melinda Taylor\n",
      "247;1;2;female;24.0;0;2;14.5;False;;Southampton;T2-1912-S-P0048;Mrs. Ashley Crawford PhD\n",
      "248;1;1;Male;37.0;1;1;52.5542;True;D;Southampton;T1-1912-S-P0025;Richard Walker\n",
      "249;0;2;Male;54.0;1;0;;True;;Southampton;T2-1912-S-P0049;Michael Baker\n",
      "250;0;3;Male;nan;0;0;7.25;True;;Southampton;T3-1912-S-P0109;William Schwartz\n",
      "251;0;3;female;29.0;1;1;10.4625;False;G;Southampton;T3-1912-S-P0110;Mary Wright\n",
      "252;0;1;Male;62.0;0;0;26.55;True;C;Southampton;T1-1912-S-P0026;Kevin Johnson\n",
      "253;0;3;Male;30.0;1;0;16.1;True;;Southampton;T3-1912-S-P0111;Jorge Lopez\n",
      "254;0;3;female;41.0;0;2;20.2125;False;;Southampton;T3-1912-S-P0112;Kimberly Miller\n",
      "255;1;3;female;29.0;0;2;15.2458;False;;Cherbourg;T3-1912-C-P0021;Stephanie Herrera\n",
      "256;1;1;female;;0;0;79.2;False;;Cherbourg;T1-1912-C-P0020;Amanda Sparks\n",
      "257;1;1;female;30.0;0;0;86.5;False;B;Southampton;T1-1912-S-P0027;Denise Anthony\n",
      "258;1;1;female;35.0;0;0;512.3292;False;;Cherbourg;T1-1912-C-P0021;Sarah Barnes\n",
      "259;1;2;female;50.0;0;1;26.0;False;;Southampton;T2-1912-S-P0050;Paula Morgan\n",
      "260;0;3;Male;;0;0;7.75;True;;Queenstown;T3-1912-Q-P0023;Ricardo Bates\n",
      "261;1;3;Male;3.0;4;2;31.3875;False;;Southampton;T3-1912-S-P0113;Jose Robbins\n",
      "262;0;1;Male;52.0;1;1;79.65;True;E;Southampton;T1-1912-S-P0028;Francisco King\n",
      "263;0;1;Male;40.0;0;0;0.0;True;B;Southampton;T1-1912-S-P0029;Jason Trujillo\n",
      "264;0;3;female;;0;0;7.75;False;;Queenstown;T3-1912-Q-P0024;Joy Ward\n",
      "265;0;2;Male;36.0;0;0;10.5 USD;True;;Southampton;T2-1912-S-P0051;Christopher Gross\n",
      "266;0;3;Male;16.0;4;1;39.6875;True;;Southampton;T3-1912-S-P0114;Richard Thomas\n",
      "267;1;3;Male;25.0;1;0;7.775;True;;Southampton;T3-1912-S-P0115;Mr. Robert Lewis\n",
      "268;1;1;female;58.0;0;1;153.4625 USD;False;C;Southampton;T1-1912-S-P0030;Christina Frederick\n",
      "269;1;1;female;35.0;0;0;;False;C;Southampton;T1-1912-S-P0031;Bethany Gallegos\n",
      "270;0;1;Male;;0;0;31.0 USD;True;;Southampton;T1-1912-S-P0032;Patrick Welch\n",
      "271;1;3;Male;25.0;0;0;0.0;True;;Southampton;T3-1912-S-P0116;Matthew Jones\n",
      "272;1;2;female;41.0;0;1;19.5;False;;Southampton;T2-1912-S-P0052;Tanya Brown\n",
      "273;0;1;Male;37.0;0;1;29.7 USD;True;C;Cherbourg;T1-1912-C-P0022;Thomas Fischer\n",
      "274;1;3;female;;0;0;7.75;False;;Queenstown;T3-1912-Q-P0025;Jessica Reed\n",
      "275;1;1;female;63.0;1;0;77.9583;False;D;Southampton;T1-1912-S-P0033;Amanda Adams\n",
      "276;0;3;female;45.0;0;0;7.75;False;;Southampton;T3-1912-S-P0117;Heather Martin\n",
      "277;0;2;Male;;0;0;0.0 USD;True;;Southampton;T2-1912-S-P0053;Michael Zamora\n",
      "278;0;3;Male;7.0;4;1;29.125;False;;Queenstown;T3-1912-Q-P0026;Gregory Bell\n",
      "279;1;3;female;35.0;1;1;20.25;False;;Southampton;T3-1912-S-P0118;Christy Smith\n",
      "280;0;3;Male;65.0;0;0;7.75;True;;Queenstown;T3-1912-Q-P0027;Donald Wells\n",
      "281;0;3;Male;28.0;0;0;7.8542;True;;Southampton;T3-1912-S-P0119;Dakota Jackson\n",
      "282;0;3;Male;16.0;0;0;9.5;True;;Southampton;T3-1912-S-P0120;Jason Ali\n",
      "283;1;3;Male;19.0;0;0;8.05;True;;Southampton;T3-1912-S-P0121;Thomas Chavez\n",
      "284;0;1;Male;;0;0;26.0;True;A;Southampton;T1-1912-S-P0034;James Smith\n",
      "285;0;3;Male;33.0;0;0;8.6625;True;;Cherbourg;T3-1912-C-P0022;Nathaniel King\n",
      "286;1;3;Male;30.0;0;0;9.5 USD;True;;Southampton;T3-1912-S-P0122;Brandon Moss\n",
      "287;0;3;Male;22.0;0;0;7.8958;True;;Southampton;T3-1912-S-P0123;Jeffrey Garcia\n",
      "288;1;2;Male;42.0;0;0;13.0;True;;Southampton;T2-1912-S-P0054;Brian Anderson\n",
      "289;1;3;female;22.0;0;0;7.75;False;;Queenstown;T3-1912-Q-P0028;Jessica Chapman\n",
      "290;1;1;female;26.0;0;0;78.85;False;;Southampton;T1-1912-S-P0035;Samantha Taylor\n",
      "291;1;1;female;19.0;1;0;91.0792;False;B;Cherbourg;T1-1912-C-P0023;Deborah Dalton\n",
      "292;0;2;Male;36.0;0;0;12.875;True;D;Cherbourg;T2-1912-C-P0006;John Lopez\n",
      "293;0;3;female;24.0;0;0;8.85 USD;False;;Southampton;T3-1912-S-P0124;Lisa White\n",
      "294;0;3;Male;24.0;0;0;7.8958;True;;Southampton;T3-1912-S-P0125;James Wise\n",
      "295;0;1;Male;;0;0;27.7208;True;;Cherbourg;T1-1912-C-P0024;Mark Lawrence\n",
      "296;0;3;Male;23.5;0;0;7.2292 USD;True;;Cherbourg;T3-1912-C-P0023;Nicholas King MD\n",
      "297;0;1;female;2.0;1;2;151.55;False;C;Southampton;T1-1912-S-P0036;Wanda Smith\n",
      "298;1;1;Male;;0;0;30.5;True;C;Southampton;T1-1912-S-P0037;Jason Todd\n",
      "299;1;1;female;50.0;0;1;247.5208 USD;False;B;Cherbourg;T1-1912-C-P0025;Stephanie Kelly\n",
      "300;1;3;female;nan;0;0;7.75;False;;Queenstown;T3-1912-Q-P0029;Shannon Bender\n",
      "301;1;3;Male;;2;0;23.25;True;;Queenstown;T3-1912-Q-P0030;Anthony Blair\n",
      "302;0;3;Male;19.0;0;0;0.0;True;;Southampton;T3-1912-S-P0126;Christopher Jackson\n",
      "303;1;2;female;;0;0;12.35 USD;False;E;Queenstown;T2-1912-Q-P0001;Anna Aguilar\n",
      "304;0;3;Male;;0;0;8.05;True;;Southampton;T3-1912-S-P0127;Matthew Nguyen\n",
      "305;1;1;Male;0.92;1;2;151.55;False;C;Southampton;T1-1912-S-P0038;Randy Rogers\n",
      "306;1;1;female;;0;0;110.8833;False;;Cherbourg;T1-1912-C-P0026;Brenda Miller\n",
      "307;1;1;female;17.0;1;0;108.9;False;C;Cherbourg;T1-1912-C-P0027;Tiffany Mckenzie\n",
      "308;0;2;Male;30.0;1;0;24.0 USD;True;;Cherbourg;T2-1912-C-P0007;Brian Sherman\n",
      "309;1;1;female;30.0;0;0;;False;E;Cherbourg;T1-1912-C-P0028;Cynthia Anderson\n",
      "310;1;1;female;24.0;0;0;83.1583 USD;False;C;Cherbourg;T1-1912-C-P0029;Rachel Duffy\n",
      "311;1;1;female;18.0;2;2;262.375 USD;False;B;Cherbourg;T1-1912-C-P0030;Beth Harrison\n",
      "312;0;2;female;26.0;1;1;26.0;False;;Southampton;T2-1912-S-P0055;Katherine Matthews\n",
      "313;0;3;Male;28.0;0;0;7.8958;True;;Southampton;T3-1912-S-P0128;Danny Baldwin\n",
      "314;0;2;Male;43.0;1;1;26.25;True;;Southampton;T2-1912-S-P0056;Patrick Henderson\n",
      "315;1;3;female;26.0;0;0;7.8542;False;;Southampton;T3-1912-S-P0129;Brittany Hernandez\n",
      "316;1;2;female;24.0;1;0;26.0;False;;Southampton;T2-1912-S-P0057;Kathleen Hoffman\n",
      "317;0;2;Male;54.0;0;0;14.0;True;;Southampton;T2-1912-S-P0058;Alan Rose\n",
      "318;1;1;female;31.0;0;2;164.8667;False;C;Southampton;T1-1912-S-P0039;Laura Brooks\n",
      "319;1;1;female;40.0;1;1;134.5;False;E;Cherbourg;T1-1912-C-P0031;Tiffany Bennett\n",
      "320;0;3;Male;22.0;0;0;7.25;True;;Southampton;T3-1912-S-P0130;Edward Hanson\n",
      "321;0;3;Male;27.0;0;0;7.8958 USD;True;;Southampton;T3-1912-S-P0131;Leonard Johnson\n",
      "322;1;2;female;30.0;0;0;12.35;False;;Queenstown;T2-1912-Q-P0002;Tracy Pittman\n",
      "323;1;2;female;22.0;1;1;29.0;False;;Southampton;T2-1912-S-P0059;Angela Riley\n",
      "324;0;3;Male;;8;2;69.55;True;;Southampton;T3-1912-S-P0132;Daniel Torres\n",
      "325;1;1;female;36.0;0;0;135.6333 USD;False;C;Cherbourg;T1-1912-C-P0032;Linda Johnson\n",
      "326;0;3;Male;61.0;0;0;6.2375;True;;Southampton;T3-1912-S-P0133;Dylan Graham\n",
      "327;1;2;female;36.0;0;0;13.0;False;D;Southampton;T2-1912-S-P0060;Lisa Hunter\n",
      "328;1;3;female;31.0;1;1;20.525;False;;Southampton;T3-1912-S-P0134;Dana Barajas\n",
      "329;1;1;female;16.0;0;1;57.9792;False;B;Cherbourg;T1-1912-C-P0033;Lisa Harper\n",
      "330;1;3;female;;2;0;23.25 USD;False;;Queenstown;T3-1912-Q-P0031;Alexa Acevedo\n",
      "331;0;1;Male;45.5;0;0;28.5;True;C;Southampton;T1-1912-S-P0040;Steve Duarte\n",
      "332;0;1;Male;38.0;0;1;153.4625;True;C;Southampton;T1-1912-S-P0041;Luis Flores\n",
      "333;0;3;Male;16.0;2;0;18.0;True;;Southampton;T3-1912-S-P0135;Casey Williams\n",
      "334;1;1;female;nan;1;0;;False;;Southampton;T1-1912-S-P0042;Natasha Powell\n",
      "335;0;3;Male;nan;0;0;7.8958;True;;Southampton;T3-1912-S-P0136;John Rodriguez MD\n",
      "336;0;1;Male;29.0;1;0;66.6;True;C;Southampton;T1-1912-S-P0043;David Harris\n",
      "337;1;1;female;41.0;0;0;134.5;False;E;Cherbourg;T1-1912-C-P0034;Linda Schneider\n",
      "338;1;3;Male;45.0;0;0;8.05;True;;Southampton;T3-1912-S-P0137;Samuel Grant\n",
      "339;0;1;Male;45.0;0;0;35.5;True;;Southampton;T1-1912-S-P0044;Brandon Wyatt\n",
      "340;1;2;Male;2.0;1;1;26.0;False;F;Southampton;T2-1912-S-P0061;Evan Rios\n",
      "341;1;1;female;24.0;3;2;263.0;False;C;Southampton;T1-1912-S-P0045;Jessica Drake\n",
      "342;0;2;Male;28.0;0;0;13.0 USD;True;;Southampton;T2-1912-S-P0062;Andrew Leach\n",
      "343;0;2;Male;25.0;0;0;13.0;True;;Southampton;T2-1912-S-P0063;Luke Jones\n",
      "344;0;2;Male;36.0;0;0;13.0;True;;Southampton;T2-1912-S-P0064;Steven Brown\n",
      "345;1;2;female;24.0;0;0;;False;F;Southampton;T2-1912-S-P0065;Jordan Johnson\n",
      "346;1;2;female;40.0;0;0;13.0;False;;Southampton;T2-1912-S-P0066;Mary Quinn\n",
      "347;1;3;female;nan;1;0;16.1;False;;Southampton;T3-1912-S-P0138;Maureen Holland\n",
      "348;1;3;Male;3.0;1;1;15.9 USD;False;;Southampton;T3-1912-S-P0139;Sean Jacobson\n",
      "349;0;3;Male;42.0;0;0;8.6625;True;;Southampton;T3-1912-S-P0140;Mr. Joshua Estrada\n",
      "350;0;3;Male;23.0;0;0;9.225;True;;Southampton;T3-1912-S-P0141;Cody Klein\n",
      "351;0;1;Male;;0;0;35.0;True;C;Southampton;T1-1912-S-P0046;Jeffrey West\n",
      "352;0;3;Male;15.0;1;1;;False;;Cherbourg;T3-1912-C-P0024;Corey Weaver\n",
      "353;0;3;Male;25.0;1;0;17.8;True;;Southampton;T3-1912-S-P0142;Julian Turner\n",
      "354;0;3;Male;nan;0;0;7.225;True;;Cherbourg;T3-1912-C-P0025;Michael Thompson\n",
      "355;0;3;Male;28.0;0;0;9.5 USD;True;;Southampton;T3-1912-S-P0143;Eric Carpenter\n",
      "356;1;1;female;22.0;0;1;55.0 USD;False;E;Southampton;T1-1912-S-P0047;Kelly Graves\n",
      "357;0;2;female;38.0;0;0;13.0;False;;Southampton;T2-1912-S-P0067;Breanna King\n",
      "358;1;3;female;;0;0;7.8792;False;;Queenstown;T3-1912-Q-P0032;Lisa Fields\n",
      "359;1;3;female;;0;0;7.8792;False;;Queenstown;T3-1912-Q-P0033;Rebecca Myers\n",
      "360;0;3;Male;40.0;1;4;27.9;True;;Southampton;T3-1912-S-P0144;Dalton Jones\n",
      "361;0;2;Male;29.0;1;0;27.7208 USD;True;;Cherbourg;T2-1912-C-P0008;James Smith\n",
      "362;0;3;female;45.0;0;1;14.4542 USD;False;;Cherbourg;T3-1912-C-P0026;Tina Hodges\n",
      "363;0;3;Male;35.0;0;0;7.05;True;;Southampton;T3-1912-S-P0145;Eric Mitchell\n",
      "364;0;3;Male;;1;0;15.5;True;;Queenstown;T3-1912-Q-P0034;Marcus Freeman\n",
      "365;0;3;Male;30.0;0;0;7.25;True;;Southampton;T3-1912-S-P0146;Nicholas Weaver\n",
      "366;1;1;female;60.0;1;0;75.25;False;D;Cherbourg;T1-1912-C-P0035;Amy Whitaker\n",
      "367;1;3;female;;0;0;7.2292 USD;False;;Cherbourg;T3-1912-C-P0027;Laura Mccullough\n",
      "368;1;3;female;nan;0;0;7.75;False;;Queenstown;T3-1912-Q-P0035;April Meyer\n",
      "369;1;1;female;24.0;0;0;69.3;False;B;Cherbourg;T1-1912-C-P0036;Sarah Martin\n",
      "370;1;1;Male;25.0;1;0;55.4417;True;E;Cherbourg;T1-1912-C-P0037;Jeremy Keller\n",
      "371;0;3;Male;18.0;1;0;6.4958;True;;Southampton;T3-1912-S-P0147;David Thompson\n",
      "372;0;3;Male;19.0;0;0;8.05;True;;Southampton;T3-1912-S-P0148;Scott Nelson\n",
      "373;0;1;Male;22.0;0;0;135.6333 USD;True;;Cherbourg;T1-1912-C-P0038;Daniel Hunt\n",
      "374;0;3;female;3.0;3;1;21.075 USD;False;;Southampton;T3-1912-S-P0149;Linda Garcia\n",
      "375;1;1;female;;1;0;82.1708;False;;Cherbourg;T1-1912-C-P0039;Melissa Pham\n",
      "376;1;3;female;22.0;0;0;;False;;Southampton;T3-1912-S-P0150;Tammy Clark\n",
      "377;0;1;Male;27.0;0;2;211.5;True;C;Cherbourg;T1-1912-C-P0040;Brian Dawson\n",
      "378;0;3;Male;20.0;0;0;4.0125;True;;Cherbourg;T3-1912-C-P0028;Todd Jones\n",
      "379;0;3;Male;19.0;0;0;7.775;True;;Southampton;T3-1912-S-P0151;Daniel Hernandez\n",
      "380;1;1;female;42.0;0;0;227.525;False;;Cherbourg;T1-1912-C-P0041;Courtney Thomas\n",
      "381;1;3;female;1.0;0;2;15.7417;False;;Cherbourg;T3-1912-C-P0029;Susan Dickerson\n",
      "382;0;3;Male;32.0;0;0;7.925 USD;True;;Southampton;T3-1912-S-P0152;Isaac Munoz\n",
      "383;1;1;female;35.0;1;0;52.0;False;;Southampton;T1-1912-S-P0048;Tracy Buchanan\n",
      "384;0;3;Male;;0;0;7.8958 USD;True;;Southampton;T3-1912-S-P0153;Joel Park\n",
      "385;0;2;Male;18.0;0;0;;True;;Southampton;T2-1912-S-P0068;Austin Weber\n",
      "386;0;3;Male;1.0;5;2;46.9;False;;Southampton;T3-1912-S-P0154;Alan Horton\n",
      "387;1;2;female;36.0;0;0;13.0 USD;False;;Southampton;T2-1912-S-P0069;Jordan Martin\n",
      "388;0;3;Male;;0;0;7.7292;True;;Queenstown;T3-1912-Q-P0036;Craig Mckinney\n",
      "389;1;2;female;17.0;0;0;12.0;False;;Cherbourg;T2-1912-C-P0009;Tabitha Adams\n",
      "390;1;1;Male;36.0;1;2;120.0;True;B;Southampton;T1-1912-S-P0049;Ronald Sanchez\n",
      "391;1;3;Male;21.0;0;0;7.7958;True;;Southampton;T3-1912-S-P0155;Derrick Miller\n",
      "392;0;3;Male;28.0;2;0;7.925 USD;True;;Southampton;T3-1912-S-P0156;Robert Combs\n",
      "393;1;1;female;23.0;1;0;113.275;False;D;Cherbourg;T1-1912-C-P0042;Krystal Salinas\n",
      "394;1;3;female;24.0;0;2;16.7;False;G;Southampton;T3-1912-S-P0157;Cheryl Williams\n",
      "395;0;3;Male;22.0;0;0;7.7958 USD;True;;Southampton;T3-1912-S-P0158;Joshua Mccormick\n",
      "396;0;3;female;31.0;0;0;7.8542;False;;Southampton;T3-1912-S-P0159;Amanda Ayala\n",
      "397;0;2;Male;46.0;0;0;26.0 USD;True;;Southampton;T2-1912-S-P0070;Allen Rodriguez\n",
      "398;0;2;Male;23.0;0;0;10.5 USD;True;;Southampton;T2-1912-S-P0071;Daniel Park\n",
      "399;1;2;female;28.0;0;0;12.65;False;;Southampton;T2-1912-S-P0072;Shelly Lopez\n",
      "400;1;3;Male;39.0;0;0;7.925 USD;True;;Southampton;T3-1912-S-P0160;Kyle Merritt\n",
      "401;0;3;Male;26.0;0;0;8.05;True;;Southampton;T3-1912-S-P0161;Cory Parker\n",
      "402;0;3;female;21.0;1;0;9.825;False;;Southampton;T3-1912-S-P0162;Rachel Adams\n",
      "403;0;3;Male;28.0;1;0;15.85;True;;Southampton;T3-1912-S-P0163;Christopher Brown\n",
      "404;0;3;female;20.0;0;0;8.6625;False;;Southampton;T3-1912-S-P0164;Jennifer Wilkins\n",
      "405;0;2;Male;34.0;1;0;;True;;Southampton;T2-1912-S-P0073;Ryan Stewart\n",
      "406;0;3;Male;51.0;0;0;7.75 USD;True;;Southampton;T3-1912-S-P0165;Justin Brown\n",
      "407;1;2;Male;3.0;1;1;18.75;False;;Southampton;T2-1912-S-P0074;Peter Flowers\n",
      "408;0;3;Male;21.0;0;0;7.775;True;;Southampton;T3-1912-S-P0166;Cory Johnson\n",
      "409;0;3;female;nan;3;1;25.4667 USD;False;;Southampton;T3-1912-S-P0167;Kaitlin Castro\n",
      "410;0;3;Male;;0;0;7.8958;True;;Southampton;T3-1912-S-P0168;John Lee\n",
      "411;0;3;Male;;0;0;6.8583 USD;True;;Queenstown;T3-1912-Q-P0037;Ethan Johnston\n",
      "412;1;1;female;33.0;1;0;;False;C;Queenstown;T1-1912-Q-P0002;Kathy Hayes\n",
      "413;0;2;Male;;0;0;0.0;True;;Southampton;T2-1912-S-P0075;Gregory Carlson\n",
      "414;1;3;Male;44.0;0;0;;True;;Southampton;T3-1912-S-P0169;Hunter Lamb\n",
      "415;0;3;female;;0;0;8.05;False;;Southampton;T3-1912-S-P0170;Rebecca Gray\n",
      "416;1;2;female;34.0;1;1;32.5;False;;Southampton;T2-1912-S-P0076;Ms. Emily Haley DDS\n",
      "417;1;2;female;18.0;0;2;13.0;False;;Southampton;T2-1912-S-P0077;Amanda Williams\n",
      "418;0;2;Male;30.0;0;0;;True;;Southampton;T2-1912-S-P0078;Christopher Johnson\n",
      "419;0;3;female;10.0;0;2;24.15 USD;False;;Southampton;T3-1912-S-P0171;Miss Linda Moses DDS\n",
      "420;0;3;Male;;0;0;7.8958;True;;Cherbourg;T3-1912-C-P0030;Timothy Wright\n",
      "421;0;3;Male;21.0;0;0;7.7333;True;;Queenstown;T3-1912-Q-P0038;Bernard Mcmahon\n",
      "422;0;3;Male;29.0;0;0;7.875;True;;Southampton;T3-1912-S-P0172;Eric Marquez\n",
      "423;0;3;female;28.0;1;1;;False;;Southampton;T3-1912-S-P0173;Lisa Turner\n",
      "424;0;3;Male;18.0;1;1;20.2125;True;;Southampton;T3-1912-S-P0174;Dr. Christopher Hernandez Jr.\n",
      "425;0;3;Male;;0;0;7.25;True;;Southampton;T3-1912-S-P0175;Mr. Luis Thompson\n",
      "426;1;2;female;28.0;1;0;;False;;Southampton;T2-1912-S-P0079;Stephanie Turner\n",
      "427;1;2;female;19.0;0;0;26.0 USD;False;;Southampton;T2-1912-S-P0080;Barbara Cohen\n",
      "428;0;3;Male;;0;0;7.75 USD;True;;Queenstown;T3-1912-Q-P0039;Donald Garcia\n",
      "429;1;3;Male;32.0;0;0;8.05;True;E;Southampton;T3-1912-S-P0176;Nathan Grant\n",
      "430;1;1;Male;28.0;0;0;26.55 USD;True;C;Southampton;T1-1912-S-P0050;Donald Sullivan\n",
      "431;1;3;female;;1;0;16.1;False;;Southampton;T3-1912-S-P0177;Barbara Lewis\n",
      "432;1;2;female;42.0;1;0;26.0;False;;Southampton;T2-1912-S-P0081;Sabrina Dudley\n",
      "433;0;3;Male;17.0;0;0;7.125;True;;Southampton;T3-1912-S-P0178;Brent Campbell\n",
      "434;0;1;Male;50.0;1;0;55.9;True;E;Southampton;T1-1912-S-P0051;Jose Mccormick\n",
      "435;1;1;female;14.0;1;2;120.0 USD;False;B;Southampton;T1-1912-S-P0052;April Beasley\n",
      "436;0;3;female;21.0;2;2;34.375;False;;Southampton;T3-1912-S-P0179;Tiffany Cabrera\n",
      "437;1;2;female;24.0;2;3;18.75;False;;Southampton;T2-1912-S-P0082;Wendy Swanson\n",
      "438;0;1;Male;64.0;1;4;;True;C;Southampton;T1-1912-S-P0053;Jacob Clark\n",
      "439;0;2;Male;31.0;0;0;10.5;True;;Southampton;T2-1912-S-P0083;Jeffrey Noble\n",
      "440;1;2;female;45.0;1;1;26.25;False;;Southampton;T2-1912-S-P0084;Brianna Roberts\n",
      "441;0;3;Male;20.0;0;0;;True;;Southampton;T3-1912-S-P0180;Gregory Garcia\n",
      "442;0;3;Male;25.0;1;0;7.775;True;;Southampton;T3-1912-S-P0181;Craig Bradley\n",
      "443;1;2;female;28.0;0;0;13.0;False;;Southampton;T2-1912-S-P0085;Ann Dodson DDS\n",
      "444;1;3;Male;;0;0;8.1125;True;;Southampton;T3-1912-S-P0182;Michael Jones\n",
      "445;1;1;Male;4.0;0;2;81.8583 USD;False;A;Southampton;T1-1912-S-P0054;Caleb Shepherd\n",
      "446;1;2;female;13.0;0;1;19.5;False;;Southampton;T2-1912-S-P0086;Brenda Hester\n",
      "447;1;1;Male;34.0;0;0;26.55 USD;True;;Southampton;T1-1912-S-P0055;Daniel Warner\n",
      "448;1;3;female;5.0;2;1;19.2583 USD;False;;Cherbourg;T3-1912-C-P0031;Emily Doyle\n",
      "449;1;1;Male;52.0;0;0;30.5;True;C;Southampton;T1-1912-S-P0056;Devin Lee\n",
      "450;0;2;Male;36.0;1;2;27.75;True;;Southampton;T2-1912-S-P0087;Shawn Khan\n",
      "451;0;3;Male;;1;0;19.9667;True;;Southampton;T3-1912-S-P0183;Bobby Williams\n",
      "452;0;1;Male;30.0;0;0;27.75;True;C;Cherbourg;T1-1912-C-P0043;Troy Gibson\n",
      "453;1;1;Male;49.0;1;0;89.1042;True;C;Cherbourg;T1-1912-C-P0044;Michael Evans\n",
      "454;0;3;Male;nan;0;0;8.05;True;;Southampton;T3-1912-S-P0184;Robert Davis\n",
      "455;1;3;Male;29.0;0;0;7.8958;True;;Cherbourg;T3-1912-C-P0032;John Robinson\n",
      "456;0;1;Male;65.0;0;0;26.55;True;E;Southampton;T1-1912-S-P0057;Steven Bauer\n",
      "457;1;1;female;;1;0;51.8625;False;D;Southampton;T1-1912-S-P0058;Taylor Torres\n",
      "458;1;2;female;50.0;0;0;10.5 USD;False;;Southampton;T2-1912-S-P0088;Alexis Turner\n",
      "459;0;3;Male;nan;0;0;7.75;True;;Queenstown;T3-1912-Q-P0040;Tony Mendoza\n",
      "460;1;1;Male;48.0;0;0;26.55;True;E;Southampton;T1-1912-S-P0059;Francisco Thompson\n",
      "461;0;3;Male;34.0;0;0;8.05;True;;Southampton;T3-1912-S-P0185;Jeffrey Tran\n",
      "462;0;1;Male;47.0;0;0;38.5;True;E;Southampton;T1-1912-S-P0060;Jeffrey Mckinney\n",
      "463;0;2;Male;48.0;0;0;13.0;True;;Southampton;T2-1912-S-P0089;Michael Carroll\n",
      "464;0;3;Male;;0;0;8.05;True;;Southampton;T3-1912-S-P0186;Nathaniel Gibbs\n",
      "465;0;3;Male;38.0;0;0;7.05;True;;Southampton;T3-1912-S-P0187;Andrew Sullivan\n",
      "466;0;2;Male;nan;0;0;0.0;True;;Southampton;T2-1912-S-P0090;John Swanson\n",
      "467;0;1;Male;56.0;0;0;26.55;True;;Southampton;T1-1912-S-P0061;Brian Johnson\n",
      "468;0;3;Male;nan;0;0;7.725;True;;Queenstown;T3-1912-Q-P0041;Ralph Gibson Jr.\n",
      "469;1;3;female;0.75;2;1;19.2583;False;;Cherbourg;T3-1912-C-P0033;Laura Walker\n",
      "470;0;3;Male;nan;0;0;7.25;True;;Southampton;T3-1912-S-P0188;Michael Johnson\n",
      "471;0;3;Male;38.0;0;0;8.6625;True;;Southampton;T3-1912-S-P0189;Joshua Jensen\n",
      "472;1;2;female;33.0;1;2;27.75;False;;Southampton;T2-1912-S-P0091;Jennifer Warren\n",
      "473;1;2;female;23.0;0;0;13.7917;False;D;Cherbourg;T2-1912-C-P0010;Kathy Harris\n",
      "474;0;3;female;22.0;0;0;9.8375 USD;False;;Southampton;T3-1912-S-P0190;Kathryn Proctor\n",
      "475;0;1;Male;;0;0;52.0;True;A;Southampton;T1-1912-S-P0062;Michael George PhD\n",
      "476;0;2;Male;34.0;1;0;21.0;True;;Southampton;T2-1912-S-P0092;James Rivera\n",
      "477;0;3;Male;29.0;1;0;7.0458 USD;True;;Southampton;T3-1912-S-P0191;Brian Wilson\n",
      "478;0;3;Male;22.0;0;0;7.5208;True;;Southampton;T3-1912-S-P0192;Joshua Brady\n",
      "479;1;3;female;2.0;0;1;12.2875;False;;Southampton;T3-1912-S-P0193;Ashley Anderson\n",
      "480;0;3;Male;9.0;5;2;46.9 USD;False;;Southampton;T3-1912-S-P0194;Mr. Zachary Blackwell\n",
      "481;0;2;Male;;0;0;0.0;True;;Southampton;T2-1912-S-P0093;Joshua Donovan\n",
      "482;0;3;Male;50.0;0;0;8.05;True;;Southampton;T3-1912-S-P0195;Clayton Nguyen\n",
      "483;1;3;female;63.0;0;0;9.5875;False;;Southampton;T3-1912-S-P0196;Linda Dalton\n",
      "484;1;1;Male;25.0;1;0;91.0792;True;B;Cherbourg;T1-1912-C-P0045;Johnny Ross\n",
      "485;0;3;female;;3;1;25.4667;False;;Southampton;T3-1912-S-P0197;Mrs. Stephanie Harmon\n",
      "486;1;1;female;35.0;1;0;90.0;False;C;Southampton;T1-1912-S-P0063;Deborah Dalton\n",
      "487;0;1;Male;58.0;0;0;29.7;True;B;Cherbourg;T1-1912-C-P0046;Raymond Novak\n",
      "488;0;3;Male;30.0;0;0;8.05;True;;Southampton;T3-1912-S-P0198;William Casey\n",
      "489;1;3;Male;9.0;1;1;15.9 USD;False;;Southampton;T3-1912-S-P0199;Charles Chavez\n",
      "490;0;3;Male;;1;0;19.9667;True;;Southampton;T3-1912-S-P0200;Jose Mckay\n",
      "491;0;3;Male;21.0;0;0;7.25;True;;Southampton;T3-1912-S-P0201;Trevor Franklin\n",
      "492;0;1;Male;55.0;0;0;30.5;True;C;Southampton;T1-1912-S-P0064;Scott Sanchez\n",
      "493;0;1;Male;71.0;0;0;49.5042;True;;Cherbourg;T1-1912-C-P0047;Jonathan Smith\n",
      "494;0;3;Male;21.0;0;0;8.05;True;;Southampton;T3-1912-S-P0202;Cameron Patel\n",
      "495;0;3;Male;;0;0;14.4583;True;;Cherbourg;T3-1912-C-P0034;Grant Cain\n",
      "496;1;1;female;54.0;1;0;78.2667 USD;False;D;Cherbourg;T1-1912-C-P0048;Kristen Young\n",
      "497;0;3;Male;nan;0;0;15.1;True;;Southampton;T3-1912-S-P0203;Blake Marquez\n",
      "498;0;1;female;25.0;1;2;151.55;False;C;Southampton;T1-1912-S-P0065;Valerie Dickerson\n",
      "499;0;3;Male;24.0;0;0;7.7958;True;;Southampton;T3-1912-S-P0204;Steven Thomas\n",
      "500;0;3;Male;17.0;0;0;8.6625 USD;True;;Southampton;T3-1912-S-P0205;David Krueger\n",
      "501;0;3;female;21.0;0;0;7.75 USD;False;;Queenstown;T3-1912-Q-P0042;Carrie Adams\n",
      "502;0;3;female;;0;0;7.6292;False;;Queenstown;T3-1912-Q-P0043;Claire Frederick DDS\n",
      "503;0;3;female;37.0;0;0;9.5875;False;;Southampton;T3-1912-S-P0206;Emily Thompson\n",
      "504;1;1;female;16.0;0;0;86.5;False;B;Southampton;T1-1912-S-P0066;Cheryl Gray\n",
      "505;0;1;Male;18.0;1;0;108.9;True;C;Cherbourg;T1-1912-C-P0049;James Richardson\n",
      "506;1;2;female;33.0;0;2;26.0;False;;Southampton;T2-1912-S-P0094;Brenda Schroeder\n",
      "507;1;1;Male;;0;0;26.55;True;;Southampton;T1-1912-S-P0067;Ryan Lopez\n",
      "508;0;3;Male;28.0;0;0;22.525;True;;Southampton;T3-1912-S-P0207;Timothy Smith\n",
      "509;1;3;Male;26.0;0;0;56.4958 USD;True;;Southampton;T3-1912-S-P0208;Michael Morales\n",
      "510;1;3;Male;29.0;0;0;7.75 USD;True;;Queenstown;T3-1912-Q-P0044;Dr. Richard Mccullough\n",
      "511;0;3;Male;;0;0;8.05 USD;True;;Southampton;T3-1912-S-P0209;Scott Hughes\n",
      "512;1;1;Male;36.0;0;0;26.2875;True;E;Southampton;T1-1912-S-P0068;Gabriel Mitchell\n",
      "513;1;1;female;54.0;1;0;59.4;False;;Cherbourg;T1-1912-C-P0050;Jessica Jackson\n",
      "514;0;3;Male;24.0;0;0;7.4958;True;;Southampton;T3-1912-S-P0210;Kyle Russell\n",
      "515;0;1;Male;47.0;0;0;34.0208;True;D;Southampton;T1-1912-S-P0069;Brandon Stewart\n",
      "516;1;2;female;34.0;0;0;10.5;False;F;Southampton;T2-1912-S-P0095;Gina Anderson\n",
      "517;0;3;Male;;0;0;;True;;Queenstown;T3-1912-Q-P0045;Thomas Jackson\n",
      "518;1;2;female;36.0;1;0;26.0;False;;Southampton;T2-1912-S-P0096;Cassandra Campbell\n",
      "519;0;3;Male;32.0;0;0;7.8958;True;;Southampton;T3-1912-S-P0211;Christopher Martinez\n",
      "520;1;1;female;30.0;0;0;93.5 USD;False;B;Southampton;T1-1912-S-P0070;Anna Collier\n",
      "521;0;3;Male;22.0;0;0;7.8958;True;;Southampton;T3-1912-S-P0212;Cesar Adams\n",
      "522;0;3;Male;;0;0;7.225 USD;True;;Cherbourg;T3-1912-C-P0035;Jason Watson\n",
      "523;1;1;female;44.0;0;1;57.9792;False;B;Cherbourg;T1-1912-C-P0051;Bianca Douglas\n",
      "524;0;3;Male;;0;0;7.2292;True;;Cherbourg;T3-1912-C-P0036;Walter Thomas\n",
      "525;0;3;Male;40.5;0;0;7.75 USD;True;;Queenstown;T3-1912-Q-P0046;Charles Rangel\n",
      "526;1;2;female;50.0;0;0;10.5 USD;False;;Southampton;T2-1912-S-P0097;Laura Cain\n",
      "527;0;1;Male;nan;0;0;221.7792;True;C;Southampton;T1-1912-S-P0071;John Cruz\n",
      "528;0;3;Male;39.0;0;0;7.925;True;;Southampton;T3-1912-S-P0213;Edward Love\n",
      "529;0;2;Male;23.0;2;1;11.5;True;;Southampton;T2-1912-S-P0098;Edward Small\n",
      "530;1;2;female;2.0;1;1;26.0;False;;Southampton;T2-1912-S-P0099;Annette Stanton\n",
      "531;0;3;Male;;0;0;7.2292 USD;True;;Cherbourg;T3-1912-C-P0037;Cory Molina\n",
      "532;0;3;Male;17.0;1;1;7.2292 USD;True;;Cherbourg;T3-1912-C-P0038;Mark Ayala\n",
      "533;1;3;female;;0;2;22.3583;False;;Cherbourg;T3-1912-C-P0039;Danielle Cross\n",
      "534;0;3;female;30.0;0;0;8.6625;False;;Southampton;T3-1912-S-P0214;Amy Peterson\n",
      "535;1;2;female;7.0;0;2;26.25 USD;False;;Southampton;T2-1912-S-P0100;Cassandra Figueroa\n",
      "536;0;1;Male;45.0;0;0;26.55;True;B;Southampton;T1-1912-S-P0072;Anthony Mata\n",
      "537;1;1;female;30.0;0;0;106.425;False;;Cherbourg;T1-1912-C-P0052;Jaclyn Cox\n",
      "538;0;3;Male;;0;0;14.5;True;;Southampton;T3-1912-S-P0215;Tim Rodriguez\n",
      "539;1;1;female;22.0;0;2;49.5;False;B;Cherbourg;T1-1912-C-P0053;Debra Carr\n",
      "540;1;1;female;36.0;0;2;71.0;False;B;Southampton;T1-1912-S-P0073;Jamie Reed\n",
      "541;0;3;female;9.0;4;2;31.275;False;;Southampton;T3-1912-S-P0216;Kimberly Lewis\n",
      "542;0;3;female;11.0;4;2;31.275;False;;Southampton;T3-1912-S-P0217;Susan Gill\n",
      "543;1;2;Male;32.0;1;0;26.0 USD;True;;Southampton;T2-1912-S-P0101;Robert Hunter\n",
      "544;0;1;Male;50.0;1;0;106.425 USD;True;C;Cherbourg;T1-1912-C-P0054;Martin Conley\n",
      "545;0;1;Male;64.0;0;0;26.0;True;;Southampton;T1-1912-S-P0074;Joseph Crawford\n",
      "546;1;2;female;19.0;1;0;26.0;False;;Southampton;T2-1912-S-P0102;Morgan Smith\n",
      "547;1;2;Male;;0;0;13.8625;True;;Cherbourg;T2-1912-C-P0011;Charles Casey\n",
      "548;0;3;Male;33.0;1;1;20.525;True;;Southampton;T3-1912-S-P0218;Jerome Clark\n",
      "549;1;2;Male;8.0;1;1;36.75;False;;Southampton;T2-1912-S-P0103;Michael Hester\n",
      "550;1;1;Male;17.0;0;2;110.8833 USD;True;C;Cherbourg;T1-1912-C-P0055;Miguel Decker\n",
      "551;0;2;Male;27.0;0;0;26.0;True;;Southampton;T2-1912-S-P0104;Steven Cook\n",
      "552;0;3;Male;;0;0;7.8292;True;;Queenstown;T3-1912-Q-P0047;Michael Mclaughlin\n",
      "553;1;3;Male;22.0;0;0;7.225;True;;Cherbourg;T3-1912-C-P0040;James Sellers\n",
      "554;1;3;female;22.0;0;0;7.775 USD;False;;Southampton;T3-1912-S-P0219;Michelle Smith\n",
      "555;0;1;Male;62.0;0;0;26.55;True;;Southampton;T1-1912-S-P0075;Chad Wilson\n",
      "556;1;1;female;48.0;1;0;39.6;False;A;Cherbourg;T1-1912-C-P0056;Tiffany Johnson\n",
      "557;0;1;Male;nan;0;0;227.525;True;;Cherbourg;T1-1912-C-P0057;Johnathan Gross\n",
      "558;1;1;female;39.0;1;1;79.65;False;E;Southampton;T1-1912-S-P0076;Angela Smith\n",
      "559;1;3;female;36.0;1;0;17.4;False;;Southampton;T3-1912-S-P0220;Christina Smith\n",
      "560;0;3;Male;;0;0;7.75;True;;Queenstown;T3-1912-Q-P0048;Nathan Hansen\n",
      "561;0;3;Male;40.0;0;0;7.8958;True;;Southampton;T3-1912-S-P0221;Nicholas Nash\n",
      "562;0;2;Male;28.0;0;0;13.5;True;;Southampton;T2-1912-S-P0105;Willie Dudley\n",
      "563;0;3;Male;nan;0;0;8.05 USD;True;;Southampton;T3-1912-S-P0222;Joseph Moss\n",
      "564;0;3;female;;0;0;8.05;False;;Southampton;T3-1912-S-P0223;Cassandra Hansen\n",
      "565;0;3;Male;24.0;2;0;24.15;True;;Southampton;T3-1912-S-P0224;Gregory Green\n",
      "566;0;3;Male;19.0;0;0;7.8958;True;;Southampton;T3-1912-S-P0225;Seth Williams\n",
      "567;0;3;female;29.0;0;4;21.075;False;;Southampton;T3-1912-S-P0226;Pamela Wright\n",
      "568;0;3;Male;;0;0;7.2292;True;;Cherbourg;T3-1912-C-P0041;Kevin Roy\n",
      "569;1;3;Male;32.0;0;0;7.8542;True;;Southampton;T3-1912-S-P0227;Paul Robbins\n",
      "570;1;2;Male;62.0;0;0;10.5;True;;Southampton;T2-1912-S-P0106;Scott Roberts\n",
      "571;1;1;female;53.0;2;0;51.4792 USD;False;C;Southampton;T1-1912-S-P0077;Angelica Cooper\n",
      "572;1;1;Male;36.0;0;0;26.3875;True;E;Southampton;T1-1912-S-P0078;Austin Williams\n",
      "573;1;3;female;;0;0;7.75;False;;Queenstown;T3-1912-Q-P0049;Jessica Morgan\n",
      "574;0;3;Male;16.0;0;0;8.05 USD;True;;Southampton;T3-1912-S-P0228;Daniel Hickman\n",
      "575;0;3;Male;19.0;0;0;14.5;True;;Southampton;T3-1912-S-P0229;Troy Griffith\n",
      "576;1;2;female;34.0;0;0;13.0;False;;Southampton;T2-1912-S-P0107;Cynthia Cooper\n",
      "577;1;1;female;39.0;1;0;55.9;False;E;Southampton;T1-1912-S-P0079;Hannah Davies\n",
      "578;0;3;female;;1;0;14.4583;False;;Cherbourg;T3-1912-C-P0042;Dr. Audrey Ellis\n",
      "579;1;3;Male;32.0;0;0;7.925;True;;Southampton;T3-1912-S-P0230;Michael Richardson\n",
      "580;1;2;female;25.0;1;1;30.0;False;;Southampton;T2-1912-S-P0108;Mrs. Jennifer Wagner\n",
      "581;1;1;female;39.0;1;1;110.8833;False;C;Cherbourg;T1-1912-C-P0058;Deborah Campbell\n",
      "582;0;2;Male;54.0;0;0;26.0;True;;Southampton;T2-1912-S-P0109;Thomas Larson DVM\n",
      "583;0;1;Male;36.0;0;0;40.125;True;A;Cherbourg;T1-1912-C-P0059;Tyler Chavez\n",
      "584;0;3;Male;;0;0;8.7125;True;;Cherbourg;T3-1912-C-P0043;Jacob Smith\n",
      "585;1;1;female;18.0;0;2;79.65;False;E;Southampton;T1-1912-S-P0080;Mrs. Cynthia Morse\n",
      "586;0;2;Male;47.0;0;0;15.0;True;;Southampton;T2-1912-S-P0110;Jeffrey Gonzalez\n",
      "587;1;1;Male;60.0;1;1;79.2;True;B;Cherbourg;T1-1912-C-P0060;Billy Wheeler\n",
      "588;0;3;Male;22.0;0;0;8.05;True;;Southampton;T3-1912-S-P0231;Bryan West\n",
      "589;0;3;Male;nan;0;0;8.05;True;;Southampton;T3-1912-S-P0232;Gabriel Steele\n",
      "590;0;3;Male;35.0;0;0;7.125 USD;True;;Southampton;T3-1912-S-P0233;Andrew Ochoa\n",
      "591;1;1;female;52.0;1;0;78.2667;False;D;Cherbourg;T1-1912-C-P0061;Melissa Weaver\n",
      "592;0;3;Male;47.0;0;0;7.25 USD;True;;Southampton;T3-1912-S-P0234;Christopher Fields\n",
      "593;0;3;female;;0;2;7.75;False;;Queenstown;T3-1912-Q-P0050;Teresa Richardson\n",
      "594;0;2;Male;37.0;1;0;26.0;True;;Southampton;T2-1912-S-P0111;Kevin Stewart\n",
      "595;0;3;Male;36.0;1;1;24.15;True;;Southampton;T3-1912-S-P0235;Craig Stevenson\n",
      "596;1;2;female;;0;0;33.0;False;;Southampton;T2-1912-S-P0112;Lindsay Taylor\n",
      "597;0;3;Male;49.0;0;0;0.0;True;;Southampton;T3-1912-S-P0236;Nicholas Chandler\n",
      "598;0;3;Male;nan;0;0;7.225;True;;Cherbourg;T3-1912-C-P0044;John Davis\n",
      "599;1;1;Male;49.0;1;0;56.9292;True;A;Cherbourg;T1-1912-C-P0062;Justin Lara\n",
      "600;1;2;female;24.0;2;1;27.0;False;;Southampton;T2-1912-S-P0113;Alicia Erickson\n",
      "601;0;3;Male;nan;0;0;7.8958;True;;Southampton;T3-1912-S-P0237;Dr. Craig Pace\n",
      "602;0;1;Male;nan;0;0;42.4 USD;True;;Southampton;T1-1912-S-P0081;Willie Johnson\n",
      "603;0;3;Male;44.0;0;0;8.05;True;;Southampton;T3-1912-S-P0238;Jonathan Robinson\n",
      "604;1;1;Male;35.0;0;0;26.55;True;;Cherbourg;T1-1912-C-P0063;Joshua Walker\n",
      "605;0;3;Male;36.0;1;0;15.55;True;;Southampton;T3-1912-S-P0239;James Lopez\n",
      "606;0;3;Male;30.0;0;0;7.8958;True;;Southampton;T3-1912-S-P0240;Billy Robertson\n",
      "607;1;1;Male;27.0;0;0;30.5;True;;Southampton;T1-1912-S-P0082;Jason Cox\n",
      "608;1;2;female;22.0;1;2;41.5792;False;;Cherbourg;T2-1912-C-P0012;Nicole Ho\n",
      "609;1;1;female;40.0;0;0;153.4625;False;C;Southampton;T1-1912-S-P0083;Barbara Salazar\n",
      "610;0;3;female;39.0;1;5;31.275;False;;Southampton;T3-1912-S-P0241;Linda Diaz\n",
      "611;0;3;Male;;0;0;7.05 USD;True;;Southampton;T3-1912-S-P0242;Ricky Smith\n",
      "612;1;3;female;;1;0;15.5;False;;Queenstown;T3-1912-Q-P0051;Ann Jones\n",
      "613;0;3;Male;nan;0;0;7.75;True;;Queenstown;T3-1912-Q-P0052;James Cervantes\n",
      "614;0;3;Male;35.0;0;0;8.05;True;;Southampton;T3-1912-S-P0243;Christian Huber\n",
      "615;1;2;female;24.0;1;2;65.0;False;;Southampton;T2-1912-S-P0114;Patricia Sullivan\n",
      "616;0;3;Male;34.0;1;1;14.4;True;;Southampton;T3-1912-S-P0244;George Hicks\n",
      "617;0;3;female;26.0;1;0;16.1;False;;Southampton;T3-1912-S-P0245;Chelsea Lopez\n",
      "618;1;2;female;4.0;2;1;39.0;False;F;Southampton;T2-1912-S-P0115;Sierra Robles\n",
      "619;0;2;Male;26.0;0;0;10.5;True;;Southampton;T2-1912-S-P0116;Robert Michael\n",
      "620;0;3;Male;27.0;1;0;14.4542;True;;Cherbourg;T3-1912-C-P0045;Kyle Coleman\n",
      "621;1;1;Male;42.0;1;0;52.5542;True;D;Southampton;T1-1912-S-P0084;Aaron Anderson\n",
      "622;1;3;Male;20.0;1;1;15.7417 USD;True;;Cherbourg;T3-1912-C-P0046;Ryan Herrera Jr.\n",
      "623;0;3;Male;21.0;0;0;7.8542 USD;True;;Southampton;T3-1912-S-P0246;Daniel Norris\n",
      "624;0;3;Male;21.0;0;0;16.1 USD;True;;Southampton;T3-1912-S-P0247;Brian Sanchez\n",
      "625;0;1;Male;61.0;0;0;32.3208;True;D;Southampton;T1-1912-S-P0085;Mark Fletcher\n",
      "626;0;2;Male;57.0;0;0;12.35;True;;Queenstown;T2-1912-Q-P0003;Dave Friedman\n",
      "627;1;1;female;21.0;0;0;77.9583;False;D;Southampton;T1-1912-S-P0086;Tina Sherman\n",
      "628;0;3;Male;26.0;0;0;7.8958;True;;Southampton;T3-1912-S-P0248;Juan Leonard\n",
      "629;0;3;Male;nan;0;0;7.7333 USD;True;;Queenstown;T3-1912-Q-P0053;Dylan Gaines\n",
      "630;1;1;Male;80.0;0;0;30.0;True;A;Southampton;T1-1912-S-P0087;David Stevens\n",
      "631;0;3;Male;51.0;0;0;7.0542;True;;Southampton;T3-1912-S-P0249;Michael Miller\n",
      "632;1;1;Male;32.0;0;0;;True;B;Cherbourg;T1-1912-C-P0064;Corey Kennedy\n",
      "633;0;1;Male;;0;0;0.0;True;;Southampton;T1-1912-S-P0088;Jason Porter\n",
      "634;0;3;female;9.0;3;2;27.9;False;;Southampton;T3-1912-S-P0250;Jordan Brock\n",
      "635;1;2;female;28.0;0;0;13.0;False;;Southampton;T2-1912-S-P0117;Barbara Mullen\n",
      "636;0;3;Male;32.0;0;0;7.925;True;;Southampton;T3-1912-S-P0251;Robert Jones\n",
      "637;0;2;Male;31.0;1;1;26.25;True;;Southampton;T2-1912-S-P0118;Brandon Peters\n",
      "638;0;3;female;41.0;0;5;39.6875;False;;Southampton;T3-1912-S-P0252;Karen Patrick\n",
      "639;0;3;Male;;1;0;16.1;True;;Southampton;T3-1912-S-P0253;Robert Davidson\n",
      "640;0;3;Male;20.0;0;0;7.8542;True;;Southampton;T3-1912-S-P0254;Joshua Watson\n",
      "641;1;1;female;24.0;0;0;69.3;False;B;Cherbourg;T1-1912-C-P0065;Ashley Aguilar\n",
      "642;0;3;female;2.0;3;2;27.9;False;;Southampton;T3-1912-S-P0255;Dr. Lisa Ferguson\n",
      "643;1;3;Male;;0;0;56.4958;True;;Southampton;T3-1912-S-P0256;Richard Lambert\n",
      "644;1;3;female;0.75;2;1;19.2583 USD;False;;Cherbourg;T3-1912-C-P0047;Elizabeth Jones\n",
      "645;1;1;Male;48.0;1;0;76.7292 USD;True;D;Cherbourg;T1-1912-C-P0066;Aaron Wheeler\n",
      "646;0;3;Male;19.0;0;0;7.8958;True;;Southampton;T3-1912-S-P0257;Donald Martinez\n",
      "647;1;1;Male;56.0;0;0;35.5 USD;True;A;Cherbourg;T1-1912-C-P0067;Paul Ball\n",
      "648;0;3;Male;;0;0;;True;;Southampton;T3-1912-S-P0258;Kevin Davis\n",
      "649;1;3;female;23.0;0;0;7.55;False;;Southampton;T3-1912-S-P0259;Janice Dawson\n",
      "650;0;3;Male;nan;0;0;7.8958 USD;True;;Southampton;T3-1912-S-P0260;Christopher Hansen\n",
      "651;1;2;female;18.0;0;1;23.0;False;;Southampton;T2-1912-S-P0119;Kelly Wolfe\n",
      "652;0;3;Male;21.0;0;0;8.4333;True;;Southampton;T3-1912-S-P0261;Jose Simon\n",
      "653;1;3;female;;0;0;7.8292;False;;Queenstown;T3-1912-Q-P0054;Haley Herman\n",
      "654;0;3;female;18.0;0;0;6.75;False;;Queenstown;T3-1912-Q-P0055;Deborah Williams\n",
      "655;0;2;Male;24.0;2;0;73.5;True;;Southampton;T2-1912-S-P0120;Carlos Hansen\n",
      "656;0;3;Male;nan;0;0;7.8958;True;;Southampton;T3-1912-S-P0262;Robert Morrison\n",
      "657;0;3;female;32.0;1;1;15.5 USD;False;;Queenstown;T3-1912-Q-P0056;Jessica Hill\n",
      "658;0;2;Male;23.0;0;0;13.0;True;;Southampton;T2-1912-S-P0121;John Morris\n",
      "659;0;1;Male;58.0;0;2;113.275;True;D;Cherbourg;T1-1912-C-P0068;Ryan Barr\n",
      "660;1;1;Male;50.0;2;0;133.65;True;;Southampton;T1-1912-S-P0089;Melvin Daniels\n",
      "661;0;3;Male;40.0;0;0;7.225;True;;Cherbourg;T3-1912-C-P0048;Brian Dean\n",
      "662;0;1;Male;47.0;0;0;25.5875;True;E;Southampton;T1-1912-S-P0090;John Flores\n",
      "663;0;3;Male;36.0;0;0;7.4958;True;;Southampton;T3-1912-S-P0263;David Sparks\n",
      "664;1;3;Male;20.0;1;0;7.925;True;;Southampton;T3-1912-S-P0264;Kenneth Simmons\n",
      "665;0;2;Male;32.0;2;0;73.5;True;;Southampton;T2-1912-S-P0122;Henry Petersen\n",
      "666;0;2;Male;25.0;0;0;13.0;True;;Southampton;T2-1912-S-P0123;Clifford Rasmussen\n",
      "667;0;3;Male;nan;0;0;7.775;True;;Southampton;T3-1912-S-P0265;Aaron Clements\n",
      "668;0;3;Male;43.0;0;0;8.05;True;;Southampton;T3-1912-S-P0266;Scott Williams\n",
      "669;1;1;female;nan;1;0;52.0;False;C;Southampton;T1-1912-S-P0091;Brenda Lopez\n",
      "670;1;2;female;40.0;1;1;39.0;False;;Southampton;T2-1912-S-P0124;Belinda Smith MD\n",
      "671;0;1;Male;31.0;1;0;52.0;True;B;Southampton;T1-1912-S-P0092;David Meyer\n",
      "672;0;2;Male;70.0;0;0;10.5;True;;Southampton;T2-1912-S-P0125;Seth Ross\n",
      "673;1;2;Male;31.0;0;0;13.0;True;;Southampton;T2-1912-S-P0126;Matthew Jacobs\n",
      "674;0;2;Male;;0;0;;True;;Southampton;T2-1912-S-P0127;Jacob Salinas\n",
      "675;0;3;Male;18.0;0;0;7.775;True;;Southampton;T3-1912-S-P0267;Aaron Carter\n",
      "676;0;3;Male;24.5;0;0;8.05 USD;True;;Southampton;T3-1912-S-P0268;Gary Sims\n",
      "677;1;3;female;18.0;0;0;9.8417;False;;Southampton;T3-1912-S-P0269;Ashley Munoz\n",
      "678;0;3;female;43.0;1;6;46.9;False;;Southampton;T3-1912-S-P0270;Melanie Cook\n",
      "679;1;1;Male;36.0;0;1;512.3292;True;B;Cherbourg;T1-1912-C-P0069;Daniel Miller\n",
      "680;0;3;female;;0;0;8.1375 USD;False;;Queenstown;T3-1912-Q-P0057;Nicole Smith\n",
      "681;1;1;Male;27.0;0;0;76.7292;True;D;Cherbourg;T1-1912-C-P0070;Harold Ward\n",
      "682;0;3;Male;20.0;0;0;9.225;True;;Southampton;T3-1912-S-P0271;David Boone\n",
      "683;0;3;Male;14.0;5;2;46.9;False;;Southampton;T3-1912-S-P0272;Christopher Edwards\n",
      "684;0;2;Male;60.0;1;1;39.0 USD;True;;Southampton;T2-1912-S-P0128;Michael Davis\n",
      "685;0;2;Male;25.0;1;2;;True;;Cherbourg;T2-1912-C-P0013;Gary Gardner\n",
      "686;0;3;Male;14.0;4;1;39.6875;False;;Southampton;T3-1912-S-P0273;Johnny Gallegos\n",
      "687;0;3;Male;19.0;0;0;10.1708;True;;Southampton;T3-1912-S-P0274;Kevin Jacobs\n",
      "688;0;3;Male;18.0;0;0;7.7958;True;;Southampton;T3-1912-S-P0275;Sean Beck\n",
      "689;1;1;female;15.0;0;1;;False;B;Southampton;T1-1912-S-P0093;Jessica Cochran\n",
      "690;1;1;Male;31.0;1;0;57.0;True;B;Southampton;T1-1912-S-P0094;Martin Cunningham\n",
      "691;1;3;female;4.0;0;1;;False;;Cherbourg;T3-1912-C-P0049;Mrs. Brandi Hicks\n",
      "692;1;3;Male;;0;0;56.4958;True;;Southampton;T3-1912-S-P0276;Bobby Riley\n",
      "693;0;3;Male;25.0;0;0;7.225;True;;Cherbourg;T3-1912-C-P0050;Michael Hester\n",
      "694;0;1;Male;60.0;0;0;26.55 USD;True;;Southampton;T1-1912-S-P0095;Michael Brady\n",
      "695;0;2;Male;52.0;0;0;13.5;True;;Southampton;T2-1912-S-P0129;Brandon Schroeder\n",
      "696;0;3;Male;44.0;0;0;8.05;True;;Southampton;T3-1912-S-P0277;Eric Taylor\n",
      "697;1;3;female;;0;0;7.7333;False;;Queenstown;T3-1912-Q-P0058;Melanie Sanford\n",
      "698;0;1;Male;49.0;1;1;110.8833;True;C;Cherbourg;T1-1912-C-P0071;Casey Bowen\n",
      "699;0;3;Male;42.0;0;0;7.65;True;F;Southampton;T3-1912-S-P0278;Brian Romero\n",
      "700;1;1;female;18.0;1;0;227.525;False;C;Cherbourg;T1-1912-C-P0072;Cindy Nicholson\n",
      "701;1;1;Male;35.0;0;0;26.2875 USD;True;E;Southampton;T1-1912-S-P0096;Robert Ramos\n",
      "702;0;3;female;18.0;0;1;14.4542 USD;False;;Cherbourg;T3-1912-C-P0051;Kelly Jefferson\n",
      "703;0;3;Male;25.0;0;0;7.7417;True;;Queenstown;T3-1912-Q-P0059;John Thomas\n",
      "704;0;3;Male;26.0;1;0;7.8542;True;;Southampton;T3-1912-S-P0279;Omar Taylor\n",
      "705;0;2;Male;39.0;0;0;;True;;Southampton;T2-1912-S-P0130;Robert Stout\n",
      "706;1;2;female;45.0;0;0;13.5 USD;False;;Southampton;T2-1912-S-P0131;Lisa Garcia\n",
      "707;1;1;Male;42.0;0;0;26.2875 USD;True;E;Southampton;T1-1912-S-P0097;Jack Cox\n",
      "708;1;1;female;22.0;0;0;151.55;False;;Southampton;T1-1912-S-P0098;Desiree Kidd\n",
      "709;1;3;Male;;1;1;15.2458 USD;True;;Cherbourg;T3-1912-C-P0052;Gabriel Harper\n",
      "710;1;1;female;24.0;0;0;49.5042;False;C;Cherbourg;T1-1912-C-P0073;Sherry Alvarado\n",
      "711;0;1;Male;;0;0;26.55;True;C;Southampton;T1-1912-S-P0099;Aaron Allen\n",
      "712;1;1;Male;48.0;1;0;52.0;True;C;Southampton;T1-1912-S-P0100;William Gardner\n",
      "713;0;3;Male;29.0;0;0;9.4833 USD;True;;Southampton;T3-1912-S-P0280;Anthony Anderson\n",
      "714;0;2;Male;52.0;0;0;13.0;True;;Southampton;T2-1912-S-P0132;George Pearson\n",
      "715;0;3;Male;19.0;0;0;7.65 USD;True;F;Southampton;T3-1912-S-P0281;Bobby Ferguson\n",
      "716;1;1;female;38.0;0;0;227.525;False;C;Cherbourg;T1-1912-C-P0074;Caitlyn Nguyen\n",
      "717;1;2;female;27.0;0;0;10.5;False;E;Southampton;T2-1912-S-P0133;Morgan Eaton\n",
      "718;0;3;Male;nan;0;0;15.5;True;;Queenstown;T3-1912-Q-P0060;Darren Kirk\n",
      "719;0;3;Male;33.0;0;0;7.775;True;;Southampton;T3-1912-S-P0282;Dustin Russell\n",
      "720;1;2;female;6.0;0;1;33.0;False;;Southampton;T2-1912-S-P0134;Pamela Kelly\n",
      "721;0;3;Male;17.0;1;0;7.0542;True;;Southampton;T3-1912-S-P0283;Kurt Daniels\n",
      "722;0;2;Male;34.0;0;0;13.0;True;;Southampton;T2-1912-S-P0135;Jaime Moore\n",
      "723;0;2;Male;50.0;0;0;13.0;True;;Southampton;T2-1912-S-P0136;Sean York\n",
      "724;1;1;Male;27.0;1;0;53.1;True;E;Southampton;T1-1912-S-P0101;David Smith\n",
      "725;0;3;Male;20.0;0;0;8.6625;True;;Southampton;T3-1912-S-P0284;Randy Shah\n",
      "726;1;2;female;30.0;3;0;21.0;False;;Southampton;T2-1912-S-P0137;Andrea Chaney\n",
      "727;1;3;female;;0;0;7.7375;False;;Queenstown;T3-1912-Q-P0061;Catherine Smith\n",
      "728;0;2;Male;25.0;1;0;26.0;True;;Southampton;T2-1912-S-P0138;Calvin Perry\n",
      "729;0;3;female;25.0;1;0;7.925;False;;Southampton;T3-1912-S-P0285;Shelby Galloway\n",
      "730;1;1;female;29.0;0;0;211.3375 USD;False;B;Southampton;T1-1912-S-P0102;Natalie Brown\n",
      "731;0;3;Male;11.0;0;0;18.7875;False;;Cherbourg;T3-1912-C-P0053;Justin Rogers\n",
      "732;0;2;Male;;0;0;0.0;True;;Southampton;T2-1912-S-P0139;Ray Kirby\n",
      "733;0;2;Male;23.0;0;0;13.0 USD;True;;Southampton;T2-1912-S-P0140;David White\n",
      "734;0;2;Male;23.0;0;0;13.0;True;;Southampton;T2-1912-S-P0141;Jose Murray\n",
      "735;0;3;Male;28.5;0;0;16.1;True;;Southampton;T3-1912-S-P0286;George Pearson\n",
      "736;0;3;female;48.0;1;3;34.375;False;;Southampton;T3-1912-S-P0287;Alison Boyle\n",
      "737;1;1;Male;35.0;0;0;512.3292 USD;True;B;Cherbourg;T1-1912-C-P0075;James Ramirez\n",
      "738;0;3;Male;;0;0;7.8958;True;;Southampton;T3-1912-S-P0288;Michael Kemp\n",
      "739;0;3;Male;;0;0;7.8958;True;;Southampton;T3-1912-S-P0289;Christian Wade\n",
      "740;1;1;Male;;0;0;;True;D;Southampton;T1-1912-S-P0103;Brian Alexander\n",
      "741;0;1;Male;36.0;1;0;78.85;True;C;Southampton;T1-1912-S-P0104;John Long\n",
      "742;1;1;female;21.0;2;2;262.375;False;B;Cherbourg;T1-1912-C-P0076;Samantha Smith\n",
      "743;0;3;Male;24.0;1;0;16.1;True;;Southampton;T3-1912-S-P0290;Luis Taylor\n",
      "744;1;3;Male;31.0;0;0;7.925;True;;Southampton;T3-1912-S-P0291;James Gutierrez\n",
      "745;0;1;Male;70.0;1;1;71.0;True;B;Southampton;T1-1912-S-P0105;Joseph Stone\n",
      "746;0;3;Male;16.0;1;1;20.25 USD;True;;Southampton;T3-1912-S-P0292;Ronnie Meadows\n",
      "747;1;2;female;30.0;0;0;13.0;False;;Southampton;T2-1912-S-P0142;Renee Warren\n",
      "748;0;1;Male;19.0;1;0;53.1;True;D;Southampton;T1-1912-S-P0106;Andrew Sullivan\n",
      "749;0;3;Male;31.0;0;0;7.75;True;;Queenstown;T3-1912-Q-P0062;Darryl Brown\n",
      "750;1;2;female;4.0;1;1;23.0;False;;Southampton;T2-1912-S-P0143;Ashley Spencer\n",
      "751;1;3;Male;6.0;0;1;12.475;False;E;Southampton;T3-1912-S-P0293;Daniel Waters\n",
      "752;0;3;Male;33.0;0;0;9.5;True;;Southampton;T3-1912-S-P0294;David Hernandez\n",
      "753;0;3;Male;23.0;0;0;7.8958 USD;True;;Southampton;T3-1912-S-P0295;Michael Thompson\n",
      "754;1;2;female;48.0;1;2;65.0;False;;Southampton;T2-1912-S-P0144;Julie Perez\n",
      "755;1;2;Male;0.67;1;1;14.5 USD;False;;Southampton;T2-1912-S-P0145;Justin Sosa\n",
      "756;0;3;Male;28.0;0;0;7.7958;True;;Southampton;T3-1912-S-P0296;Stephen Day MD\n",
      "757;0;2;Male;18.0;0;0;11.5;True;;Southampton;T2-1912-S-P0146;Brandon Irwin\n",
      "758;0;3;Male;34.0;0;0;8.05;True;;Southampton;T3-1912-S-P0297;Michael Anderson\n",
      "759;1;1;female;33.0;0;0;86.5 USD;False;B;Southampton;T1-1912-S-P0107;Felicia Sheppard\n",
      "760;0;3;Male;nan;0;0;14.5;True;;Southampton;T3-1912-S-P0298;Alex Campos\n",
      "761;0;3;Male;41.0;0;0;7.125;True;;Southampton;T3-1912-S-P0299;Richard Johnson\n",
      "762;1;3;Male;20.0;0;0;7.2292;True;;Cherbourg;T3-1912-C-P0054;Mark Olson\n",
      "763;1;1;female;36.0;1;2;120.0 USD;False;B;Southampton;T1-1912-S-P0108;Kristin Edwards\n",
      "764;0;3;Male;16.0;0;0;7.775;True;;Southampton;T3-1912-S-P0300;Matthew Lee\n",
      "765;1;1;female;51.0;1;0;77.9583;False;D;Southampton;T1-1912-S-P0109;Hannah Young\n",
      "766;0;1;Male;;0;0;39.6;True;;Cherbourg;T1-1912-C-P0077;Joshua Hampton\n",
      "767;0;3;female;30.5;0;0;7.75;False;;Queenstown;T3-1912-Q-P0063;Becky Williams\n",
      "768;0;3;Male;;1;0;24.15;True;;Queenstown;T3-1912-Q-P0064;Steven Hudson DDS\n",
      "769;0;3;Male;32.0;0;0;8.3625;True;;Southampton;T3-1912-S-P0301;Timothy Stafford\n",
      "770;0;3;Male;24.0;0;0;9.5;True;;Southampton;T3-1912-S-P0302;Charles Walker\n",
      "771;0;3;Male;48.0;0;0;7.8542;True;;Southampton;T3-1912-S-P0303;Keith Gilbert\n",
      "772;0;2;female;57.0;0;0;10.5;False;E;Southampton;T2-1912-S-P0147;Bonnie Kelly\n",
      "773;0;3;Male;;0;0;7.225;True;;Cherbourg;T3-1912-C-P0055;Philip Morgan\n",
      "774;1;2;female;54.0;1;3;23.0;False;;Southampton;T2-1912-S-P0148;Jacqueline Henry\n",
      "775;0;3;Male;18.0;0;0;7.75;True;;Southampton;T3-1912-S-P0304;Dylan Gomez\n",
      "776;0;3;Male;nan;0;0;7.75;True;F;Queenstown;T3-1912-Q-P0065;David Schultz\n",
      "777;1;3;female;5.0;0;0;;False;;Southampton;T3-1912-S-P0305;Kimberly Hunter\n",
      "778;0;3;Male;;0;0;7.7375;True;;Queenstown;T3-1912-Q-P0066;Kevin Fitzpatrick\n",
      "779;1;1;female;43.0;0;1;211.3375;False;B;Southampton;T1-1912-S-P0110;Bridget Baker\n",
      "780;1;3;female;13.0;0;0;7.2292;False;;Cherbourg;T3-1912-C-P0056;Emily Gardner\n",
      "781;1;1;female;17.0;1;0;57.0;False;B;Southampton;T1-1912-S-P0111;Emily Davis\n",
      "782;0;1;Male;29.0;0;0;30.0;True;D;Southampton;T1-1912-S-P0112;Joshua Jones\n",
      "783;0;3;Male;;1;2;23.45;True;;Southampton;T3-1912-S-P0306;Jordan Thompson\n",
      "784;0;3;Male;25.0;0;0;7.05;True;;Southampton;T3-1912-S-P0307;Alexander Martinez\n",
      "785;0;3;Male;25.0;0;0;7.25 USD;True;;Southampton;T3-1912-S-P0308;Jeffrey Gibson\n",
      "786;1;3;female;18.0;0;0;7.4958;False;;Southampton;T3-1912-S-P0309;Michelle Shelton\n",
      "787;0;3;Male;8.0;4;1;29.125 USD;False;;Queenstown;T3-1912-Q-P0067;Robert Avery\n",
      "788;1;3;Male;1.0;1;2;20.575;False;;Southampton;T3-1912-S-P0310;Austin Bowen\n",
      "789;0;1;Male;46.0;0;0;79.2;True;B;Cherbourg;T1-1912-C-P0078;Anthony Hall\n",
      "790;0;3;Male;nan;0;0;7.75;True;;Queenstown;T3-1912-Q-P0068;Robert Martin\n",
      "791;0;2;Male;16.0;0;0;26.0;True;;Southampton;T2-1912-S-P0149;Daryl Marks\n",
      "792;0;3;female;;8;2;69.55;False;;Southampton;T3-1912-S-P0311;Brittany Scott\n",
      "793;0;1;Male;;0;0;30.6958 USD;True;;Cherbourg;T1-1912-C-P0079;Alejandro Lopez\n",
      "794;0;3;Male;25.0;0;0;7.8958;True;;Southampton;T3-1912-S-P0312;Alejandro Sanders\n",
      "795;0;2;Male;39.0;0;0;13.0;True;;Southampton;T2-1912-S-P0150;David Norton\n",
      "796;1;1;female;49.0;0;0;25.9292;False;D;Southampton;T1-1912-S-P0113;Theresa Foster\n",
      "797;1;3;female;31.0;0;0;8.6833;False;;Southampton;T3-1912-S-P0313;Teresa Dyer\n",
      "798;0;3;Male;30.0;0;0;7.2292;True;;Cherbourg;T3-1912-C-P0057;Zachary Zimmerman\n",
      "799;0;3;female;30.0;1;1;24.15 USD;False;;Southampton;T3-1912-S-P0314;Dr. Susan Brown MD\n",
      "800;0;2;Male;34.0;0;0;13.0;True;;Southampton;T2-1912-S-P0151;Brian Brown\n",
      "801;1;2;female;31.0;1;1;26.25;False;;Southampton;T2-1912-S-P0152;Angela Davis\n",
      "802;1;1;Male;11.0;1;2;120.0 USD;False;B;Southampton;T1-1912-S-P0114;Kyle Myers\n",
      "803;1;3;Male;0.42;0;1;;False;;Cherbourg;T3-1912-C-P0058;Brian Snyder\n",
      "804;1;3;Male;27.0;0;0;6.975 USD;True;;Southampton;T3-1912-S-P0315;Karl Zimmerman\n",
      "805;0;3;Male;31.0;0;0;7.775 USD;True;;Southampton;T3-1912-S-P0316;Andrew Gross\n",
      "806;0;1;Male;39.0;0;0;0.0 USD;True;A;Southampton;T1-1912-S-P0115;Daryl Luna\n",
      "807;0;3;female;18.0;0;0;7.775;False;;Southampton;T3-1912-S-P0317;Lisa Mitchell\n",
      "808;0;2;Male;39.0;0;0;13.0;True;;Southampton;T2-1912-S-P0153;Christian Kramer\n",
      "809;1;1;female;33.0;1;0;53.1;False;E;Southampton;T1-1912-S-P0116;Jennifer Johnson\n",
      "810;0;3;Male;26.0;0;0;7.8875;True;;Southampton;T3-1912-S-P0318;Corey Bates\n",
      "811;0;3;Male;39.0;0;0;24.15;True;;Southampton;T3-1912-S-P0319;Gregory Terrell\n",
      "812;0;2;Male;35.0;0;0;10.5;True;;Southampton;T2-1912-S-P0154;John Webb\n",
      "813;0;3;female;6.0;4;2;31.275;False;;Southampton;T3-1912-S-P0320;Tracy Clark\n",
      "814;0;3;Male;30.5;0;0;8.05 USD;True;;Southampton;T3-1912-S-P0321;Harold Wilson\n",
      "815;0;1;Male;;0;0;0.0;True;B;Southampton;T1-1912-S-P0117;Gary Long\n",
      "816;0;3;female;23.0;0;0;7.925;False;;Southampton;T3-1912-S-P0322;Ann Johnson\n",
      "817;0;2;Male;31.0;1;1;37.0042;True;;Cherbourg;T2-1912-C-P0014;James Armstrong\n",
      "818;0;3;Male;43.0;0;0;6.45;True;;Southampton;T3-1912-S-P0323;Steven Brown\n",
      "819;0;3;Male;10.0;3;2;27.9 USD;False;;Southampton;T3-1912-S-P0324;Jason Gray\n",
      "820;1;1;female;52.0;1;1;93.5 USD;False;B;Southampton;T1-1912-S-P0118;Donna Davis\n",
      "821;1;3;Male;27.0;0;0;8.6625;True;;Southampton;T3-1912-S-P0325;Scott Hoover\n",
      "822;0;1;Male;38.0;0;0;0.0;True;;Southampton;T1-1912-S-P0119;Daniel Stanley PhD\n",
      "823;1;3;female;27.0;0;1;12.475 USD;False;E;Southampton;T3-1912-S-P0326;Jessica Kim\n",
      "824;0;3;Male;2.0;4;1;39.6875;False;;Southampton;T3-1912-S-P0327;Michael Petersen\n",
      "825;0;3;Male;;0;0;6.95 USD;True;;Queenstown;T3-1912-Q-P0069;Anthony Phillips\n",
      "826;0;3;Male;;0;0;56.4958;True;;Southampton;T3-1912-S-P0328;Juan Guerrero\n",
      "827;1;2;Male;1.0;0;2;37.0042;False;;Cherbourg;T2-1912-C-P0015;Matthew Miller\n",
      "828;1;3;Male;;0;0;7.75;True;;Queenstown;T3-1912-Q-P0070;Joshua Cherry\n",
      "829;1;1;female;62.0;0;0;80.0 USD;False;B;;T1-1912-U-P0002;Penny Miller\n",
      "830;1;3;female;15.0;1;0;14.4542 USD;False;;Cherbourg;T3-1912-C-P0059;Jessica Montgomery\n",
      "831;1;2;Male;0.83;1;1;18.75;False;;Southampton;T2-1912-S-P0155;Randy Castillo\n",
      "832;0;3;Male;nan;0;0;7.2292 USD;True;;Cherbourg;T3-1912-C-P0060;Brian Figueroa\n",
      "833;0;3;Male;23.0;0;0;7.8542;True;;Southampton;T3-1912-S-P0329;Timothy Gibbs\n",
      "834;0;3;Male;18.0;0;0;8.3 USD;True;;Southampton;T3-1912-S-P0330;Mr. James Price\n",
      "835;1;1;female;39.0;1;1;83.1583;False;E;Cherbourg;T1-1912-C-P0080;Ms. Tammy Hernandez\n",
      "836;0;3;Male;21.0;0;0;8.6625;True;;Southampton;T3-1912-S-P0331;Joshua Gibson\n",
      "837;0;3;Male;;0;0;8.05;True;;Southampton;T3-1912-S-P0332;Anthony Santiago\n",
      "838;1;3;Male;32.0;0;0;56.4958;True;;Southampton;T3-1912-S-P0333;Phillip Evans\n",
      "839;1;1;Male;;0;0;29.7;True;C;Cherbourg;T1-1912-C-P0081;Daniel Thompson\n",
      "840;0;3;Male;20.0;0;0;7.925;True;;Southampton;T3-1912-S-P0334;Alvin Harvey\n",
      "841;0;2;Male;16.0;0;0;10.5;True;;Southampton;T2-1912-S-P0156;Bryce Ellison\n",
      "842;1;1;female;30.0;0;0;31.0;False;;Cherbourg;T1-1912-C-P0082;Cynthia Proctor\n",
      "843;0;3;Male;34.5;0;0;6.4375 USD;True;;Cherbourg;T3-1912-C-P0061;Dr. Robert Castillo Jr.\n",
      "844;0;3;Male;17.0;0;0;8.6625;True;;Southampton;T3-1912-S-P0335;Donald Berry\n",
      "845;0;3;Male;42.0;0;0;7.55;True;;Southampton;T3-1912-S-P0336;Joshua Carlson\n",
      "846;0;3;Male;nan;8;2;69.55 USD;True;;Southampton;T3-1912-S-P0337;Benjamin Lewis\n",
      "847;0;3;Male;35.0;0;0;7.8958;True;;Cherbourg;T3-1912-C-P0062;Chad Peterson\n",
      "848;0;2;Male;28.0;0;1;33.0;True;;Southampton;T2-1912-S-P0157;Daniel Carey\n",
      "849;1;1;female;;1;0;89.1042;False;C;Cherbourg;T1-1912-C-P0083;Carrie Esparza MD\n",
      "850;0;3;Male;4.0;4;2;31.275;False;;Southampton;T3-1912-S-P0338;Aaron Lopez\n",
      "851;0;3;Male;74.0;0;0;7.775;True;;Southampton;T3-1912-S-P0339;Mark Mayer\n",
      "852;0;3;female;9.0;1;1;15.2458 USD;False;;Cherbourg;T3-1912-C-P0063;Emily Chase\n",
      "853;1;1;female;16.0;0;1;39.4;False;D;Southampton;T1-1912-S-P0120;Emily Preston\n",
      "854;0;2;female;44.0;1;0;26.0;False;;Southampton;T2-1912-S-P0158;Elizabeth Hernandez\n",
      "855;1;3;female;18.0;0;1;9.35 USD;False;;Southampton;T3-1912-S-P0340;Megan Barrett\n",
      "856;1;1;female;45.0;1;1;164.8667;False;;Southampton;T1-1912-S-P0121;Amanda Mcdonald\n",
      "857;1;1;Male;51.0;0;0;26.55;True;E;Southampton;T1-1912-S-P0122;Aaron Smith\n",
      "858;1;3;female;24.0;0;3;19.2583 USD;False;;Cherbourg;T3-1912-C-P0064;Amy Allen\n",
      "859;0;3;Male;nan;0;0;7.2292;True;;Cherbourg;T3-1912-C-P0065;Paul Guzman\n",
      "860;0;3;Male;41.0;2;0;14.1083;True;;Southampton;T3-1912-S-P0341;Michael Ewing\n",
      "861;0;2;Male;21.0;1;0;11.5 USD;True;;Southampton;T2-1912-S-P0159;Darin Trujillo\n",
      "862;1;1;female;48.0;0;0;25.9292;False;D;Southampton;T1-1912-S-P0123;Lauren Allen\n",
      "863;0;3;female;;8;2;69.55;False;;Southampton;T3-1912-S-P0342;Alyssa Mitchell\n",
      "864;0;2;Male;24.0;0;0;13.0;True;;Southampton;T2-1912-S-P0160;Douglas Pena\n",
      "865;1;2;female;42.0;0;0;13.0;False;;Southampton;T2-1912-S-P0161;Kimberly Rocha\n",
      "866;1;2;female;27.0;1;0;13.8583;False;;Cherbourg;T2-1912-C-P0016;Kelly Lopez\n",
      "867;0;1;Male;31.0;0;0;50.4958 USD;True;A;Southampton;T1-1912-S-P0124;Francis Taylor\n",
      "868;0;3;Male;;0;0;9.5;True;;Southampton;T3-1912-S-P0343;Kurt Lowe\n",
      "869;1;3;Male;4.0;1;1;11.1333;False;;Southampton;T3-1912-S-P0344;Adam Hayes\n",
      "870;0;3;Male;26.0;0;0;7.8958;True;;Southampton;T3-1912-S-P0345;Franklin Randolph\n",
      "871;1;1;female;47.0;1;1;52.5542 USD;False;D;Southampton;T1-1912-S-P0125;Michelle Martin\n",
      "872;0;1;Male;33.0;0;0;5.0;True;B;Southampton;T1-1912-S-P0126;Richard Rodriguez\n",
      "873;0;3;Male;47.0;0;0;;True;;Southampton;T3-1912-S-P0346;Todd Evans\n",
      "874;1;2;female;28.0;1;0;24.0;False;;Cherbourg;T2-1912-C-P0017;Jocelyn Vega\n",
      "875;1;3;female;15.0;0;0;7.225;False;;Cherbourg;T3-1912-C-P0066;Pamela Lewis\n",
      "876;0;3;Male;20.0;0;0;9.8458;True;;Southampton;T3-1912-S-P0347;Thomas Cannon\n",
      "877;0;3;Male;19.0;0;0;7.8958;True;;Southampton;T3-1912-S-P0348;Brian Hill\n",
      "878;0;3;Male;nan;0;0;7.8958;True;;Southampton;T3-1912-S-P0349;Brandon Freeman\n",
      "879;1;1;female;56.0;0;1;83.1583;False;C;Cherbourg;T1-1912-C-P0084;Robin Johnson\n",
      "880;1;2;female;25.0;0;1;26.0;False;;Southampton;T2-1912-S-P0162;Caitlyn Thompson\n",
      "881;0;3;Male;33.0;0;0;;True;;Southampton;T3-1912-S-P0350;Scott Mathis\n",
      "882;0;3;female;22.0;0;0;10.5167;False;;Southampton;T3-1912-S-P0351;Samantha Moore\n",
      "883;0;2;Male;28.0;0;0;10.5;True;;Southampton;T2-1912-S-P0163;Gary Casey\n",
      "884;0;3;Male;25.0;0;0;7.05;True;;Southampton;T3-1912-S-P0352;Michael Carroll\n",
      "885;0;3;female;39.0;0;5;29.125;False;;Queenstown;T3-1912-Q-P0071;Amy Gonzales\n",
      "886;0;2;Male;27.0;0;0;13.0;True;;Southampton;T2-1912-S-P0164;Michael Johnson\n",
      "887;1;1;female;19.0;0;0;30.0;False;B;Southampton;T1-1912-S-P0127;Lisa Buck\n",
      "888;0;3;female;nan;1;2;23.45;False;;Southampton;T3-1912-S-P0353;Alexandria Lawrence\n",
      "889;1;1;Male;26.0;0;0;;True;C;Cherbourg;T1-1912-C-P0085;Christopher Reed\n",
      "890;0;3;Male;32.0;0;0;7.75;True;;Queenstown;T3-1912-Q-P0072;James Horton\n"
     ]
    }
   ],
   "source": [
    "# the delimiter being used is ;\n",
    "!cat messy_titanic.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21319e16",
   "metadata": {},
   "source": [
    "Now try again using `sep`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "43c0d132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>passengerId</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>T3-1912-S-P0001</td>\n",
       "      <td>Dustin Flores</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>T1-1912-C-P0001</td>\n",
       "      <td>Mary Tyler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.925</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>T3-1912-S-P0002</td>\n",
       "      <td>Anna Mendoza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>T1-1912-S-P0001</td>\n",
       "      <td>Debra Pearson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.05</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>T3-1912-S-P0003</td>\n",
       "      <td>Anthony Taylor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>886</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>T2-1912-S-P0164</td>\n",
       "      <td>Michael Johnson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>887</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>T1-1912-S-P0127</td>\n",
       "      <td>Lisa Buck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>888</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.45</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>T3-1912-S-P0353</td>\n",
       "      <td>Alexandria Lawrence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>889</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>T1-1912-C-P0085</td>\n",
       "      <td>Christopher Reed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>890</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.75</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>T3-1912-Q-P0072</td>\n",
       "      <td>James Horton</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  survived  pclass     sex   age  sibsp  parch     fare  \\\n",
       "0             0         0       3    Male  22.0      1      0     7.25   \n",
       "1             1         1       1  female  38.0      1      0  71.2833   \n",
       "2             2         1       3  female  26.0      0      0    7.925   \n",
       "3             3         1       1  female  35.0      1      0     53.1   \n",
       "4             4         0       3    Male  35.0      0      0     8.05   \n",
       "..          ...       ...     ...     ...   ...    ...    ...      ...   \n",
       "886         886         0       2    Male  27.0      0      0     13.0   \n",
       "887         887         1       1  female  19.0      0      0     30.0   \n",
       "888         888         0       3  female   NaN      1      2    23.45   \n",
       "889         889         1       1    Male  26.0      0      0      NaN   \n",
       "890         890         0       3    Male  32.0      0      0     7.75   \n",
       "\n",
       "     adult_male deck  embark_town      passengerId                 name  \n",
       "0          True  NaN  Southampton  T3-1912-S-P0001        Dustin Flores  \n",
       "1         False    C    Cherbourg  T1-1912-C-P0001           Mary Tyler  \n",
       "2         False  NaN  Southampton  T3-1912-S-P0002         Anna Mendoza  \n",
       "3         False    C  Southampton  T1-1912-S-P0001        Debra Pearson  \n",
       "4          True  NaN  Southampton  T3-1912-S-P0003       Anthony Taylor  \n",
       "..          ...  ...          ...              ...                  ...  \n",
       "886        True  NaN  Southampton  T2-1912-S-P0164      Michael Johnson  \n",
       "887       False    B  Southampton  T1-1912-S-P0127            Lisa Buck  \n",
       "888       False  NaN  Southampton  T3-1912-S-P0353  Alexandria Lawrence  \n",
       "889        True    C    Cherbourg  T1-1912-C-P0085     Christopher Reed  \n",
       "890        True  NaN   Queenstown  T3-1912-Q-P0072         James Horton  \n",
       "\n",
       "[891 rows x 13 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('messy_titanic.csv', sep=';')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b291249",
   "metadata": {},
   "source": [
    "**It seems that someone saved the index by mistake... Lets get rid of it!**\n",
    "\n",
    "### Q.1.1 Drop the unnamed column.\n",
    "- Use `df.drop(columns=['col_name'], inplace=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2f24948d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>passengerId</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>T3-1912-S-P0001</td>\n",
       "      <td>Dustin Flores</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>T1-1912-C-P0001</td>\n",
       "      <td>Mary Tyler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.925</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>T3-1912-S-P0002</td>\n",
       "      <td>Anna Mendoza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>T1-1912-S-P0001</td>\n",
       "      <td>Debra Pearson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.05</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>T3-1912-S-P0003</td>\n",
       "      <td>Anthony Taylor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare  adult_male deck  \\\n",
       "0         0       3    Male  22.0      1      0     7.25        True  NaN   \n",
       "1         1       1  female  38.0      1      0  71.2833       False    C   \n",
       "2         1       3  female  26.0      0      0    7.925       False  NaN   \n",
       "3         1       1  female  35.0      1      0     53.1       False    C   \n",
       "4         0       3    Male  35.0      0      0     8.05        True  NaN   \n",
       "\n",
       "   embark_town      passengerId            name  \n",
       "0  Southampton  T3-1912-S-P0001   Dustin Flores  \n",
       "1    Cherbourg  T1-1912-C-P0001      Mary Tyler  \n",
       "2  Southampton  T3-1912-S-P0002    Anna Mendoza  \n",
       "3  Southampton  T1-1912-S-P0001   Debra Pearson  \n",
       "4  Southampton  T3-1912-S-P0003  Anthony Taylor  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a1f9e1",
   "metadata": {},
   "source": [
    "### Q.1.2 How many rows and columns there is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1fa65f0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 891 rows and 12 columns\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476f3f47",
   "metadata": {},
   "source": [
    "### Q.1.3 Check of each column data type\n",
    "\n",
    "- Is every column data type the most appropriate for it?\n",
    "- Are there any column with null/missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "233140a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   survived     891 non-null    int64  \n",
      " 1   pclass       891 non-null    int64  \n",
      " 2   sex          891 non-null    object \n",
      " 3   age          714 non-null    float64\n",
      " 4   sibsp        891 non-null    int64  \n",
      " 5   parch        891 non-null    int64  \n",
      " 6   fare         846 non-null    object \n",
      " 7   adult_male   891 non-null    bool   \n",
      " 8   deck         203 non-null    object \n",
      " 9   embark_town  889 non-null    object \n",
      " 10  passengerId  891 non-null    object \n",
      " 11  name         891 non-null    object \n",
      "dtypes: bool(1), float64(1), int64(4), object(6)\n",
      "memory usage: 77.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd8c2be",
   "metadata": {},
   "source": [
    "### Q.1.4 Verify descritive statistics\n",
    "- All numeric columns are being displayed?\n",
    "- Those descritive statistcs are meaningfull for every column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "21e751c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         survived      pclass         age       sibsp       parch\n",
       "count  891.000000  891.000000  714.000000  891.000000  891.000000\n",
       "mean     0.383838    2.308642   29.699118    0.523008    0.381594\n",
       "std      0.486592    0.836071   14.526497    1.102743    0.806057\n",
       "min      0.000000    1.000000    0.420000    0.000000    0.000000\n",
       "25%      0.000000    2.000000   20.125000    0.000000    0.000000\n",
       "50%      0.000000    3.000000   28.000000    0.000000    0.000000\n",
       "75%      1.000000    3.000000   38.000000    1.000000    0.000000\n",
       "max      1.000000    3.000000   80.000000    8.000000    6.000000"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d81bc5e",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "Now that we have at least some information about the dataset, it is more than obvious that there are many problems.\n",
    "\n",
    "Let's preprocess this dataset to make it more viable and clean for future analysis.\n",
    "\n",
    "### Q.2.1 First of all, the documentation provided a column name that does not exist.\n",
    "- Identify and `rename` it:\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "df.rename(columns={'old_name': 'new_name'}, inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e65fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>passengerId</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>T3-1912-S-P0001</td>\n",
       "      <td>Dustin Flores</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>T1-1912-C-P0001</td>\n",
       "      <td>Mary Tyler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.925</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>T3-1912-S-P0002</td>\n",
       "      <td>Anna Mendoza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>T1-1912-S-P0001</td>\n",
       "      <td>Debra Pearson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.05</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>T3-1912-S-P0003</td>\n",
       "      <td>Anthony Taylor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare  adult_male cabin  \\\n",
       "0         0       3    Male  22.0      1      0     7.25        True   NaN   \n",
       "1         1       1  female  38.0      1      0  71.2833       False     C   \n",
       "2         1       3  female  26.0      0      0    7.925       False   NaN   \n",
       "3         1       1  female  35.0      1      0     53.1       False     C   \n",
       "4         0       3    Male  35.0      0      0     8.05        True   NaN   \n",
       "\n",
       "   embark_town      passengerId            name  \n",
       "0  Southampton  T3-1912-S-P0001   Dustin Flores  \n",
       "1    Cherbourg  T1-1912-C-P0001      Mary Tyler  \n",
       "2  Southampton  T3-1912-S-P0002    Anna Mendoza  \n",
       "3  Southampton  T1-1912-S-P0001   Debra Pearson  \n",
       "4  Southampton  T3-1912-S-P0003  Anthony Taylor  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets rename deck to deck\n",
    "df.rename(columns={'deck': 'deck'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e27140",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary style=\"background-color: #fff3cd; padding: 10px; border-radius: 5px; cursor: pointer;\">\n",
    "    <h2 style=\"display: inline;\">\n",
    "        Understanding Numerical vs. Categorical Data\n",
    "        <span style=\"color: red; font-size: 0.8em;\">(click to expand/collapse)</span>\n",
    "    </h2>\n",
    "</summary>\n",
    "\n",
    "It is important to understand **what type of data each column represents**.  \n",
    "Choosing the **correct type** improves **memory usage**, **speeds up operations**, and makes **analysis more reliable**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Numerical Data**\n",
    ">\n",
    ">These are values that represent quantities and support mathematical operations.\n",
    ">\n",
    ">**Examples:**\n",
    ">- Age (`22`, `45.0`)\n",
    ">- Fare (`71.283`, `15.50`)\n",
    ">- Number of siblings (`3`, `0`)\n",
    ">- Any measured or counted quantity\n",
    ">\n",
    ">**Two main subtypes:**\n",
    ">- **int** â†’ whole numbers (0, 1, 2â€¦)\n",
    ">- **float** â†’ decimal numbers (0.0, 3.14â€¦)\n",
    ">\n",
    ">**Numerical columns allow:**\n",
    ">- statistics (mean, median, std)\n",
    ">- plotting histograms\n",
    ">- mathematical operations\n",
    ">- machine learning models\n",
    "---\n",
    "### **2. Categorical Data**\n",
    ">\n",
    ">These represent **labels**, **groups**, or **categories**, not quantities.\n",
    ">\n",
    ">**Examples:**\n",
    ">- Sex (`\"male\"`, `\"female\"`)\n",
    ">- Embarkation port (`\"C\"`, `\"Q\"`, `\"S\"`)\n",
    ">- Passenger class (`\"First\"`, `\"Second\"`, `\"Third\"`)\n",
    ">- Deck (`\"A\"`, `\"B\"`, `\"Unknown\"`)\n",
    ">\n",
    ">**Types commonly used:**\n",
    ">- **string (object)** â†’ free text labels  \n",
    ">- **category** â†’ optimized categorical labels (recommended)\n",
    ">\n",
    ">**Categorical columns allow:**\n",
    ">- grouping (`df.groupby(\"sex\")`)\n",
    ">- value counts\n",
    ">- filtering\n",
    ">- faster processing when stored as `category`\n",
    "---\n",
    "### **Why this distinction matters**\n",
    ">\n",
    ">If a column is stored with the wrong dtype:\n",
    ">\n",
    ">- A numeric column stored as string **cannot be computed**:  \n",
    ">  `\"45\" + \"10\"` â†’ `\"4510\"` (string concatenation!)\n",
    ">- A categorical column stored as numbers **breaks meaning**\n",
    ">- Machine learning pipelines **fail or behave poorly**\n",
    ">- Missing values become difficult to detect and fix\n",
    ">- Memory usage increases (especially with text)\n",
    ">\n",
    ">**So the first step in Data Wrangling is:**\n",
    ">\n",
    ">ðŸ‘‰ **Identify** whether each column is numerical or categorical  \n",
    ">ðŸ‘‰ **Fix** the dtype accordingly\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eb9992",
   "metadata": {},
   "source": [
    "## Fixing Column Types\n",
    "\n",
    "Datasets often contain columns stored with the wrong data type.  \n",
    "This usually happens when values are mixed (e.g., numbers + text), when the file was exported incorrectly, or when data is collected from multiple sources.\n",
    "\n",
    "Working with incorrect types can break calculations, comparisons, plots, or machine-learning pipelines.  \n",
    "So the first step is to **inspect** and **fix** them.\n",
    "\n",
    "**Common strategies for fixing the column types are:**\n",
    "\n",
    "___\n",
    ">**Direct convertion:**\n",
    ">\n",
    ">If you know that the column will be able to be directly converted\n",
    ">```python\n",
    ">df['column_name'] = df['column_name'].astype(dtype)\n",
    ">```\n",
    "___\n",
    "**1. Convert a column to numeric**\n",
    ">\n",
    ">Use when the column **should be a number**, but contains strings, symbols, or inconsistent formatting.\n",
    ">\n",
    ">```python\n",
    ">df[\"column_name\"] = pd.to_numeric(df[\"column_name\"], errors=\"coerce\")\n",
    ">```\n",
    ">`errors=\"coerce\"` converts invalid values into **NaN**, which can later be imputed.\n",
    "___\n",
    "\n",
    "**2. Clean and convert a column that contains units or extra text**\n",
    ">\n",
    ">Useful for columns like \"100 USD\", \"45 years\", \"200 km\".\n",
    ">\n",
    ">```python\n",
    ">df[\"column_name\"] = (\n",
    ">    df[\"column_name\"]\n",
    ">        .astype(str)\n",
    ">        .str.replace(\" USD\", \"\", regex=False)\n",
    ">)\n",
    ">\n",
    "># now casting to numeric dtype\n",
    ">df[\"column_name\"] = pd.to_numeric(df[\"column_name\"], errors=\"coerce\")\n",
    ">```\n",
    "___\n",
    "**3. Convert a column to string (categorical text)**\n",
    ">\n",
    ">Use when the column should represent labels, not numbers.\n",
    ">\n",
    ">```python\n",
    ">df[\"column_name\"] = df[\"column_name\"].astype(str)\n",
    ">```\n",
    "___\n",
    "**4. Convert a column to category (optional optimization)**\n",
    ">\n",
    ">Useful for columns with repeated labels (e.g., \"male\", \"female\", \"unknown\").\n",
    ">\n",
    ">```python\n",
    ">df[\"column_name\"] = df[\"column_name\"].astype(\"category\")\n",
    ">```\n",
    "___\n",
    "**5. Convert a column to datetime**\n",
    ">\n",
    ">For dates like \"2020-01-05\" or \"05/01/2020\".\n",
    ">```python\n",
    ">df[\"column_name\"] = pd.to_datetime(df[\"column_name\"], errors=\"coerce\")\n",
    ">```\n",
    "___\n",
    "**After fixing all types, it is good practice to check the result:**\n",
    ">\n",
    ">- `.info()`\n",
    ">- `.dtypes`\n",
    "\n",
    "### Q 2.2 Inspect each column, decide the most appropriate `dtype` and `fix` it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49059452",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3453ddd7",
   "metadata": {},
   "source": [
    "`survived`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "58e3c609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived\n",
       "0    549\n",
       "1    342\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['survived'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c3409ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('bool')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# could be boolean or category - lets convert to boolean\n",
    "df['survived'] = df['survived'].astype(bool)\n",
    "df['survived'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bf8a48bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived\n",
       "False    549\n",
       "True     342\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['survived'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "dc165710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want as category, make sure to use map() or replace() if you want the category value different than the current value\n",
    "# df['survived'] = df['survived'].map({0: 'no', 1: 'yes'}).astype('category')\n",
    "\n",
    "# use df['survived'].cat.categories to verify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cff8c4",
   "metadata": {},
   "source": [
    "`fare`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ccca5e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived\n",
       "False    549\n",
       "True     342\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b84b61c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         7.25\n",
       "1      71.2833\n",
       "2        7.925\n",
       "3         53.1\n",
       "4         8.05\n",
       "        ...   \n",
       "886       13.0\n",
       "887       30.0\n",
       "888      23.45\n",
       "889        NaN\n",
       "890       7.75\n",
       "Name: fare, Length: 891, dtype: object"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fare is currently as object\n",
    "df['fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c1fec108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7       21.075 USD\n",
       "8      11.1333 USD\n",
       "10        16.7 USD\n",
       "11       26.55 USD\n",
       "28      7.8792 USD\n",
       "          ...     \n",
       "855       9.35 USD\n",
       "858    19.2583 USD\n",
       "861       11.5 USD\n",
       "867    50.4958 USD\n",
       "871    52.5542 USD\n",
       "Name: fare, Length: 170, dtype: object"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets identify those values that are making the column as object\n",
    "# pd.to_numeric(df['fare'], errors='coerce') -> tries to convert every value in df['fare'] to a number (float).\n",
    "# If a value cannot be converted (like '?', 'abc', 'Â£20'), it becomes NaN.\n",
    "# .isna() returns True where the value is NaN and False where the value is a valid number.\n",
    "# .notna() is: True where the original value is not NaN and False where it is NaN\n",
    "\n",
    "\"\"\"\n",
    "In other words, we are filtering for rows where:\n",
    "\n",
    "the value fails to convert to a number (becomes NaN),\n",
    "AND at the same time the original value was not already NaN.\n",
    "\"\"\"\n",
    "\n",
    "mask = pd.to_numeric(df['fare'], errors='coerce').isna() & df['fare'].notna()\n",
    "df.loc[mask, 'fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "58f89cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are going to use .replace() to remove the ' USD' suffix and convert to float\n",
    "df['fare'] = (\n",
    "    df['fare']\n",
    "        .str.replace(' USD', '')\n",
    "        .astype(float)\n",
    ")\n",
    "df['fare'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "616da8de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: fare, dtype: float64)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check again\n",
    "mask = pd.to_numeric(df['fare'], errors='coerce').isna() & df['fare'].notna()\n",
    "df.loc[mask, 'fare']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd28a0de",
   "metadata": {},
   "source": [
    "`age`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "cd1d4512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age\n",
       "24.00    30\n",
       "22.00    27\n",
       "18.00    26\n",
       "28.00    25\n",
       "30.00    25\n",
       "         ..\n",
       "24.50     1\n",
       "0.67      1\n",
       "0.42      1\n",
       "34.50     1\n",
       "74.00     1\n",
       "Name: count, Length: 88, dtype: int64"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# age as float?\n",
    "df.age.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0a34db25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57     28.50\n",
       "78      0.83\n",
       "111    14.50\n",
       "116    70.50\n",
       "122    32.50\n",
       "123    32.50\n",
       "148    36.50\n",
       "152    55.50\n",
       "153    40.50\n",
       "203    45.50\n",
       "227    20.50\n",
       "296    23.50\n",
       "305     0.92\n",
       "331    45.50\n",
       "469     0.75\n",
       "525    40.50\n",
       "644     0.75\n",
       "676    24.50\n",
       "735    28.50\n",
       "755     0.67\n",
       "767    30.50\n",
       "803     0.42\n",
       "814    30.50\n",
       "831     0.83\n",
       "843    34.50\n",
       "Name: age, dtype: float64"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we do not want to see the NaNs\n",
    "filter_notna = df['age'].notna()\n",
    "filter_decimals = df['age'] % 1 != 0\n",
    "df[filter_notna & filter_decimals]['age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "29067c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age\n",
       "28.50    2\n",
       "0.83     2\n",
       "32.50    2\n",
       "40.50    2\n",
       "45.50    2\n",
       "0.75     2\n",
       "30.50    2\n",
       "70.50    1\n",
       "36.50    1\n",
       "14.50    1\n",
       "20.50    1\n",
       "55.50    1\n",
       "0.92     1\n",
       "23.50    1\n",
       "24.50    1\n",
       "0.67     1\n",
       "0.42     1\n",
       "34.50    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['age'] % 1 != 0]['age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "385b53ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(177)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will ideally convert age to int... notice that this will reduce the granularity about the babys...\n",
    "# we also need to decide what we are going to do about the NaNs\n",
    "df['age'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "efbafba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([ 57,  78, 111, 116, 122, 123, 148, 152, 153, 203, 227, 296, 305, 331,\n",
       "       469, 525, 644, 676, 735, 755, 767, 803, 814, 831, 843],\n",
       "      dtype='int64')"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets truncate each value ... you could also round() the value\n",
    "# there are many ways you can do this...\n",
    "# in this example, we are going to use our filter previously used to inspect the age column and obtain the rows index where there is a problem\n",
    "filter_notna = df['age'].notna()\n",
    "filter_decimals = df['age'] % 1 != 0\n",
    "df[filter_notna & filter_decimals].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "fb8989a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(28.5)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now lets fix just those age values\n",
    "problematic_age = list(df[filter_notna & filter_decimals].index)\n",
    "\n",
    "# remember that we can use loc to filter\n",
    "df.loc[57, 'age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8e4974",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[list(df['sex']=='Male'), 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0ef8bde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for age_index in problematic_age:\n",
    "    df.loc[age_index, 'age'] = int(df.loc[age_index, 'age'])\n",
    "\n",
    "# this will truncate but the column dtype is still float because there are other floats already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "f8748b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(28.0)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[57, 'age']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f1afb220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Dtype()"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age'] = df['age'].astype('Int64')\n",
    "df['age'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5d4676c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        22\n",
       "1        38\n",
       "2        26\n",
       "3        35\n",
       "4        35\n",
       "       ... \n",
       "886      27\n",
       "887      19\n",
       "888    <NA>\n",
       "889      26\n",
       "890      32\n",
       "Name: age, Length: 891, dtype: Int64"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184528f7",
   "metadata": {},
   "source": [
    "**ðŸ‘‰ notice that we have missing values in this column... the dtype might be what we want but we can still clean this column**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d2a60b",
   "metadata": {},
   "source": [
    "## Standardizing Categorical Values\n",
    "\n",
    "Sometimes you may also find inconsistent or incorrect labels in categorical columns.  \n",
    "For example, the same category may appear in multiple formats:\n",
    "\n",
    "- \"female\", \"Female\", \"F\", \"f\"\n",
    "- \"male\", \"Male\", \"M\", \"m\"\n",
    "\n",
    "These inconsistencies make analysis, filtering, and grouping unreliable.  \n",
    "To fix this, we **standardize** all categorical values into a consistent format.\n",
    "\n",
    "---\n",
    "**1. Convert everything to lowercase**\n",
    ">\n",
    ">This removes differences like \"Female\" vs \"female\".\n",
    ">\n",
    ">```python\n",
    ">df[\"column_name\"] = df[\"column_name\"].astype(str).str.lower()\n",
    ">```\n",
    "---\n",
    "**2. If there are many variations, a `mapping dictionary` might help:**\n",
    ">\n",
    ">```python\n",
    ">mapping = {\n",
    ">    \"female\": \"female\",\n",
    ">    \"f\": \"female\",\n",
    ">    \"woman\": \"female\",\n",
    ">    \"male\": \"male\",\n",
    ">    \"m\": \"male\",\n",
    ">    \"man\": \"male\"\n",
    ">}\n",
    ">\n",
    ">df[\"column_name\"] = df[\"column_name\"].replace(mapping)\n",
    ">```\n",
    "___\n",
    "**Tips:**\n",
    ">\n",
    ">- Verify the frequencies of `categorical columns`\n",
    ">```python\n",
    ">df[\"column_name\"].value_counts()\n",
    ">```\n",
    ">- Converting to `category` offers optimization in terms of memory footprint, comparinson speed and interpretability\n",
    ">```python\n",
    ">df[\"column_name\"] = df[\"column_name\"].astype(\"category\")\n",
    ">```\n",
    "\n",
    "### Q.2.3 Verify if any categorical column needs to be standardized\n",
    "- Inspect columns of dtype `category`, `object`, `bool`, `int`\n",
    "- Inspect using `df['column'].values_count()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "2fad9114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sex\n",
       "Male      577\n",
       "female    314\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "15d247e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'male'"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets convert all values to lowercase\n",
    "'Male'.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12e7403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        male\n",
       "1      female\n",
       "2      female\n",
       "3      female\n",
       "4        male\n",
       "        ...  \n",
       "886      male\n",
       "887    female\n",
       "888    female\n",
       "889      male\n",
       "890      male\n",
       "Name: sex, Length: 891, dtype: object"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the convertion before applying\n",
    "df['sex'].astype(str).str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8db480f",
   "metadata": {},
   "source": [
    "**now we can apply it with**\n",
    "\n",
    "```python\n",
    "df['sex'] = df['sex'].astype(str).str.lower()\n",
    "```\n",
    "\n",
    "**but lets suppose that we had more problems...**\n",
    "\n",
    "**let's use a mapping dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "09700c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        male\n",
       "1      female\n",
       "2      female\n",
       "3      female\n",
       "4        male\n",
       "        ...  \n",
       "886      male\n",
       "887    female\n",
       "888    female\n",
       "889      male\n",
       "890      male\n",
       "Name: sex, Length: 891, dtype: object"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will test before applying it\n",
    "mapping_dict = {\n",
    "    'Male': 'male',\n",
    "    'Female': 'female',\n",
    "    'Other': 'other',\n",
    "    'M': 'male',\n",
    "    'F': 'female',\n",
    "    'O': 'other',\n",
    "    np.nan: 'not specified'\n",
    "}\n",
    "\n",
    "\n",
    "df['sex'].replace(mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "c8742d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sex\n",
       "male      577\n",
       "female    314\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sex'] = df['sex'].replace(mapping_dict)\n",
    "df['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b74c59a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the dtype is O which stands for object\n",
    "df['sex'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "4f870650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        male\n",
       "1      female\n",
       "2      female\n",
       "3      female\n",
       "4        male\n",
       "        ...  \n",
       "886      male\n",
       "887    female\n",
       "888    female\n",
       "889      male\n",
       "890      male\n",
       "Name: sex, Length: 891, dtype: category\n",
       "Categories (2, object): ['female', 'male']"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it would not hurt to cast this column to category, right?\n",
    "df['sex'] = df['sex'].astype('category')\n",
    "df['sex']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcfad3d",
   "metadata": {},
   "source": [
    "## Missing values\n",
    "\n",
    "Most real-world datasets contain missing or inconsistent values.\n",
    "\n",
    "We must decide how to handle them.\n",
    "\n",
    "**The main strategies are:**\n",
    "___\n",
    "**1. Remove rows with missing values**\n",
    "\n",
    ">Useful when only a few rows contain missing values.\n",
    ">\n",
    ">- Removing them does not distort the dataset.\n",
    ">\n",
    ">```python\n",
    ">df_clean = df.dropna()\n",
    ">```\n",
    "___\n",
    "**2. Remove columns with too many missing values**\n",
    "\n",
    ">Useful when a column is mostly empty (e.g., >80% missing maybe even lower threshold)\n",
    ">\n",
    ">- The column is not essential for the analysis\n",
    ">\n",
    ">```python\n",
    ">df_clean = df.drop(columns=[\"column_name\"])\n",
    ">```\n",
    "___\n",
    "**3. Impute missing values (numeric columns)**\n",
    "\n",
    ">Common inputs:\n",
    ">\n",
    ">- Mean\n",
    ">\n",
    ">- Median (more robust to outliers)\n",
    ">\n",
    ">- Constant value\n",
    ">\n",
    ">```python\n",
    "># mean\n",
    ">df[\"column_name\"] = df[\"column_name\"].fillna(df[\"column_name\"].mean())\n",
    ">\n",
    "># median\n",
    ">df[\"column_name\"] = df[\"column_name\"].fillna(df[\"column_name\"].median())\n",
    ">\n",
    "># constant value\n",
    ">df[\"column_name\"] = df[\"column_name\"].fillna(1)\n",
    ">```\n",
    "___\n",
    "**4. Impute missing values (categorical columns)**\n",
    "\n",
    ">Typical strategies:\n",
    ">\n",
    ">Replace with the most frequent category (mode)\n",
    ">\n",
    ">Replace with a special label such as \"Unknown\"\n",
    ">\n",
    ">\n",
    ">```python\n",
    "># mode\n",
    ">df[\"column_name\"] = df[\"column_name\"].fillna(df[\"column_name\"].mode()[0])\n",
    ">\n",
    "># special label\n",
    ">df[\"column_name\"] = df[\"column_name\"].fillna(\"Unknown\")\n",
    ">```\n",
    "___\n",
    "**There are more advanced methods of imputation but those above cover most of the problems that you might have initially.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8c861e",
   "metadata": {},
   "source": [
    "### Q.2.4 Check for missing values in every column:\n",
    "- Try to obtain a sum of missing values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "4b4357ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived            0\n",
       "pclass              0\n",
       "sex                 0\n",
       "age                 0\n",
       "sibsp               0\n",
       "parch               0\n",
       "fare                0\n",
       "adult_male          0\n",
       "cabin             688\n",
       "embark_town         0\n",
       "passengerId         0\n",
       "name                0\n",
       "fare_bin            0\n",
       "age_fare_ratio     14\n",
       "surname             0\n",
       "class               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa34858",
   "metadata": {},
   "source": [
    "### Q.2.5 Which column is the most problematic in terms of missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7db750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you answer here\n",
    "# deck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c643f9b",
   "metadata": {},
   "source": [
    "### Q.2.6 Let's find out the percentage of missing values in the most problematic column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b8971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(77.21661054994388)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "(df['deck'].isnull().sum() / len(df['deck'])) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2459f1e5",
   "metadata": {},
   "source": [
    "### Q.2.7 Define a function that will check for missing values in percentage each column that you provide as a param.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "def percentage_missing_values(df: pd.DataFrame, col: str) -> float:\n",
    "\n",
    "    # your code here\n",
    "    \n",
    "    return percentage_of_missing\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ede344a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(77.21661054994388)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "def percentage_missing_values(df: pd.DataFrame, col: str) -> float:\n",
    "    return (df[col].isnull().sum() / len(df[col])) * 100\n",
    "\n",
    "percentage_missing_values(df, 'cabin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8738d4",
   "metadata": {},
   "source": [
    "### Q 2.8. Impute missing values for each column that requires cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5608e22c",
   "metadata": {},
   "source": [
    "`age`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "2e9ff275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age'].fillna(df['age'].median(), inplace=True)\n",
    "df['age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc7ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to solve this problem would be\n",
    "df['age'] = (\n",
    "    pd.to_numeric(df['age'], errors='coerce')           # convert to numeric\n",
    "      .fillna(df['age'].median())                       # imput NaN with median\n",
    "      .astype(int)                                      # trunc\n",
    ")\n",
    "\n",
    "# one-liner: df['age'] = pd.to_numeric(df['age'], errors='coerce').fillna(df['age'].median()).astype(int)\n",
    "\n",
    "# much easier :D pay attention to the pipeline but hadling missing values is covered on the following section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5f90f3",
   "metadata": {},
   "source": [
    "`fare`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df5c732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(45)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets input the median too\n",
    "df['fare'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6d8d29e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['fare'].fillna(df['fare'].median(), inplace=True)\n",
    "df['fare'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa14da5",
   "metadata": {},
   "source": [
    "**- 1. we could be more specific and input the median per ticket class...**\n",
    "\n",
    "**- 2. we could be even more specific and also consider the port where the passanger embarked**\n",
    "\n",
    "**Try any of that!!!** (you migh need to reload the dataset to obtain the missing values again)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d75e1b",
   "metadata": {},
   "source": [
    "`embark_town`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "1a48cfe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(2)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['embark_town'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eda5a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "embark_town\n",
       "Southampton    644\n",
       "Cherbourg      168\n",
       "Queenstown      77\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['embark_town'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffb76a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>passengerId</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T1-1912-U-P0001</td>\n",
       "      <td>Kelly Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T1-1912-U-P0002</td>\n",
       "      <td>Penny Miller</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  pclass     sex  age  sibsp  parch  fare  adult_male cabin  \\\n",
       "61       True       1  female   38      0      0  80.0       False     B   \n",
       "829      True       1  female   62      0      0  80.0       False     B   \n",
       "\n",
       "    embark_town      passengerId          name  \n",
       "61          NaN  T1-1912-U-P0001   Kelly Smith  \n",
       "829         NaN  T1-1912-U-P0002  Penny Miller  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['embark_town'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422bfa87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61     NaN\n",
       "829    NaN\n",
       "Name: embark_town, dtype: object"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['embark_town'].isnull(), 'embark_town']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "004830ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['embark_town'].isnull(), 'embark_town'] = 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "30e98a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>passengerId</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [survived, pclass, sex, age, sibsp, parch, fare, adult_male, cabin, embark_town, passengerId, name]\n",
       "Index: []"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check again\n",
    "df[df['embark_town'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f9483730",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['embark_town'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8286597d",
   "metadata": {},
   "source": [
    "`deck`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4220db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is over 77% of missing data... there is nothing we could do right now to save this column :D\n",
    "# there are advanced techniques that rely on building models that based on other features could infer those values but we won't dwelve deep on this\n",
    "# and the success of such approach would need to be considered with some discretion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7032fa",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Now that the dataset is clean, with corrected data types, standardized categorical values, and missing values imputed, we can begin improving the dataset by **creating new features**.\n",
    "\n",
    "Feature Engineering helps models extract more meaningful patterns by transforming the raw data into something more informative and more useful for analysis or machine learning.\n",
    "\n",
    "---\n",
    ">**1.Mathematical combinations**\n",
    ">Create new features by combining existing numeric features.\n",
    ">\n",
    ">**Example: family size**\n",
    ">```python\n",
    ">df[\"family_size\"] = df[\"sibsp\"] + df[\"parch\"] + 1\n",
    ">```\n",
    "---\n",
    ">**2. Binarizing or grouping categories**\n",
    ">Transform categories into simpler or grouped labels.\n",
    ">\n",
    ">**Example: traveling alone vs not**\n",
    ">```python\n",
    ">df[\"is_alone\"] = (df[\"family_size\"] == 1)\n",
    ">```\n",
    "---\n",
    ">**3. Extracting information from existing columns**\n",
    ">\n",
    ">**Example: title extracted from the passenger name**\n",
    ">```python\n",
    ">df[\"title\"] = df[\"name\"].str.extract(r\",\\s*([^\\.]*)\\.\", expand=False)\n",
    ">df[\"title\"] = df[\"title\"].str.strip().str.lower()\n",
    ">```\n",
    "---\n",
    ">**4. Discretization (binning)**\n",
    ">Convert continuous variables into intervals.\n",
    ">\n",
    ">**Example using `cut`:**\n",
    ">```python\n",
    ">df[\"age_group\"] = pd.qcut(\n",
    ">    df[\"age\"],\n",
    ">    bins=[0, 12, 18, 35, 60, 100],\n",
    ">    labels=[\"child\", \"teen\", \"young_adult\", \"adult\", \"senior\"]\n",
    ">)\n",
    ">```\n",
    ">**-  `pd.cut` will consider intervals as `( ]`**\n",
    ">**- `open on left (exclusive)` and `closed on right (inclusive)`**\n",
    ">\n",
    ">**Example using `qcut` (data will be split in equal parts)**\n",
    ">```python\n",
    ">df[\"age_group\"] = pd.cut(\n",
    ">    df[\"age\"],\n",
    ">    q=5,\n",
    ">    labels=[\"child\", \"teen\", \"young_adult\", \"adult\", \"senior\"]\n",
    ">)\n",
    ">```\n",
    "---\n",
    ">**5. Encoding boolean or numeric signals**\n",
    ">\n",
    ">**Example: indicator for high fare**\n",
    ">```python\n",
    ">df[\"high_fare\"] = df[\"fare\"] > df[\"fare\"].median()\n",
    ">```\n",
    "---\n",
    ">**6. Interaction features**\n",
    ">Features created by combining multiple existing ones.\n",
    ">\n",
    ">**Example: price paid per person**\n",
    ">```python\n",
    ">df[\"fare_per_person\"] = df[\"fare\"] / df[\"family_size\"]\n",
    ">```\n",
    "---\n",
    "**Example Creating feature set**\n",
    ">\n",
    ">```python\n",
    ">df[\"family_size\"] = df[\"sibsp\"] + df[\"parch\"] + 1\n",
    ">df[\"is_alone\"] = (df[\"family_size\"] == 1)\n",
    ">\n",
    ">df[\"title\"] = df[\"name\"].str.extract(r\",\\s*([^\\.]*)\\.\", expand=False)\n",
    ">df[\"title\"] = df[\"title\"].str.strip().str.lower()\n",
    ">\n",
    ">df[\"age_group\"] = pd.cut(\n",
    ">    df[\"age\"],\n",
    ">    bins=[0, 12, 18, 35, 60, 100],\n",
    ">    labels=[\"child\", \"teen\", \"young_adult\", \"adult\", \"senior\"]\n",
    ">)\n",
    ">\n",
    ">df[\"high_fare\"] = df[\"fare\"] > df[\"fare\"].median()\n",
    ">df[\"fare_per_person\"] = df[\"fare\"] / df[\"family_size\"]\n",
    ">\n",
    ">df.head()\n",
    ">```\n",
    "---\n",
    "### Q 3.1. Create a new column called `is_child` that is `True` when **age < 12**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "b67b9576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      False\n",
       "1      False\n",
       "2      False\n",
       "3      False\n",
       "4      False\n",
       "       ...  \n",
       "886    False\n",
       "887    False\n",
       "888    False\n",
       "889    False\n",
       "890    False\n",
       "Name: is_child, Length: 891, dtype: boolean"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_child'] = df['age'] < 12\n",
    "df['is_child']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a672c86",
   "metadata": {},
   "source": [
    "### **Q 3.2.** Create a column `family_size` based on sibsp e parch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "387d65e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2\n",
       "1      2\n",
       "2      1\n",
       "3      2\n",
       "4      1\n",
       "      ..\n",
       "886    1\n",
       "887    1\n",
       "888    4\n",
       "889    1\n",
       "890    1\n",
       "Name: family_size, Length: 891, dtype: int64"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['family_size'] = df['sibsp'] + df['parch'] + 1\n",
    "df['family_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88ef0b4",
   "metadata": {},
   "source": [
    "### Q 3.3. Create a `class` column based on the `Ticket Class`.\n",
    "- **1 -> First**\n",
    "- **2 -> Second**\n",
    "- **3 -> Third**\n",
    "\n",
    "- **Set the `dtype` of this column to `category`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "df813db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'] = df['pclass'].replace({\n",
    "    1: 'First',\n",
    "    2: 'Second',\n",
    "    3: 'Third'\n",
    "}).astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e200b4e",
   "metadata": {},
   "source": [
    "### Q.3.4. Create a `embarked` column based on the Port where the passenger embarked.\n",
    "- **Use the initials as the category value.**\n",
    "- **Set the `dtype` of this column to `category`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ecdb5c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      S\n",
       "1      C\n",
       "2      S\n",
       "3      S\n",
       "4      S\n",
       "      ..\n",
       "886    S\n",
       "887    S\n",
       "888    S\n",
       "889    C\n",
       "890    Q\n",
       "Name: embarked, Length: 891, dtype: object"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['embarked'] = df['embark_town'].str[0]\n",
    "df['embarked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "f9e7bca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "embarked\n",
       "S    644\n",
       "C    168\n",
       "Q     77\n",
       "U      2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['embarked'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c403df3",
   "metadata": {},
   "source": [
    "### Q 3.5. Create a `fare_bin` column by splitting fare into 4 quantiles using `pd.qcut`.\n",
    "- use `labels` as a list of `'Low', 'Medium', 'High', 'Very High'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "e79f7d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            Low\n",
       "1      Very High\n",
       "2            Low\n",
       "3      Very High\n",
       "4         Medium\n",
       "         ...    \n",
       "886       Medium\n",
       "887    Very High\n",
       "888         High\n",
       "889       Medium\n",
       "890          Low\n",
       "Name: fare_bin, Length: 891, dtype: category\n",
       "Categories (4, object): ['Low' < 'Medium' < 'High' < 'Very High']"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['fare_bin'] = pd.qcut(df['fare'], q=4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "df['fare_bin']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8551f5e6",
   "metadata": {},
   "source": [
    "### Q 3.6. Create a column `age_fare_ratio` equal to `age / fare`, and comment on whether this feature makes sense.\n",
    "- be careful about zero-division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef27528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      3.034483\n",
       "1      0.533084\n",
       "2      3.280757\n",
       "3      0.659134\n",
       "4      4.347826\n",
       "         ...   \n",
       "886    2.076923\n",
       "887    0.633333\n",
       "888     1.19403\n",
       "889    1.798785\n",
       "890    4.129032\n",
       "Name: age_fare_ratio, Length: 891, dtype: Float64"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the replace is to avoid zero-division\n",
    "df['age_fare_ratio'] = df['age'] / df['fare'].replace(0, np.nan)\n",
    "df['age_fare_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb9e77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(14)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets verify if there is null values...\n",
    "df['age_fare_ratio'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9314cff",
   "metadata": {},
   "source": [
    "**ðŸ‘‰ Important**\n",
    "- **we just derived this feature - creating it introduced some nans that we could clean. Perhaps based on some strategie we used before or a new one...**\n",
    "- **check the next question... maybe we could use it and input the median of `age_fare_ratio` based on the `age_group`**\n",
    "- **try it after the `Q.3.7`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5427c7d",
   "metadata": {},
   "source": [
    "### Q 3.7. Create `age_group` **(you decide the bins and labels)** and count how many passengers exist in each `age_group`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "5461b582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age_group\n",
       "young adult    449\n",
       "adult          239\n",
       "teenager        71\n",
       "child           61\n",
       "senior          60\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age_group'] = pd.cut(df['age'], bins=[0, 11, 18, 30, 50, 70], labels=['child', 'teenager', 'young adult', 'adult', 'senior'])\n",
    "df['age_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc7f015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['young adult', 'adult', 'senior', 'child', 'teenager', NaN]\n",
       "Categories (5, object): ['child' < 'teenager' < 'young adult' < 'adult' < 'senior']"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it seems correct right? but what if there were passengers with age above 70?\n",
    "# what about the passengers with 0 years?\n",
    "# notice the NaN when we verify for the unique values. .values_count() does not show NaNs...\n",
    "\n",
    "df['age_group'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "47b5a522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>71</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>71</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>74</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age age_group\n",
       "96    71       NaN\n",
       "493   71       NaN\n",
       "630   80       NaN\n",
       "851   74       NaN"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['age'] > 70][['age', 'age_group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "3a6bae21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age age_group\n",
       "78     0       NaN\n",
       "305    0       NaN\n",
       "469    0       NaN\n",
       "644    0       NaN\n",
       "755    0       NaN\n",
       "803    0       NaN\n",
       "831    0       NaN"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['age'] == 0][['age', 'age_group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "b03999b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(80)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df['age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be88759",
   "metadata": {},
   "source": [
    "**let's make sure that we catch all the intervals now**\n",
    "\n",
    "**we can use the -1 on the left since there is no age below 0 and the max(df['age'])**\n",
    "\n",
    "**we could also discretize the adult and senior interval a bit more**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "da152c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age_group\n",
       "young adult    449\n",
       "adult          239\n",
       "teenager        71\n",
       "mature          56\n",
       "child           38\n",
       "baby            30\n",
       "senior           8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age_group'] = pd.cut(df['age'], bins=[-1, 3, 11, 18, 30, 50, 65, max(df['age'])], labels=['baby', 'child', 'teenager', 'young adult', 'adult', 'mature', 'senior'],)\n",
    "df['age_group'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd5d40b",
   "metadata": {},
   "source": [
    "**alternatively, we can use the `float(-inf)` and `float('inf')`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb887129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age_group\n",
       "young adult    449\n",
       "adult          239\n",
       "teenager        71\n",
       "mature          56\n",
       "child           38\n",
       "baby            30\n",
       "senior           8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age_group'] = pd.cut(df['age'], bins=[float('-inf'), 3, 11, 18, 30, 50, 65, float('inf')], labels=['baby', 'child', 'teenager', 'young adult', 'adult', 'mature', 'senior'])\n",
    "df['age_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ef0c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now all babys are covered\n",
    "len(df[(df['age'] >= 0) & (df['age'] <= 3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05201fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_child\n",
       "False    823\n",
       "True      68\n",
       "Name: count, dtype: Int64"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 30 babys + 38 childs == 68 is_child\n",
    "df['is_child'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f724bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>71</td>\n",
       "      <td>senior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>71</td>\n",
       "      <td>senior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>80</td>\n",
       "      <td>senior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>74</td>\n",
       "      <td>senior</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age age_group\n",
       "96    71    senior\n",
       "493   71    senior\n",
       "630   80    senior\n",
       "851   74    senior"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and all seniors too\n",
    "df[df['age'] > 70][['age', 'age_group']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68874d25",
   "metadata": {},
   "source": [
    "### Q 3.8. Create a new column `surname` containing the passengerâ€™s surname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "a7eef7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Flores\n",
       "1         Tyler\n",
       "2       Mendoza\n",
       "3       Pearson\n",
       "4        Taylor\n",
       "         ...   \n",
       "886     Johnson\n",
       "887        Buck\n",
       "888    Lawrence\n",
       "889        Reed\n",
       "890      Horton\n",
       "Name: surname, Length: 891, dtype: object"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['surname'] = df['name'].str.split().str[-1]\n",
    "df['surname']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7ef837",
   "metadata": {},
   "source": [
    "### **Q 3.9.** Using the engineered features, compute a summary showing:\n",
    "- average `fare` per `age_group`\n",
    "- average `family_size` per `class`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c58b1f0",
   "metadata": {},
   "source": [
    "average `fare` per `age_group`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "c674b42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age_group\n",
       "young adult    449\n",
       "adult          239\n",
       "teenager        71\n",
       "mature          56\n",
       "child           38\n",
       "baby            30\n",
       "senior           8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0325b498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['baby', 'child', 'teenager', 'young adult', 'adult', 'mature', 'senior']"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check again the categories as a list\n",
    "list(df['age_group'].cat.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "f024b08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['young adult', 'adult', 'mature', 'baby', 'teenager', 'child', 'senior']"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we could also use unique but it would not respect the category order...\n",
    "list(df['age_group'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83d7213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(33.00153)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first try to build the filter - then you can generalize\n",
    "df[df['age_group'] == 'baby']['fare'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cc2481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Fare of baby: 33.00153\n",
      "Average Fare of child: 29.021163157894733\n",
      "Average Fare of teenager: 29.728584507042253\n",
      "Average Fare of young adult: 24.385290868596883\n",
      "Average Fare of adult: 41.14185983263599\n",
      "Average Fare of mature: 41.23095357142857\n",
      "Average Fare of senior: 27.710425\n"
     ]
    }
   ],
   "source": [
    "# now we will use a variable instead of the baby category and iterate on it\n",
    "for group in list(df['age_group'].cat.categories):\n",
    "    print(f\"Average Fare of {group}: {df.loc[df['age_group'] == group, 'fare'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "dbddf249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Fare of baby: 33.00153\n",
      "Average Fare of child: 29.021163157894733\n",
      "Average Fare of teenager: 29.728584507042253\n",
      "Average Fare of young adult: 24.385290868596883\n",
      "Average Fare of adult: 41.14185983263599\n",
      "Average Fare of mature: 41.23095357142857\n",
      "Average Fare of senior: 27.710425\n"
     ]
    }
   ],
   "source": [
    "# without loc\n",
    "for group in list(df['age_group'].cat.categories):\n",
    "    print(f\"Average Fare of {group}: {df[df['age_group'] == group]['fare'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c166ad",
   "metadata": {},
   "source": [
    "average `family_size` per `class`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "b2eb53f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['First', 'Second', 'Third'], dtype='object')"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d94ac8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.7731481481481481)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build your filter for one category then generalize\n",
    "df.loc[df['class'] == 'First', 'family_size'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "b6417c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Family Size of First: 1.7731481481481481\n",
      "Average Family Size of Second: 1.7826086956521738\n",
      "Average Family Size of Third: 2.0081466395112018\n"
     ]
    }
   ],
   "source": [
    "for group in list(df['class'].cat.categories):\n",
    "    print(f\"Average Family Size of {group}: {df.loc[df['class'] == group, 'family_size'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "58996673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Family Size of First: 1.7731481481481481\n",
      "Average Family Size of Second: 1.7826086956521738\n",
      "Average Family Size of Third: 2.0081466395112018\n"
     ]
    }
   ],
   "source": [
    "# without loc\n",
    "for group in list(df['class'].cat.categories):\n",
    "    print(f\"Average Family Size of {group}: {df[df['class'] == group]['family_size'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0089f98e",
   "metadata": {},
   "source": [
    "## 4. Export Dataset\n",
    "\n",
    "Now that our dataset is clean, we might want to save time by not needing to preprocess it again.\n",
    "\n",
    "Exporting it is a good way to avoid running all cleaning steps every time you start your notebook.\n",
    "\n",
    "There are many formats you can choose, and the most common are:\n",
    "\n",
    "---\n",
    "\n",
    "### âœ”ï¸ Export to CSV\n",
    ">```python\n",
    ">df_clean.to_csv(\"titanic_clean.csv\", index=False)\n",
    ">```\n",
    ">\n",
    ">- `index=False` avoids saving the DataFrame index as an extra column.\n",
    ">- CSV files are human-readable and compatible with most tools.\n",
    "---\n",
    "### âœ”ï¸ Export to Excel\n",
    ">```python\n",
    ">df_clean.to_excel(\"titanic_clean.xlsx\", index=False)\n",
    ">```\n",
    ">\n",
    ">- Requires `openpyxl` or `xlsxwriter` installed.\n",
    ">- Useful when sharing data with non-programmers.\n",
    "---\n",
    "### âœ”ï¸ Export to JSON\n",
    ">```python\n",
    ">df_clean.to_json(\"titanic_clean.json\", orient=\"records\")\n",
    ">```\n",
    ">\n",
    ">**Common orientations:**\n",
    ">\n",
    ">- `\"records\"` â†’ list of dictionaries (most common)\n",
    ">- `\"split\"` â†’ dictionary with `index`, `columns`, `data`\n",
    ">- `\"table\"` â†’ includes metadata, useful for interoperability\n",
    "---\n",
    "### âœ”ï¸ Loading Back the Dataset Later\n",
    ">```python\n",
    ">df_loaded = pd.read_csv(\"titanic_clean.csv\")\n",
    ">```\n",
    ">\n",
    ">**or:**\n",
    ">\n",
    ">```python\n",
    ">df_loaded = pd.read_json(\"titanic_clean.json\")\n",
    ">```\n",
    "---\n",
    "**if you are curious, search for `pickle` and `parquet`.**\n",
    "\n",
    "**Now you can skip all preprocessing and jump directly into analysis.**\n",
    "\n",
    "### Q 4.1. Export the dataset to `csv`. Set `index=True` as an additional param."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "c404c261",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_titanic.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8ba341",
   "metadata": {},
   "source": [
    "### Q 4.2 Load into a variable named `df_loaded`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "bccdb52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>cabin</th>\n",
       "      <th>...</th>\n",
       "      <th>passengerId</th>\n",
       "      <th>name</th>\n",
       "      <th>fare_bin</th>\n",
       "      <th>age_fare_ratio</th>\n",
       "      <th>surname</th>\n",
       "      <th>class</th>\n",
       "      <th>is_child</th>\n",
       "      <th>family_size</th>\n",
       "      <th>embarked</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>T3-1912-S-P0001</td>\n",
       "      <td>Dustin Flores</td>\n",
       "      <td>Low</td>\n",
       "      <td>3.034483</td>\n",
       "      <td>Flores</td>\n",
       "      <td>Third</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>S</td>\n",
       "      <td>young adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>T1-1912-C-P0001</td>\n",
       "      <td>Mary Tyler</td>\n",
       "      <td>Very High</td>\n",
       "      <td>0.533084</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>First</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>C</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>T3-1912-S-P0002</td>\n",
       "      <td>Anna Mendoza</td>\n",
       "      <td>Low</td>\n",
       "      <td>3.280757</td>\n",
       "      <td>Mendoza</td>\n",
       "      <td>Third</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>young adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>T1-1912-S-P0001</td>\n",
       "      <td>Debra Pearson</td>\n",
       "      <td>Very High</td>\n",
       "      <td>0.659134</td>\n",
       "      <td>Pearson</td>\n",
       "      <td>First</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>S</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>T3-1912-S-P0003</td>\n",
       "      <td>Anthony Taylor</td>\n",
       "      <td>Medium</td>\n",
       "      <td>4.347826</td>\n",
       "      <td>Taylor</td>\n",
       "      <td>Third</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>886</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>Male</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>T2-1912-S-P0164</td>\n",
       "      <td>Michael Johnson</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2.076923</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>Second</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>young adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>887</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>T1-1912-S-P0127</td>\n",
       "      <td>Lisa Buck</td>\n",
       "      <td>Very High</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>Buck</td>\n",
       "      <td>First</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>young adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>888</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>T3-1912-S-P0353</td>\n",
       "      <td>Alexandria Lawrence</td>\n",
       "      <td>High</td>\n",
       "      <td>1.194030</td>\n",
       "      <td>Lawrence</td>\n",
       "      <td>Third</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>S</td>\n",
       "      <td>young adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>889</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Male</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.4542</td>\n",
       "      <td>True</td>\n",
       "      <td>C</td>\n",
       "      <td>...</td>\n",
       "      <td>T1-1912-C-P0085</td>\n",
       "      <td>Christopher Reed</td>\n",
       "      <td>Medium</td>\n",
       "      <td>1.798785</td>\n",
       "      <td>Reed</td>\n",
       "      <td>First</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>young adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>890</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>T3-1912-Q-P0072</td>\n",
       "      <td>James Horton</td>\n",
       "      <td>Low</td>\n",
       "      <td>4.129032</td>\n",
       "      <td>Horton</td>\n",
       "      <td>Third</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Q</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0  survived  pclass     sex  age  sibsp  parch     fare  \\\n",
       "0             0     False       3    Male   22      1      0   7.2500   \n",
       "1             1      True       1  female   38      1      0  71.2833   \n",
       "2             2      True       3  female   26      0      0   7.9250   \n",
       "3             3      True       1  female   35      1      0  53.1000   \n",
       "4             4     False       3    Male   35      0      0   8.0500   \n",
       "..          ...       ...     ...     ...  ...    ...    ...      ...   \n",
       "886         886     False       2    Male   27      0      0  13.0000   \n",
       "887         887      True       1  female   19      0      0  30.0000   \n",
       "888         888     False       3  female   28      1      2  23.4500   \n",
       "889         889      True       1    Male   26      0      0  14.4542   \n",
       "890         890     False       3    Male   32      0      0   7.7500   \n",
       "\n",
       "     adult_male cabin  ...      passengerId                 name   fare_bin  \\\n",
       "0          True   NaN  ...  T3-1912-S-P0001        Dustin Flores        Low   \n",
       "1         False     C  ...  T1-1912-C-P0001           Mary Tyler  Very High   \n",
       "2         False   NaN  ...  T3-1912-S-P0002         Anna Mendoza        Low   \n",
       "3         False     C  ...  T1-1912-S-P0001        Debra Pearson  Very High   \n",
       "4          True   NaN  ...  T3-1912-S-P0003       Anthony Taylor     Medium   \n",
       "..          ...   ...  ...              ...                  ...        ...   \n",
       "886        True   NaN  ...  T2-1912-S-P0164      Michael Johnson     Medium   \n",
       "887       False     B  ...  T1-1912-S-P0127            Lisa Buck  Very High   \n",
       "888       False   NaN  ...  T3-1912-S-P0353  Alexandria Lawrence       High   \n",
       "889        True     C  ...  T1-1912-C-P0085     Christopher Reed     Medium   \n",
       "890        True   NaN  ...  T3-1912-Q-P0072         James Horton        Low   \n",
       "\n",
       "    age_fare_ratio   surname   class is_child  family_size  embarked  \\\n",
       "0         3.034483    Flores   Third    False            2         S   \n",
       "1         0.533084     Tyler   First    False            2         C   \n",
       "2         3.280757   Mendoza   Third    False            1         S   \n",
       "3         0.659134   Pearson   First    False            2         S   \n",
       "4         4.347826    Taylor   Third    False            1         S   \n",
       "..             ...       ...     ...      ...          ...       ...   \n",
       "886       2.076923   Johnson  Second    False            1         S   \n",
       "887       0.633333      Buck   First    False            1         S   \n",
       "888       1.194030  Lawrence   Third    False            4         S   \n",
       "889       1.798785      Reed   First    False            1         C   \n",
       "890       4.129032    Horton   Third    False            1         Q   \n",
       "\n",
       "       age_group  \n",
       "0    young adult  \n",
       "1          adult  \n",
       "2    young adult  \n",
       "3          adult  \n",
       "4          adult  \n",
       "..           ...  \n",
       "886  young adult  \n",
       "887  young adult  \n",
       "888  young adult  \n",
       "889  young adult  \n",
       "890        adult  \n",
       "\n",
       "[891 rows x 21 columns]"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# be careful, do not overwrite your current dataframe variable\n",
    "df_loaded = pd.read_csv('cleaned_titanic.csv')\n",
    "df_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c34683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# that's why we usually dont save with the index set to True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04417830",
   "metadata": {},
   "source": [
    "**You also could just drop the artificial index column...** `df.drop(columns=['col'], inplace=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb53c33",
   "metadata": {},
   "source": [
    "### Q 4.3 Export the dataset to `csv`. Set `index=False` as an additional param."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "35b18fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_titanic.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b4ebe9",
   "metadata": {},
   "source": [
    "## 5. Final Clean Dataset\n",
    "\n",
    "### Load the Clean Dataset\n",
    "\n",
    "- **Test first with a different variable name to avoid overwriting by accident.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "2d3a7818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>passengerId</th>\n",
       "      <th>name</th>\n",
       "      <th>fare_bin</th>\n",
       "      <th>age_fare_ratio</th>\n",
       "      <th>surname</th>\n",
       "      <th>class</th>\n",
       "      <th>is_child</th>\n",
       "      <th>family_size</th>\n",
       "      <th>embarked</th>\n",
       "      <th>age_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>T3-1912-S-P0001</td>\n",
       "      <td>Dustin Flores</td>\n",
       "      <td>Low</td>\n",
       "      <td>3.034483</td>\n",
       "      <td>Flores</td>\n",
       "      <td>Third</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>S</td>\n",
       "      <td>young adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>T1-1912-C-P0001</td>\n",
       "      <td>Mary Tyler</td>\n",
       "      <td>Very High</td>\n",
       "      <td>0.533084</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>First</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>C</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>T3-1912-S-P0002</td>\n",
       "      <td>Anna Mendoza</td>\n",
       "      <td>Low</td>\n",
       "      <td>3.280757</td>\n",
       "      <td>Mendoza</td>\n",
       "      <td>Third</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>young adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>T1-1912-S-P0001</td>\n",
       "      <td>Debra Pearson</td>\n",
       "      <td>Very High</td>\n",
       "      <td>0.659134</td>\n",
       "      <td>Pearson</td>\n",
       "      <td>First</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>S</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>T3-1912-S-P0003</td>\n",
       "      <td>Anthony Taylor</td>\n",
       "      <td>Medium</td>\n",
       "      <td>4.347826</td>\n",
       "      <td>Taylor</td>\n",
       "      <td>Third</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>T2-1912-S-P0164</td>\n",
       "      <td>Michael Johnson</td>\n",
       "      <td>Medium</td>\n",
       "      <td>2.076923</td>\n",
       "      <td>Johnson</td>\n",
       "      <td>Second</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>young adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>T1-1912-S-P0127</td>\n",
       "      <td>Lisa Buck</td>\n",
       "      <td>Very High</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>Buck</td>\n",
       "      <td>First</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>young adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>T3-1912-S-P0353</td>\n",
       "      <td>Alexandria Lawrence</td>\n",
       "      <td>High</td>\n",
       "      <td>1.194030</td>\n",
       "      <td>Lawrence</td>\n",
       "      <td>Third</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>S</td>\n",
       "      <td>young adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.4542</td>\n",
       "      <td>True</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>T1-1912-C-P0085</td>\n",
       "      <td>Christopher Reed</td>\n",
       "      <td>Medium</td>\n",
       "      <td>1.798785</td>\n",
       "      <td>Reed</td>\n",
       "      <td>First</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>young adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>T3-1912-Q-P0072</td>\n",
       "      <td>James Horton</td>\n",
       "      <td>Low</td>\n",
       "      <td>4.129032</td>\n",
       "      <td>Horton</td>\n",
       "      <td>Third</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Q</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  pclass     sex  age  sibsp  parch     fare  adult_male deck  \\\n",
       "0       False       3    male   22      1      0   7.2500        True  NaN   \n",
       "1        True       1  female   38      1      0  71.2833       False    C   \n",
       "2        True       3  female   26      0      0   7.9250       False  NaN   \n",
       "3        True       1  female   35      1      0  53.1000       False    C   \n",
       "4       False       3    male   35      0      0   8.0500        True  NaN   \n",
       "..        ...     ...     ...  ...    ...    ...      ...         ...  ...   \n",
       "886     False       2    male   27      0      0  13.0000        True  NaN   \n",
       "887      True       1  female   19      0      0  30.0000       False    B   \n",
       "888     False       3  female   28      1      2  23.4500       False  NaN   \n",
       "889      True       1    male   26      0      0  14.4542        True    C   \n",
       "890     False       3    male   32      0      0   7.7500        True  NaN   \n",
       "\n",
       "     embark_town      passengerId                 name   fare_bin  \\\n",
       "0    Southampton  T3-1912-S-P0001        Dustin Flores        Low   \n",
       "1      Cherbourg  T1-1912-C-P0001           Mary Tyler  Very High   \n",
       "2    Southampton  T3-1912-S-P0002         Anna Mendoza        Low   \n",
       "3    Southampton  T1-1912-S-P0001        Debra Pearson  Very High   \n",
       "4    Southampton  T3-1912-S-P0003       Anthony Taylor     Medium   \n",
       "..           ...              ...                  ...        ...   \n",
       "886  Southampton  T2-1912-S-P0164      Michael Johnson     Medium   \n",
       "887  Southampton  T1-1912-S-P0127            Lisa Buck  Very High   \n",
       "888  Southampton  T3-1912-S-P0353  Alexandria Lawrence       High   \n",
       "889    Cherbourg  T1-1912-C-P0085     Christopher Reed     Medium   \n",
       "890   Queenstown  T3-1912-Q-P0072         James Horton        Low   \n",
       "\n",
       "     age_fare_ratio   surname   class  is_child  family_size embarked  \\\n",
       "0          3.034483    Flores   Third     False            2        S   \n",
       "1          0.533084     Tyler   First     False            2        C   \n",
       "2          3.280757   Mendoza   Third     False            1        S   \n",
       "3          0.659134   Pearson   First     False            2        S   \n",
       "4          4.347826    Taylor   Third     False            1        S   \n",
       "..              ...       ...     ...       ...          ...      ...   \n",
       "886        2.076923   Johnson  Second     False            1        S   \n",
       "887        0.633333      Buck   First     False            1        S   \n",
       "888        1.194030  Lawrence   Third     False            4        S   \n",
       "889        1.798785      Reed   First     False            1        C   \n",
       "890        4.129032    Horton   Third     False            1        Q   \n",
       "\n",
       "       age_group  \n",
       "0    young adult  \n",
       "1          adult  \n",
       "2    young adult  \n",
       "3          adult  \n",
       "4          adult  \n",
       "..           ...  \n",
       "886  young adult  \n",
       "887  young adult  \n",
       "888  young adult  \n",
       "889  young adult  \n",
       "890        adult  \n",
       "\n",
       "[891 rows x 20 columns]"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_loaded = pd.read_csv('cleaned_titanic.csv')\n",
    "df_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d688fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
