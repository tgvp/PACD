{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8ca0de4",
   "metadata": {},
   "source": [
    "## Week 5 ‚Äî Data Transformation\n",
    "\n",
    "In previous weeks, we learned how to load data, clean missing values, handle categorical variables,  \n",
    "perform aggregations, and extract meaningful insights.\n",
    "\n",
    "Now we move to a **new phase** that is part of modern workflows:\n",
    "\n",
    "> **Preparing the dataset for modeling using feature transformations.**\n",
    "\n",
    "This includes:\n",
    "- Review of Cleaning Techniques\n",
    "- Encoding categorical features\n",
    "- Scaling numerical features\n",
    "- Avoiding multicollinearity  \n",
    "\n",
    "**Dataset Reference:** üîó https://archive.ics.uci.edu/dataset/2/adult\n",
    "\n",
    "---\n",
    "## SETUP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7553e524",
   "metadata": {},
   "source": [
    "**Install the required package to load the dataset from UCI repository**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e763bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following line to install the required package\n",
    "#!pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02d90cf",
   "metadata": {},
   "source": [
    "**Import necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e90ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "adult = fetch_ucirepo(id=2)\n",
    "X = adult.data.features\n",
    "y = adult.data.targets\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8a4634",
   "metadata": {},
   "source": [
    "## 1. Load the Dataset\n",
    "\n",
    "> When loading the Adult Census Income dataset from the UCI repository, you will notice\n",
    "> that the data is split into **two separate DataFrames**:\n",
    ">\n",
    "> - `X` contains all feature columns (Attribute1 ‚Ä¶ Attribute20)\n",
    "> - `y` contains the target variable (`class`)\n",
    ">\n",
    "> This separation is common in Machine Learning libraries because it clearly\n",
    "> distinguishes:\n",
    ">\n",
    "> - **independent variables** ‚Üí used to make predictions  \n",
    "> - **dependent variable** ‚Üí the value we want to predict\n",
    ">\n",
    "> However, for **Exploratory Data Analysis (EDA)**, it is usually more convenient\n",
    "> to work with a **single unified table**.\n",
    ">\n",
    "> Having both features and the target in the same DataFrame simplifies:\n",
    ">\n",
    "> - inspecting the overall structure  \n",
    "> - checking distributions  \n",
    "> - computing correlations\n",
    "> - detecting missing values  \n",
    "> - visualizing relationships between variables\n",
    ">\n",
    "> To prepare for EDA, we will **concatenate** the two parts into one unified table.\n",
    ">\n",
    "> ### Concatenating DataFrames\n",
    ">\n",
    "> The simplest way to combine `X` and `y` is with `pd.concat`, which allows us to\n",
    "> join DataFrames **side-by-side** using `axis=1`:\n",
    ">\n",
    "> - `pd.concat([...])` ‚Üí specifies the DataFrames to combine  \n",
    "> - `axis=1` or `axis='columns'` ‚Üí concatenate **column-wise**, placing the\n",
    ">   target column next to the features  \n",
    ">\n",
    "> **Example:**\n",
    ">\n",
    "> ```python\n",
    "> df = pd.concat([df_1, df_2], axis=1)\n",
    ">\n",
    "> # or equivalently\n",
    ">\n",
    "> df = pd.concat([df_1, df_2], axis=\"columns\")\n",
    "> ```\n",
    ">\n",
    "> ### What about `axis=0` or `axis='rows'`?\n",
    ">\n",
    "> - This stacks DataFrames **row-wise**, one on top of the other.  \n",
    "> - It requires both DataFrames to have the **same columns**.  \n",
    "> - Therefore it is *not* appropriate for joining `X` and `y`.\n",
    "\n",
    "---\n",
    "\n",
    "### Q1.1 Verify both datasets which are separated DataFrames `X` and `y`, then concatenate them into one using `pd.concat`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fae537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd01f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473af762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13525f2",
   "metadata": {},
   "source": [
    "## Data Dictionary\n",
    ">\n",
    ">Below is the official data dictionary for the **Adult Census Income** dataset (UCI ML Repository).  \n",
    ">\n",
    ">Unlike the German Credit dataset, the Adult dataset already includes descriptive column names.\n",
    ">However, several columns contain coded categories, ambiguous meanings, or missing values masked as `\" ?\"`,  \n",
    ">which we will address in the cleaning and preprocessing stages.\n",
    ">\n",
    ">| Variable Name     | Role    | Type         | Demographic     | Description                                                                                               | Units | Missing Values |\n",
    ">|-------------------|---------|--------------|-----------------|-----------------------------------------------------------------------------------------------------------|--------|----------------|\n",
    ">| age               | Feature | Integer      | Age             | Age of the individual                                                                                     | years  | no             |\n",
    ">| workclass         | Feature | Categorical  | Income/Employment | Employment status (Private, Self-emp-not-inc, Federal-gov, ‚Ä¶ )                                           |        | yes (encoded as `\" ?\"`) |\n",
    ">| fnlwgt            | Feature | Integer      | ‚Äî               | Final sampling weight (used by US Census Bureau)                                                          |        | no             |\n",
    ">| education         | Feature | Categorical  | Education       | Highest level of education achieved (Bachelors, HS-grad, Some-college, ‚Ä¶)                                 |        | no             |\n",
    ">| education-num     | Feature | Integer      | Education       | Numerical representation of education level                                                                |        | no             |\n",
    ">| marital-status    | Feature | Categorical  | Other           | Marital status (Married, Divorced, Never-married, ‚Ä¶)                                                      |        | no             |\n",
    ">| occupation        | Feature | Categorical  | Employment      | Type of occupation (Tech-support, Sales, Exec-managerial, ‚Ä¶)                                              |        | yes (encoded as `\" ?\"`) |\n",
    ">| relationship      | Feature | Categorical  | Other           | Relationship of the individual to their household                                                          |        | no             |\n",
    ">| race              | Feature | Categorical  | Race            | Race group (White, Black, Asian-Pac-Islander, ‚Ä¶)                                                           |        | no             |\n",
    ">| sex               | Feature | Binary       | Sex             | Biological sex (Male, Female)                                                                              |        | no             |\n",
    ">| capital-gain      | Feature | Integer      | ‚Äî               | Capital gain from investment income                                                                        | USD    | no             |\n",
    ">| capital-loss      | Feature | Integer      | ‚Äî               | Capital loss from investment income                                                                        | USD    | no             |\n",
    ">| hours-per-week    | Feature | Integer      | Employment      | Working hours per week                                                                                     | hours  | no             |\n",
    ">| native-country    | Feature | Categorical  | Other           | Country of origin                                                                                          |        | yes (encoded as `\" ?\"`) |\n",
    ">| income            | Target  | Binary       | Income          | Income category: `<=50K` or `>50K`                                                                         |        | no             |\n",
    "---\n",
    "### Q1.2 Obtain the `.info()` from the Dataset:\n",
    "\n",
    ">Investigate the datatypes of each column. Are they appropriate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0f1b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c686704",
   "metadata": {},
   "source": [
    "### Q1.3 Obtain descriptive statistics using `.describe()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181f4b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa854d7",
   "metadata": {},
   "source": [
    "### Q1.4 Investigate how many missing values are in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1f9f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e576d62",
   "metadata": {},
   "source": [
    "## 2. Clean the Dataset\n",
    "\n",
    "Before we proceed any further, it is necessary to ensure that the dataset is **consistent** and **clean**.\n",
    "\n",
    "> Even though the Adult Income dataset already has descriptive columns,  \n",
    "> it still contains issues that must be addressed before transformation:\n",
    ">\n",
    "> - missing values\n",
    "> - categorical values with inconsistent spacing  \n",
    "> - mixed types inside categorical columns  \n",
    "> - skewed numerical variables  \n",
    "> - redundant or correlated columns\n",
    "___\n",
    "### Q2.1. Create a categorical list containing the columns of the appropriate dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26a553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e675a1b3",
   "metadata": {},
   "source": [
    "### Q2.2. Create a numerical list containing the columns of the appropriate dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ef9a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77096992",
   "metadata": {},
   "source": [
    "### Q2.3. For every categorical column verify the unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b91c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b282eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52a03f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bd1d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336aea86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3f7633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfc2814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bb271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10735caa",
   "metadata": {},
   "source": [
    "### Q2.4. Clean the `income` column:\n",
    "> - Use `df[\"col\"].str.replace(\".\", \"\", regex=False)`\n",
    "> - `regex=False` is to guarantee that you are not going to remove all characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b8d8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fc37a6",
   "metadata": {},
   "source": [
    "### Q2.5 Replace `\"?\"` with proper `NaN` in all categorical columns\n",
    "\n",
    "> As seen in the unique values above, several categorical columns use `\"?\"`  \n",
    "> instead of `NaN` to mark missing entries.\n",
    ">\n",
    "> Replace **all occurrences** of `\"?\"` with `np.nan`.\n",
    ">\n",
    ">**Example in specific column:**\n",
    ">```python\n",
    ">df['col'].replace(\"?\", np.nan, inplace=True)\n",
    ">```\n",
    ">**Example in the whole DataFrame:**\n",
    ">```python\n",
    ">df.replace(\"?\", np.nan, inplace=True)\n",
    ">```\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f8b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd7bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea990fa",
   "metadata": {},
   "source": [
    "### Q2.6. Create a `categoricals_with_nans` list containing categorical columns with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cb9f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1456c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57674b3",
   "metadata": {},
   "source": [
    "### Q2.7. Verify the proportion of each column from the list of categoricals with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32b125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e55677",
   "metadata": {},
   "source": [
    "### Q2.8. Verify the percentage of missing values in `categoricals_with_nans`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b3eaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faf8637",
   "metadata": {},
   "source": [
    "### Q2.9 Decide the imputation strategy for:\n",
    ">- `workclass`\n",
    ">- `occupation`\n",
    ">- `native-country`\n",
    ">\n",
    ">**Example of imputation with mode using `.fillna(df[col].mode()[0])`.**\n",
    ">```python\n",
    ">   df['col'] = df['col'].fillna(df['col'].mode()[0])\n",
    ">```\n",
    ">\n",
    ">**Example dropping rows with missing values:**\n",
    ">```python\n",
    ">   df = df.dropna(subset=[\"col1\", \"col2\", \"...\"])\n",
    ">```\n",
    ">\n",
    ">**Example of group-based imputation:**\n",
    ">```python\n",
    ">   def group_impute_mode(df, target_col, group_col):\n",
    ">       \"\"\"\n",
    ">       Impute missing values in `target_col` using the mode of each group defined by `group_col`.\n",
    ">       \"\"\"\n",
    ">       group_modes = {}\n",
    ">\n",
    ">       # Compute mode per group\n",
    ">       for group_value, group_df in df.groupby(group_col):\n",
    ">           mode_val = group_df[target_col].mode()[0]\n",
    ">           group_modes[group_value] = mode_val\n",
    ">\n",
    ">       # Apply the imputation group-wise\n",
    ">       for group_value, mode_val in group_modes.items():\n",
    ">           mask = (df[group_col] == group_value)\n",
    ">           df.loc[mask, target_col] = df.loc[mask, target_col].fillna(mode_val)\n",
    ">\n",
    ">       return df\n",
    ">```\n",
    "___\n",
    "**Run the following cell if you decide to use this function...**\n",
    "\n",
    "**You still need to decide the `target_col` and the `group_col`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acc0549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_impute_mode(df, target_col, group_col):\n",
    "    \"\"\"\n",
    "    Impute missing values in `target_col` using the mode of each group defined by `group_col`.\n",
    "    \"\"\"\n",
    "    group_modes = {}\n",
    "\n",
    "    # Compute mode per group\n",
    "    for group_value, group_df in df.groupby(group_col):\n",
    "        mode_val = group_df[target_col].mode()[0]\n",
    "        group_modes[group_value] = mode_val\n",
    "\n",
    "    # Apply the imputation group-wise\n",
    "    for group_value, mode_val in group_modes.items():\n",
    "        mask = (df[group_col] == group_value)\n",
    "        df.loc[mask, target_col] = df.loc[mask, target_col].fillna(mode_val)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Usage:\n",
    "# df = group_impute_mode(df, target_col=\"col_with_missing\", group_col=\"col_to_groupby\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f7069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0398751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ee5b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fc3a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddf8cd0",
   "metadata": {},
   "source": [
    "### ü§∑‚Äç‚ôÇÔ∏è As discussed in previous classes, all strategies have disadvantages and may introduce bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e15588f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c66a5194",
   "metadata": {},
   "source": [
    "## 3. Why Data Transformation?\n",
    ">\n",
    "> Raw datasets usually **cannot** be directly fed into models.\n",
    ">\n",
    "> **Many algorithms expect features to be:**\n",
    "> - on comparable scales  \n",
    "> - numerical  \n",
    "> - free of redundant or perfectly correlated dimensions  \n",
    "> - consistent between train and test sets  \n",
    ">\n",
    "> **In practice, real-world datasets contain:**\n",
    "> - variables measured in different units  \n",
    "> - skewed distributions  \n",
    "> - categorical labels  \n",
    "> - missing values  \n",
    "> - outliers  \n",
    ">\n",
    "> **Transformations help ensure:**\n",
    ">\n",
    ">- numerical stability  \n",
    ">- faster gradient convergence  \n",
    ">- better performance in distance-based models  \n",
    ">- reduced bias in parameter estimation  \n",
    ">- interpretability and model consistency  \n",
    ">- compatibility with modern pipelines  \n",
    "___\n",
    ">\n",
    ">## Label Encoding\n",
    ">\n",
    ">Label Encoding consists of assigning a **numeric code** to each category.\n",
    ">\n",
    ">**Example:**\n",
    ">\n",
    ">```python\n",
    ">   education_map = {\n",
    ">        \"Preschool\": 0,\n",
    ">        \"1st-4th\": 1,\n",
    ">        \"5th-6th\": 2,\n",
    ">        \"7th-8th\": 3,\n",
    ">        \"9th\": 4,\n",
    ">        \"10th\": 5,\n",
    ">        \"11th\": 6,\n",
    ">        \"12th\": 7,\n",
    ">        \"HS-grad\": 8,\n",
    ">        \"Some-college\": 9,\n",
    ">        \"Assoc-voc\": 10,\n",
    ">        \"Assoc-acdm\": 11,\n",
    ">        \"Bachelors\": 12,\n",
    ">        \"Masters\": 13,\n",
    ">        \"Doctorate\": 14,\n",
    ">        \"Prof-school\": 15\n",
    ">    }\n",
    ">\n",
    ">    df[\"education_encoded\"] = df[\"education\"].map(education_map)\n",
    ">```\n",
    ">When **Label Encoding** is appropriate?\n",
    "> - When a categorical feature has a natural order:\n",
    ">\n",
    ">   - Education level\n",
    ">\n",
    ">   - Satisfaction scores\n",
    ">\n",
    ">   - Risk levels\n",
    ">\n",
    ">   - Stage or level progressions\n",
    ">\n",
    ">In these cases, the assigned numbers represent rank, not category labels.\n",
    "___\n",
    "### Q3.1. Use the **mapping dictionary** below to `Label Encode` the `education` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1504678",
   "metadata": {},
   "outputs": [],
   "source": [
    "education_map = {\n",
    "    \"Preschool\": 0,\n",
    "    \"1st-4th\": 1,\n",
    "    \"5th-6th\": 2,\n",
    "    \"7th-8th\": 3,\n",
    "    \"9th\": 4,\n",
    "    \"10th\": 5,\n",
    "    \"11th\": 6,\n",
    "    \"12th\": 7,\n",
    "    \"HS-grad\": 8,\n",
    "    \"Some-college\": 9,\n",
    "    \"Assoc-voc\": 10,\n",
    "    \"Assoc-acdm\": 11,\n",
    "    \"Bachelors\": 12,\n",
    "    \"Masters\": 13,\n",
    "    \"Prof-school\": 14,\n",
    "    \"Doctorate\": 15\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf5ce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae480a98",
   "metadata": {},
   "source": [
    "## One-Hot Encoding (OHE)\n",
    "\n",
    "> After understanding Label Encoding, we now need a method that works for  \n",
    "> categorical variables **without natural order**.  \n",
    ">\n",
    "> Assigning integers to categories like `\"Red\" ‚Üí 0`, `\"Blue\" ‚Üí 1`, `\"Green\" ‚Üí 2`  \n",
    "> would incorrectly imply:\n",
    ">\n",
    "> - Blue > Red  \n",
    "> - Green > Blue  \n",
    ">\n",
    "> which creates a **fake ordering relationship** that does not exist.\n",
    ">\n",
    "> To avoid this problem, we use **One-Hot Encoding (OHE)**.\n",
    "___\n",
    "### What is One-Hot Encoding?\n",
    ">\n",
    "> One-Hot Encoding converts each category into a **binary indicator column**.\n",
    ">\n",
    "> Suppose the column **`color`** contains the following categories:\n",
    ">\n",
    "> - <span style=\"color:#D62828; font-weight:bold;\">Red</span>  \n",
    "> - <span style=\"color:#1D3557; font-weight:bold;\">Blue</span>  \n",
    "> - <span style=\"color:#2A9D8F; font-weight:bold;\">Green</span>  \n",
    ">\n",
    "> Using **One-Hot Encoding**, each category becomes its own variable.\n",
    ">\n",
    "> **Assume the input column `color`:**\n",
    ">\n",
    "> | color |\n",
    "> |-------|\n",
    "> | <span style=\"color:#D62828; font-weight:bold;\">Red</span> |\n",
    "> | <span style=\"color:#1D3557; font-weight:bold;\">Blue</span> |\n",
    "> | <span style=\"color:#2A9D8F; font-weight:bold;\">Green</span> |\n",
    ">\n",
    ">**Example using One-Hot Encoding with `pd.get_dummies()`**\n",
    ">\n",
    "> ```python\n",
    ">   df_encoded = pd.get_dummies(df, columns=list_of_columns, drop_first=True, dtype=type)\n",
    "> ```\n",
    ">\n",
    ">**Expected result:**\n",
    "> | color_Blue | color_Green |\n",
    "> |------------|--------------|\n",
    "> |     0      |      0       |\n",
    "> |     1      |      0       |\n",
    "> |     0      |      1       |\n",
    ">\n",
    "> **Why is the first category omitted?**\n",
    ">\n",
    "> Because dropping one dummy avoids the  <span style=\"color:#E76F51; font-weight:bold;\">dummy variable trap</span> caused by **perfect multicollinearity**, where:\n",
    ">\n",
    "> ```\n",
    "> Red = 1 ‚àí (Blue + Green)\n",
    "> ```\n",
    ">\n",
    "> **Now every row still uniquely identifies its color, without redundancy.**\n",
    "___\n",
    "### Q3.2. Create a DataFrame named `df_encoded` using `OHE` on the remaining categorical columns:\n",
    ">- Do not forget that `education` column must not be included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9af714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe6898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139306be",
   "metadata": {},
   "source": [
    "### Q3.3. Verify if the original columns are still in the DataFrame\n",
    ">- Use `occupation` column for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f6d402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abda5ea4",
   "metadata": {},
   "source": [
    "### Q3.4. Verify the number of columns after the `OHE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0b6927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39adffe",
   "metadata": {},
   "source": [
    "### Q3.5. Concatenate `df_encoded` with the `education` encoded column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9899000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0075ea92",
   "metadata": {},
   "source": [
    "### Q3.6 Verify if there are **outliers** in `numerical` columns using the `IQR method`\n",
    "\n",
    "> To detect outliers in a numerical column, we can use the **Interquartile Range (IQR) method**.\n",
    "> The IQR represents the spread of the middle 50% of the data.\n",
    ">\n",
    "> The formula works as follows:\n",
    ">\n",
    "> - Compute the 1st quartile (Q1) ‚Üí 25th percentile  \n",
    "> - Compute the 3rd quartile (Q3) ‚Üí 75th percentile  \n",
    "> - Compute the **IQR**:\n",
    ">\n",
    "> $$\n",
    "> \\text{IQR} = Q3 - Q1\n",
    "> $$\n",
    ">\n",
    "> Outliers are any observations outside the following bounds:\n",
    ">\n",
    "> $$\n",
    "> \\text{Lower Bound} = Q1 - 1.5 \\times \\text{IQR}\n",
    "> $$\n",
    "> $$\n",
    "> \\text{Upper Bound} = Q3 + 1.5 \\times \\text{IQR}\n",
    "> $$\n",
    ">\n",
    "> Values smaller than the lower bound or greater than the upper bound are considered **outliers**.\n",
    ">\n",
    "> Now, define a function that verifies whether a column contains outliers:\n",
    ">\n",
    ">```python\n",
    ">def verify_outliers(df: pd.DataFrame, col: str) -> bool:\n",
    ">    q1 = df[col].quantile(0.25)\n",
    ">    q3 = df[col].quantile(0.75)\n",
    ">    iqr = q3 - q1\n",
    ">    # continue from here\n",
    ">```\n",
    ">\n",
    ">Return a bool from the function and apply it on every `numerical` column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320d3ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_outliers(df: pd.DataFrame, col: str) -> bool:\n",
    "    q1 = df[col].quantile(0.25)\n",
    "    q3 = df[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    # continue from here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd4980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe8181f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><h3>Can we also use the IQR method to remove outliers?</h3></summary>\n",
    "\n",
    "> Yes, the same mathematical rule used to *detect* outliers can also be used\n",
    "> to *remove* them.  \n",
    ">\n",
    "> Once we compute the lower and upper bounds:\n",
    ">\n",
    "> $$\n",
    "> \\text{Lower Bound} = Q1 - 1.5 \\times \\text{IQR}\n",
    "> $$\n",
    "> $$\n",
    "> \\text{Upper Bound} = Q3 + 1.5 \\times \\text{IQR}\n",
    "> $$\n",
    ">\n",
    "> We can simply filter the DataFrame to keep only the values **within these limits**.\n",
    ">\n",
    "> This is known as **IQR-based outlier removal** and is one of the most common\n",
    "> preprocessing techniques in data cleaning, especially for algorithms that are \n",
    "> sensitive to extreme values.\n",
    ">\n",
    "> Example function to *remove* outliers from a column:\n",
    ">\n",
    "> ```python\n",
    "> def remove_outliers_iqr(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    ">     q1 = df[col].quantile(0.25)\n",
    ">     q3 = df[col].quantile(0.75)\n",
    ">     iqr = q3 - q1\n",
    ">\n",
    ">     lower = q1 - 1.5 * iqr\n",
    ">     upper = q3 + 1.5 * iqr\n",
    ">\n",
    ">     return df[(df[col] >= lower) & (df[col] <= upper)]\n",
    "> ```\n",
    ">\n",
    "___\n",
    "<div style=\"background-color:#f2f2f2; padding:12px; border-left:4px solid #d9534f; border-radius:4px; margin:10px 0;\">\n",
    "<strong>‚ö†Ô∏è NOTE:</strong> REMOVING OUTLIERS IS NOT ALWAYS RECOMMENDED.<br>\n",
    "It depends on the context and whether extreme values are real observations or measurement errors.<br>\n",
    "In credit scoring datasets like this one, outliers may represent important patterns of risk.\n",
    "</div>\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910dd8ef",
   "metadata": {},
   "source": [
    "### Transforming Numerical Features\n",
    ">\n",
    ">Before applying more advanced preprocessing techniques, we must ensure that our **numerical features are properly transformed**.\n",
    ">\n",
    ">**Raw numerical variables typically present issues such as:**\n",
    ">\n",
    ">- very different scales  \n",
    ">- extreme outliers  \n",
    ">- heavy-tailed shapes  \n",
    ">- inconsistent units  \n",
    ">\n",
    ">These problems can negatively impact both **data analysis** and **modeling**.\n",
    "___\n",
    "### Why Numerical Transformation Matters\n",
    ">\n",
    ">**Many algorithms assume that:**\n",
    ">\n",
    "> - features are on comparable scales  \n",
    "> - values are not dominated by extreme outliers  \n",
    "> - distributions are not extremely skewed  \n",
    "> - distances between samples are meaningful  \n",
    ">\n",
    ">**However, real-world datasets often include:**\n",
    ">\n",
    ">- **large scale differences**  \n",
    ">   - e.g., `capital-gain` ranges up to 100,000+  \n",
    ">   - e.g., `hours-per-week` ranges from 1 to ~60  \n",
    ">\n",
    ">- **strong skewness**  \n",
    ">   - e.g., most people have zero capital gain or loss\n",
    ">\n",
    ">\n",
    ">**Numerical transformations help ensure:**\n",
    ">\n",
    ">- numerical stability  \n",
    ">- consistency between features  \n",
    ">- reduction of bias introduced by outliers  \n",
    ">- improved interpretability  \n",
    ">- better suitability for downstream ML tasks  \n",
    "---\n",
    "### Standardization (Z-Score Scaling)\n",
    ">\n",
    "> Standardization rescales a feature so that it has:\n",
    ">\n",
    "> - mean = **0**  \n",
    "> - standard deviation = **1**\n",
    ">\n",
    ">**Mathematically:**\n",
    ">\n",
    ">$$\n",
    ">z = \\frac{x - \\mu}{\\sigma}\n",
    ">$$\n",
    ">\n",
    ">**Key characteristics:**\n",
    ">\n",
    ">- preserves the shape of the distribution  \n",
    ">- keeps outliers (does not remove them)  \n",
    ">- spreads values around zero  \n",
    ">- widely used in:  \n",
    ">  - Linear / Logistic Regression  \n",
    ">  - Neural Networks  \n",
    ">  - PCA  \n",
    ">  - SVM\n",
    ">\n",
    ">**Example:**\n",
    ">\n",
    ">```python\n",
    ">   def standardize(column: pd.Series) -> pd.Series:\n",
    ">      \"\"\"\n",
    ">      Applies Z-score standardization to a numerical column.\n",
    ">      (x - mean) / std\n",
    ">      \"\"\"\n",
    ">      mean = column.mean()\n",
    ">      std = column.std()\n",
    ">      return (column - mean) / std\n",
    ">\n",
    ">   df_standard = pd.DataFrame()\n",
    ">   for col in numericals:\n",
    ">       df_standard[col] = standardize(df[col])\n",
    ">\n",
    ">   df_standard.head()\n",
    ">```\n",
    ">\n",
    ">**Alternatively, you can use `sklearn.preprocessing.StandardScaler` for more efficient scaling.**\n",
    ">\n",
    ">```python\n",
    ">   from sklearn.preprocessing import StandardScaler\n",
    ">   scaler = StandardScaler()\n",
    ">   df_standard = pd.DataFrame(scaler.fit_transform(df[numericals]), columns=numericals)\n",
    ">```\n",
    "___\n",
    "### Q3.7. Create a new DataFrame named `df_standard` containing the standardized versions of the `numerical` columns\n",
    ">- `age` **-> Good candidate for scaling**\n",
    ">   - Moderate range (17‚Äì90)\n",
    ">   - No extreme outliers\n",
    "___\n",
    ">- `fnlwgt` **‚Üí Strong candidate for scaling**\n",
    ">   - Very large magnitude (20,000 to over 1,000,000)  \n",
    ">   - Extremely skewed and dominates the dataset if not scaled  \n",
    ">   - Standardization reduces magnitude-related bias  \n",
    "---\n",
    ">- `education-num` **‚Üí Do *NOT* standardize**\n",
    ">   - Encodes an **ordinal** variable (1 to 16)  \n",
    ">   - Small range and meaningful progression  \n",
    ">   - Scaling removes interpretability without adding value  \n",
    "---\n",
    ">- `capital-gain` **‚Üí Do *NOT* scale raw values**\n",
    ">   - 99% of observations are **zero**  \n",
    ">   - Very few extremely large values  \n",
    "---\n",
    ">- `capital-loss` **‚Üí Do *NOT* scale raw values**\n",
    ">  - Same skewness behavior as `capital-gain`  \n",
    ">  - Mostly zeros with rare large values\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6320058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659b2ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25506c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e42494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7602c63c",
   "metadata": {},
   "source": [
    "### Q3.8 Concatenate `df_standard` with the rest of the dataset (`df_encoded`) to create the final cleaned DataFrame named `df_final`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e3963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a09049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d66b6cc",
   "metadata": {},
   "source": [
    "### Q3.9 Export the dataset to a `csv` file as `cleaned_adult_income.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2e7913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146de505",
   "metadata": {},
   "source": [
    ">### Next class we will introduce `Preprocessing Pipelines` using `scikit-learn`\n",
    ">**This will allow us to combine all the steps above into a single reusable workflow.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
