{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f279fb15",
   "metadata": {},
   "source": [
    "# Week 4 - Data Wrangling and Group-Based Aggregations\n",
    "\n",
    "In this notebook we will practice data cleaning and group-based aggregations using a *messy* version of the german credit risk dataset.\n",
    "\n",
    "Dataset reference: üîó https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data\n",
    "\n",
    "Topics covered:\n",
    "- Concatenating DataFrames\n",
    "- Preprocessing\n",
    "    - Categorical x Numerical Data\n",
    "    - Fixing column types\n",
    "    - Standardizing Categorical Values\n",
    "    - Missing values (Identifying and Imputation)\n",
    "- Group-Based Aggregations\n",
    "\n",
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e90ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8a4634",
   "metadata": {},
   "source": [
    "## 1. Load the Dataset\n",
    "\n",
    "> When loading the German Credit dataset from the UCI repository, you will notice\n",
    "> that the data is split into **two separate DataFrames**:\n",
    ">\n",
    "> - `X` contains all feature columns (Attribute1 ‚Ä¶ Attribute20)\n",
    "> - `y` contains the target variable (`class`)\n",
    ">\n",
    "> This separation is common in Machine Learning libraries because it clearly\n",
    "> distinguishes:\n",
    ">\n",
    "> - **independent variables** ‚Üí used to make predictions  \n",
    "> - **dependent variable** ‚Üí the value we want to predict\n",
    ">\n",
    "> However, for **Exploratory Data Analysis (EDA)**, it is usually more convenient\n",
    "> to work with a **single unified table**.\n",
    ">\n",
    "> Having both features and the target in the same DataFrame simplifies:\n",
    ">\n",
    "> - inspecting the overall structure  \n",
    "> - checking distributions  \n",
    "> - computing correlations  \n",
    "> - detecting missing values  \n",
    "> - visualizing relationships between variables\n",
    ">\n",
    "> To prepare for EDA, we will **concatenate** the two parts into one unified table.\n",
    ">\n",
    "> ### Concatenating DataFrames\n",
    ">\n",
    "> The simplest way to combine `X` and `y` is with `pd.concat`, which allows us to\n",
    "> join DataFrames **side-by-side** using `axis=1`:\n",
    ">\n",
    "> - `pd.concat([...])` ‚Üí specifies the DataFrames to combine  \n",
    "> - `axis=1` or `axis='columns'` ‚Üí concatenate **column-wise**, placing the\n",
    ">   target column next to the features  \n",
    ">\n",
    "> **Example:**\n",
    ">\n",
    "> ```python\n",
    "> df = pd.concat([df_1, df_2], axis=1)\n",
    ">\n",
    "> # or equivalently\n",
    ">\n",
    "> df = pd.concat([df_1, df_2], axis=\"columns\")\n",
    "> ```\n",
    ">\n",
    "> ### What about `axis=0` or `axis='rows'`?\n",
    ">\n",
    "> - This stacks DataFrames **row-wise**, one on top of the other.  \n",
    "> - It requires both DataFrames to have the **same columns**.  \n",
    "> - Therefore it is *not* appropriate for joining `X` and `y`.\n",
    "\n",
    "---\n",
    "\n",
    "### Q1.1 Load both datasets in separate DataFrames `X` and `y`, then concatenate them into one using `pd.concat`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba60a20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd01f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473af762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13525f2",
   "metadata": {},
   "source": [
    "## Data Dictionary\n",
    ">\n",
    ">Below is the official data dictionary for the German Credit dataset.  \n",
    ">\n",
    ">Notice how the variables are originally labeled as `Attribute1`, `Attribute2`, ‚Ä¶ `Attribute20`.  \n",
    ">\n",
    ">Although this scheme preserves the order of the variables, it is **not descriptive**, which makes the dataset hard to read during analysis.\n",
    ">\n",
    ">| Variable Name | Role    | Type         | Demographic     | Description                                             | Units |\n",
    ">|---------------|---------|--------------|-----------------|---------------------------------------------------------|-------|\n",
    ">| Attribute1    | Feature | Categorical  |                 | Status of existing checking account                     |       |\n",
    ">| Attribute2    | Feature | Integer      |                 | Duration                                                | months|\n",
    ">| Attribute3    | Feature | Categorical  |                 | Credit history                                          |       |\n",
    ">| Attribute4    | Feature | Categorical  |                 | Purpose                                                 |       |\n",
    ">| Attribute5    | Feature | Integer      |                 | Credit amount                                           |       |\n",
    ">| Attribute6    | Feature | Categorical  |                 | Savings account/bonds                                   |       |\n",
    ">| Attribute7    | Feature | Categorical  | Other           | Present employment since                                |       |\n",
    ">| Attribute8    | Feature | Integer      |                 | Installment rate as % of disposable income              |       |\n",
    ">| Attribute9    | Feature | Categorical  | Marital Status  | Personal status and sex                                 |       |\n",
    ">| Attribute10   | Feature | Categorical  |                 | Other debtors / guarantors                              |       |\n",
    ">| Attribute11   | Feature | Integer      |                 | Present residence since                                 |       |\n",
    ">| Attribute12   | Feature | Categorical  |                 | Property owned                                          |       |\n",
    ">| Attribute13   | Feature | Integer      | Age             | Age                                                     | years |\n",
    ">| Attribute14   | Feature | Categorical  |                 | Other installment plans                                 |       |\n",
    ">| Attribute15   | Feature | Categorical  | Other           | Housing                                                 |       |\n",
    ">| Attribute16   | Feature | Integer      |                 | Number of existing credits at this bank                 |       |\n",
    ">| Attribute17   | Feature | Categorical  | Occupation      | Job                                                     |       |\n",
    ">| Attribute18   | Feature | Integer      |                 | Number of dependents                                    |       |\n",
    ">| Attribute19   | Feature | Binary       |                 | Telephone                                               |       |\n",
    ">| Attribute20   | Feature | Binary       | Other           | Foreign worker                                          |       |\n",
    ">| class         | Target  | Binary       |                 | 1 = Good, 2 = Bad                                       |       |\n",
    "---\n",
    "## 2. Clean the Data\n",
    "\n",
    "> You may have noticed that the column names are mostly **impractical for quick or direct analysis**.\n",
    ">\n",
    "> Labels like `Attribute3` or `Attribute14` do not convey meaning and force us to constantly consult the data dictionary.\n",
    ">\n",
    "> Before doing any EDA, it is important to assign **clear, consistent, and descriptive** column names.\n",
    "> This improves:\n",
    ">\n",
    "> - readability  \n",
    "> - visualization and plotting  \n",
    "> - correlation analysis  \n",
    "> - interpretability of models later on\n",
    "\n",
    "---\n",
    "\n",
    "### Q2. Use the dictionary below to rename all columns to meaningful, standardized names.\n",
    " \n",
    "- Apply it using `.rename(columns=...)` right after concatenating `X` and `y`.\n",
    "\n",
    "```python\n",
    "rename_dict = {\n",
    "    \"Attribute1\":  \"checking_status\",\n",
    "    \"Attribute2\":  \"duration_months\",\n",
    "    \"Attribute3\":  \"credit_history\",\n",
    "    \"Attribute4\":  \"purpose\",\n",
    "    \"Attribute5\":  \"credit_amount\",\n",
    "    \"Attribute6\":  \"savings_account\",\n",
    "    \"Attribute7\":  \"employment_since\",\n",
    "    \"Attribute8\":  \"installment_rate\",\n",
    "    \"Attribute9\":  \"personal_status_sex\",\n",
    "    \"Attribute10\": \"other_debtors\",\n",
    "    \"Attribute11\": \"residence_since\",\n",
    "    \"Attribute12\": \"property\",\n",
    "    \"Attribute13\": \"age\",\n",
    "    \"Attribute14\": \"other_installment_plans\",\n",
    "    \"Attribute15\": \"housing\",\n",
    "    \"Attribute16\": \"existing_credits\",\n",
    "    \"Attribute17\": \"job\",\n",
    "    \"Attribute18\": \"dependents\",\n",
    "    \"Attribute19\": \"telephone\",\n",
    "    \"Attribute20\": \"foreign_worker\",\n",
    "    \"class\":       \"credit_risk\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29daf1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {\n",
    "    \"Attribute1\":  \"checking_status\",\n",
    "    \"Attribute2\":  \"duration_months\",\n",
    "    \"Attribute3\":  \"credit_history\",\n",
    "    \"Attribute4\":  \"purpose\",\n",
    "    \"Attribute5\":  \"credit_amount\",\n",
    "    \"Attribute6\":  \"savings_account\",\n",
    "    \"Attribute7\":  \"employment_since\",\n",
    "    \"Attribute8\":  \"installment_rate\",\n",
    "    \"Attribute9\":  \"personal_status_sex\",\n",
    "    \"Attribute10\": \"other_debtors\",\n",
    "    \"Attribute11\": \"residence_since\",\n",
    "    \"Attribute12\": \"property\",\n",
    "    \"Attribute13\": \"age\",\n",
    "    \"Attribute14\": \"other_installment_plans\",\n",
    "    \"Attribute15\": \"housing\",\n",
    "    \"Attribute16\": \"existing_credits\",\n",
    "    \"Attribute17\": \"job\",\n",
    "    \"Attribute18\": \"dependents\",\n",
    "    \"Attribute19\": \"telephone\",\n",
    "    \"Attribute20\": \"foreign_worker\",\n",
    "    \"Attribute21\": \"months\",\n",
    "    \"Attribute22\": \"postal_area\",\n",
    "    \"class\":       \"credit_risk\"\n",
    "    \n",
    "}\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4ec77b",
   "metadata": {},
   "source": [
    "### Q2.1 Obtain the `.info()` from the Dataset:\n",
    "\n",
    ">Investigate the datatypes of each column. Are they appropriate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0f1b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c686704",
   "metadata": {},
   "source": [
    "### Q2.2 Obtain descriptive statistics using `.describe()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181f4b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa854d7",
   "metadata": {},
   "source": [
    "### Q2.3 Investigate how many missing values are in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1f9f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a0ee24",
   "metadata": {},
   "source": [
    "### Q2.4 Create `numerical` and `categorical` lists\n",
    "- you can check based on dtype (`'O'`) for object\n",
    "- you can also check using `df.select_dtypes(include=object)` or `np.number` for numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44382325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033eacdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b0682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b789701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea4b22f",
   "metadata": {},
   "source": [
    "### Q2.5 Standadize categorical columns\n",
    "\n",
    "- if possible, define strategies that could be used in columns with the same problem\n",
    "- if there are distinct problems, create lists containing a subset of columns with the same problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b93981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511309d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5931707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c4a6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97530ab4",
   "metadata": {},
   "source": [
    "### Q2.6 Verify the percentage of missing values in each `categorical` column:\n",
    "- if it's below `5%`, input the `Mode` (this may not be the best approach but we are cleaning the best we can with what we have learned so far)\n",
    "- if it's above `40%` drop the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704a6225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7635269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57812043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d845f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bdd72d",
   "metadata": {},
   "source": [
    "### Q2.7. Verify the percentage of missing values in each `numerical` column\n",
    "- if it's above `40%` drop the column\n",
    "- inpute the `mean`, `median` or `mode`, decide yourself which you are going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6726d4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b330611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f234565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0075ea92",
   "metadata": {},
   "source": [
    "### Q2.8 Verify if there are **outliers** in `numerical` columns using the `IQR method`\n",
    "\n",
    "> To detect outliers in a numerical column, we can use the **Interquartile Range (IQR) method**.\n",
    "> The IQR represents the spread of the middle 50% of the data.\n",
    ">\n",
    "> The formula works as follows:\n",
    ">\n",
    "> - Compute the 1st quartile (Q1) ‚Üí 25th percentile  \n",
    "> - Compute the 3rd quartile (Q3) ‚Üí 75th percentile  \n",
    "> - Compute the **IQR**:\n",
    ">\n",
    "> $$\n",
    "> \\text{IQR} = Q3 - Q1\n",
    "> $$\n",
    ">\n",
    "> Outliers are any observations outside the following bounds:\n",
    ">\n",
    "> $$\n",
    "> \\text{Lower Bound} = Q1 - 1.5 \\times \\text{IQR}\n",
    "> $$\n",
    "> $$\n",
    "> \\text{Upper Bound} = Q3 + 1.5 \\times \\text{IQR}\n",
    "> $$\n",
    ">\n",
    "> Values smaller than the lower bound or greater than the upper bound are considered **outliers**.\n",
    ">\n",
    "> Now, define a function that verifies whether a column contains outliers:\n",
    ">\n",
    ">```python\n",
    ">def verify_outliers(df: pd.DataFrame, col: str) -> bool:\n",
    ">    q1 = df[col].quantile(0.25)\n",
    ">    q3 = df[col].quantile(0.75)\n",
    ">    iqr = q3 - q1\n",
    ">    # continue from here\n",
    ">```\n",
    ">\n",
    ">Return a bool from the function and apply it on every `numerical` column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320d3ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_outliers(df: pd.DataFrame, col: str) -> bool:\n",
    "    q1 = df[col].quantile(0.25)\n",
    "    q3 = df[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    # continue from here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd4980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe8181f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><h3>Can we also use the IQR method to remove outliers?</h3></summary>\n",
    "\n",
    "> Yes, the same mathematical rule used to *detect* outliers can also be used\n",
    "> to *remove* them.  \n",
    ">\n",
    "> Once we compute the lower and upper bounds:\n",
    ">\n",
    "> $$\n",
    "> \\text{Lower Bound} = Q1 - 1.5 \\times \\text{IQR}\n",
    "> $$\n",
    "> $$\n",
    "> \\text{Upper Bound} = Q3 + 1.5 \\times \\text{IQR}\n",
    "> $$\n",
    ">\n",
    "> We can simply filter the DataFrame to keep only the values **within these limits**.\n",
    ">\n",
    "> This is known as **IQR-based outlier removal** and is one of the most common\n",
    "> preprocessing techniques in data cleaning, especially for algorithms that are \n",
    "> sensitive to extreme values.\n",
    ">\n",
    "> Example function to *remove* outliers from a column:\n",
    ">\n",
    "> ```python\n",
    "> def remove_outliers_iqr(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    ">     q1 = df[col].quantile(0.25)\n",
    ">     q3 = df[col].quantile(0.75)\n",
    ">     iqr = q3 - q1\n",
    ">\n",
    ">     lower = q1 - 1.5 * iqr\n",
    ">     upper = q3 + 1.5 * iqr\n",
    ">\n",
    ">     return df[(df[col] >= lower) & (df[col] <= upper)]\n",
    "> ```\n",
    ">\n",
    "___\n",
    "<div style=\"background-color:#f2f2f2; padding:12px; border-left:4px solid #d9534f; border-radius:4px; margin:10px 0;\">\n",
    "<strong>‚ö†Ô∏è NOTE:</strong> REMOVING OUTLIERS IS NOT ALWAYS RECOMMENDED.<br>\n",
    "It depends on the context and whether extreme values are real observations or measurement errors.<br>\n",
    "In credit scoring datasets like this one, outliers may represent important patterns of risk.\n",
    "</div>\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9d967e",
   "metadata": {},
   "source": [
    "### Q2.9 Any other problematic column?\n",
    "- Check for dtypes and duplicated information üòâ\n",
    "- Convert the columns and drop duplicated information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444c0d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6320058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041bfe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86485e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7602c63c",
   "metadata": {},
   "source": [
    "### Q2.10 Export the dataset to a `csv` file as `cleaned_credit_risk_dataset.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2e7913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b70e9f",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Before we continue with groupby-based exploration, it is important to notice that  \n",
    "many columns in the German Credit dataset contain *coded categorical values* such as:\n",
    "\n",
    "- `A11`, `A12`, `A13`, ‚Ä¶\n",
    "- `A30`, `A31`, ‚Ä¶\n",
    "- `A40`, `A41`, ‚Ä¶\n",
    "- `A171`, `A172`, ‚Ä¶\n",
    "\n",
    "These codes make the dataset harder to read and interpret during analysis.\n",
    "\n",
    "> This is extremely common in real datasets:\n",
    "> - data may come encoded for storage efficiency  \n",
    "> - documentation may be separate from the data  \n",
    "> - variables may need mapping tables to become understandable  \n",
    "\n",
    "To make our exploratory analysis clearer ‚Äî and to avoid constantly checking the data dictionary ‚Äî  \n",
    "we will now apply an explicit **mapping** from coded values to descriptive labels.\n",
    "\n",
    "The mappings below were created based on the official dataset documentation provided in:\n",
    "\n",
    "üîó https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b34bda",
   "metadata": {},
   "source": [
    ">**Install the library.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3344bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662b536d",
   "metadata": {},
   "source": [
    ">**Load the original dataset using the `ucimlrepo` library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f21f31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "# fetch dataset \n",
    "statlog_german_credit_data = fetch_ucirepo(id=144) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = statlog_german_credit_data.data.features \n",
    "y = statlog_german_credit_data.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(statlog_german_credit_data.metadata) \n",
    "  \n",
    "# variable information \n",
    "#print(statlog_german_credit_data.variables) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d41eae6",
   "metadata": {},
   "source": [
    ">**Renaming the columns with human-readable names.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9ad428",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {\n",
    "    \"Attribute1\":  \"checking_status\",\n",
    "    \"Attribute2\":  \"duration_months\",\n",
    "    \"Attribute3\":  \"credit_history\",\n",
    "    \"Attribute4\":  \"purpose\",\n",
    "    \"Attribute5\":  \"credit_amount\",\n",
    "    \"Attribute6\":  \"savings_account\",\n",
    "    \"Attribute7\":  \"employment_since\",\n",
    "    \"Attribute8\":  \"installment_rate\",\n",
    "    \"Attribute9\":  \"personal_status_sex\",\n",
    "    \"Attribute10\": \"other_debtors\",\n",
    "    \"Attribute11\": \"residence_since\",\n",
    "    \"Attribute12\": \"property\",\n",
    "    \"Attribute13\": \"age\",\n",
    "    \"Attribute14\": \"other_installment_plans\",\n",
    "    \"Attribute15\": \"housing\",\n",
    "    \"Attribute16\": \"existing_credits\",\n",
    "    \"Attribute17\": \"job\",\n",
    "    \"Attribute18\": \"dependents\",\n",
    "    \"Attribute19\": \"telephone\",\n",
    "    \"Attribute20\": \"foreign_worker\",\n",
    "    \"Attribute21\": \"months\",\n",
    "    \"Attribute22\": \"postal_area\",\n",
    "    \"class\":       \"credit_risk\"\n",
    "    \n",
    "}\n",
    "\n",
    "df.rename(columns=rename_dict, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee25cf63",
   "metadata": {},
   "source": [
    ">**Run the following block to replace the coded categorical values with human-readable descriptions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224a9aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------\n",
    "# SAVE ORIGINAL PERSONAL_STATUS_SEX CODES\n",
    "# (needed later to extract 'sex' and clean personal status)\n",
    "# -----------------------------------------\n",
    "df[\"personal_status_sex_code\"] = df[\"personal_status_sex\"].copy()\n",
    "\n",
    "# -----------------------------------------\n",
    "# MAPPINGS FOR QUALITATIVE VARIABLES\n",
    "# -----------------------------------------\n",
    "\n",
    "map_status = {\n",
    "    \"A11\": \"< 0 DM\",\n",
    "    \"A12\": \"0<=X<200 DM\",\n",
    "    \"A13\": \">=200 DM / salary assignments ‚â• 1 year\",\n",
    "    \"A14\": \"no checking account\"\n",
    "}\n",
    "\n",
    "map_history = {\n",
    "    \"A30\": \"no credits taken / all paid back duly\",\n",
    "    \"A31\": \"all credits at this bank paid back duly\",\n",
    "    \"A32\": \"existing credits paid back duly till now\",\n",
    "    \"A33\": \"delay in paying off in the past\",\n",
    "    \"A34\": \"critical account / other credits elsewhere\"\n",
    "}\n",
    "\n",
    "map_purpose = {\n",
    "    \"A40\": \"car (new)\",\n",
    "    \"A41\": \"car (used)\",\n",
    "    \"A42\": \"furniture/equipment\",\n",
    "    \"A43\": \"radio/television\",\n",
    "    \"A44\": \"domestic appliances\",\n",
    "    \"A45\": \"repairs\",\n",
    "    \"A46\": \"education\",\n",
    "    # A47 does not exist in the original dataset\n",
    "    \"A48\": \"retraining\",\n",
    "    \"A49\": \"business\",\n",
    "    \"A410\": \"others\"\n",
    "}\n",
    "\n",
    "map_savings = {\n",
    "    \"A61\": \"<100 DM\",\n",
    "    \"A62\": \"100<=X<500 DM\",\n",
    "    \"A63\": \"500<=X<1000 DM\",\n",
    "    \"A64\": \">=1000 DM\",\n",
    "    \"A65\": \"unknown/no savings\"\n",
    "}\n",
    "\n",
    "map_employment = {\n",
    "    \"A71\": \"unemployed\",\n",
    "    \"A72\": \"<1 year\",\n",
    "    \"A73\": \"1‚Äì4 years\",\n",
    "    \"A74\": \"4‚Äì7 years\",\n",
    "    \"A75\": \">=7 years\"\n",
    "}\n",
    "\n",
    "# Combined personal status + sex text\n",
    "map_personal_status_sex = {\n",
    "    \"A91\": \"male: divorced/separated\",\n",
    "    \"A92\": \"female: divorced/separated/married\",\n",
    "    \"A93\": \"male: single\",\n",
    "    \"A94\": \"male: married/widowed\",\n",
    "    \"A95\": \"female: single\"\n",
    "}\n",
    "\n",
    "map_debtors = {\n",
    "    \"A101\": \"none\",\n",
    "    \"A102\": \"co-applicant\",\n",
    "    \"A103\": \"guarantor\"\n",
    "}\n",
    "\n",
    "map_property = {\n",
    "    \"A121\": \"real estate\",\n",
    "    \"A122\": \"building society savings/life insurance\",\n",
    "    \"A123\": \"car or other (not in savings)\",\n",
    "    \"A124\": \"unknown/no property\"\n",
    "}\n",
    "\n",
    "map_installment_plans = {\n",
    "    \"A141\": \"bank\",\n",
    "    \"A142\": \"stores\",\n",
    "    \"A143\": \"none\"\n",
    "}\n",
    "\n",
    "map_housing = {\n",
    "    \"A151\": \"rent\",\n",
    "    \"A152\": \"own\",\n",
    "    \"A153\": \"for free\"\n",
    "}\n",
    "\n",
    "map_job = {\n",
    "    \"A171\": \"unemployed/unskilled ‚Äì non-resident\",\n",
    "    \"A172\": \"unskilled ‚Äì resident\",\n",
    "    \"A173\": \"skilled employee/official\",\n",
    "    \"A174\": \"management/self-employed/highly qualified\"\n",
    "}\n",
    "\n",
    "map_telephone = {\n",
    "    \"A191\": \"none\",\n",
    "    \"A192\": \"yes, registered\"\n",
    "}\n",
    "\n",
    "map_foreign = {\n",
    "    \"A201\": \"yes\",\n",
    "    \"A202\": \"no\"\n",
    "}\n",
    "\n",
    "# -----------------------------------------\n",
    "# APPLY MAPPINGS TO THE DATAFRAME\n",
    "# -----------------------------------------\n",
    "\n",
    "df = df.replace({\n",
    "    \"status\": map_status,\n",
    "    \"credit_history\": map_history,\n",
    "    \"purpose\": map_purpose,\n",
    "    \"savings\": map_savings,\n",
    "    \"present_employment\": map_employment,\n",
    "    \"personal_status_sex\": map_personal_status_sex,  # human-readable combined field\n",
    "    \"other_debtors\": map_debtors,\n",
    "    \"property\": map_property,\n",
    "    \"other_installment_plans\": map_installment_plans,\n",
    "    \"housing\": map_housing,\n",
    "    \"job\": map_job,\n",
    "    \"telephone\": map_telephone,\n",
    "    \"foreign_worker\": map_foreign\n",
    "})\n",
    "\n",
    "# -----------------------------------------\n",
    "# SPLIT personal_status_sex INTO 'sex' AND CLEAN 'personal_status'\n",
    "# (using the original codes saved in personal_status_sex_code)\n",
    "# -----------------------------------------\n",
    "\n",
    "# Mapping to extract sex only\n",
    "map_sex = {\n",
    "    \"A91\": \"male\",\n",
    "    \"A92\": \"female\",\n",
    "    \"A93\": \"male\",\n",
    "    \"A94\": \"male\",\n",
    "    \"A95\": \"female\"\n",
    "}\n",
    "\n",
    "# Mapping to extract civil/marital status only\n",
    "map_personal_status_clean = {\n",
    "    \"A91\": \"divorced/separated\",\n",
    "    \"A92\": \"divorced/separated/married\",\n",
    "    \"A93\": \"single\",\n",
    "    \"A94\": \"married/widowed\",\n",
    "    \"A95\": \"single\"\n",
    "}\n",
    "\n",
    "# Create the new 'sex' column\n",
    "df[\"sex\"] = df[\"personal_status_sex_code\"].map(map_sex)\n",
    "\n",
    "# Create a new 'personal_status' column with only civil status\n",
    "df[\"personal_status\"] = df[\"personal_status_sex_code\"].map(map_personal_status_clean)\n",
    "\n",
    "# Drop the temporary code column\n",
    "df.drop(columns=[\"personal_status_sex_code\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b76460",
   "metadata": {},
   "source": [
    "## 3. Introduction to `groupby()` for Exploratory Analysis\n",
    "\n",
    "> Until now, we have used methods such as `value_counts()`, `mean()`, or `describe()`  \n",
    "> to inspect columns individually.\n",
    ">\n",
    "> However, real datasets often have **subgroups** that behave differently, and we may want\n",
    "> to understand how a variable behaves *inside* each subgroup.\n",
    ">\n",
    "> For this, Pandas provides the command:\n",
    ">\n",
    "> `df.groupby(\"column\")`\n",
    ">\n",
    "> which splits the dataset into smaller groups based on the values of one column.\n",
    ">\n",
    "> Each group can then be inspected separately.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.1 Counting Values Inside Groups ‚Äî `.groupby().value_counts()`\n",
    "\n",
    "> This tells us **how a categorical variable behaves inside each subgroup**.\n",
    ">\n",
    ">**Example:**\n",
    ">\n",
    ">```python\n",
    ">   df.groupby(\"housing\")[\"checking_status\"].value_counts()\n",
    ">```\n",
    "\n",
    "### Q3.1. Inspect Categorical Distributions Inside Groups\n",
    "\n",
    "- Using `.groupby('col1')['related_col'].value_counts()`, compute how the column\n",
    "`personal_status` is distributed inside each `credit_risk` group.\n",
    "\n",
    "- Your output should show, **for each value** of `credit_risk`,\n",
    "**the count of each category** in `personal_status`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0869b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff653220",
   "metadata": {},
   "source": [
    "### Q3.2 Inspect Categorical Distributions Inside Sub-Groups\n",
    "\n",
    "- Using `.groupby(['col1', 'col2'])['related_col'].value_counts()`, compute how the column\n",
    "`personal_status` is distributed across each `sex` category within each `credit_risk` group.\n",
    "\n",
    "- The output should display, for every value of credit_risk, the count of each category in personal_status, separated by sex.\n",
    "\n",
    ">**Keep in mind that as we add more grouping columns, the resulting output becomes less intuitive to read.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15cb3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84356ab2",
   "metadata": {},
   "source": [
    "### Q3.3 Compare the distribution of `housing` Inside each `credit_risk` group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d1082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b369d3e7",
   "metadata": {},
   "source": [
    "### Q3.4 Which professionals category have the highest average credit amount?\n",
    "\n",
    "- For each `job` category, compute the mean of `credit_amount`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec1de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfe6474",
   "metadata": {},
   "source": [
    "### Q3.5. Inspect `age` statistics inside each `credit_risk` group\n",
    "- Compute descriptive statistics (`.describe()`) for the column `age` inside each `credit_risk` group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23f41c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5105bbe0",
   "metadata": {},
   "source": [
    "### Q3.6. Create an `age` **binning column** (`age_group`) to explore group statistics\n",
    "\n",
    "\n",
    ">**Remember that we can create bins using `cut` like:**\n",
    ">\n",
    ">```python\n",
    ">   bins = [0, 25, 40, 60, 120]   # interval limits\n",
    ">   labels = [\"<25\", \"25‚Äì40\", \"40‚Äì60\", \"60+\"]  # names of the age groups\n",
    ">\n",
    ">   df[\"age_bin\"] = pd.cut(df[\"col\"], bins=bins, labels=labels)\n",
    ">```\n",
    ">\n",
    ">**Using `qcut` to split into equal `n` parts like:**\n",
    ">\n",
    ">```python\n",
    ">   df[\"age_bin_q\"] = pd.qcut(df[\"col\"], q=n, labels=[\"Q1\", \"Q2\", \"Q3\", ..., \"QN\"])\n",
    ">```\n",
    "\n",
    "- **We want meaningful age groups such as (e.g., `<25`, `25‚Äì40`, `40‚Äì60`, `60+`).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c16a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3416337f",
   "metadata": {},
   "source": [
    "### Q3.7. Compare credit risk across age groups\n",
    "\n",
    "- Using the `age_group` created in the previous question, analyze how `credit_risk` is distributed inside each age group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b8f5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66a93da",
   "metadata": {},
   "source": [
    "### Q3.8 Compute the percentage of bad credit risk per age Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36cd4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b82c54d",
   "metadata": {},
   "source": [
    "### Q3.9 Based on the previous question answer, younger or older customers are more likely to have good or bad credit risk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fb32db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014c38aa",
   "metadata": {},
   "source": [
    "### Q3.10 Compare the average credit amount across age Groups\n",
    "\n",
    "- Compute the **mean** value of `credit_amount` for each age group.\n",
    "- Which age group tends to request the highest credit amounts?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b328f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc9d4b7",
   "metadata": {},
   "source": [
    "### Q3.11 Compare Employment Duration Across Age Groups\n",
    "\n",
    "- Compute the count of each `employment_since` category inside each `age_group`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe12b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15198a6b",
   "metadata": {},
   "source": [
    "### Q3.12 Explore Purpose of Credit Within Age Groups\n",
    "\n",
    "- For each `age_group`, compute how many people requested credit for each type of `purpose`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ad446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93817bbf",
   "metadata": {},
   "source": [
    "### Q3.13 Number of Existing Credits by Age Group\n",
    "- determine which age group tends to have more existing credit lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53670733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeb69e2",
   "metadata": {},
   "source": [
    "### Q3.14 Cross-Analyze Age Groups and Housing\n",
    "\n",
    "- For each age_group, compute how many people fall into each housing category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3f7cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a0d146",
   "metadata": {},
   "source": [
    "## 3.2 Aggregating Multiple Statistics with `.agg()`\n",
    "\n",
    "> Until now, we have computed one summary statistic at a time:\n",
    ">\n",
    "> - `.mean()`\n",
    "> - `.size()`\n",
    "> - `.value_counts()`\n",
    "> - `.describe()`\n",
    ">\n",
    "> These methods are useful, but they only compute **one metric at a time**.\n",
    ">\n",
    "> The real power of `groupby()` comes when we want to calculate **several statistics at once**,  \n",
    "> either for:\n",
    ">\n",
    "> - the **same column**, or  \n",
    "> - **multiple columns** with different metrics.\n",
    ">\n",
    "> For this, Pandas provides the method:\n",
    ">\n",
    "> ```python\n",
    "> df.groupby(\"column\").agg({...})\n",
    "> ```\n",
    ">\n",
    "> which allows us to define exactly **which statistics** to compute.\n",
    "---\n",
    ">\n",
    ">**Example 1 ‚Äî Multiple Statistics for One Column**\n",
    ">\n",
    ">```python\n",
    ">   df.groupby(\"age_group\")[\"credit_amount\"].agg([\"mean\", \"median\", \"max\"])\n",
    ">```\n",
    "___\n",
    ">**Example 2 ‚Äî Different Statistics for Different Columns**\n",
    ">```python\n",
    ">   df.groupby(\"age_bin\").agg({\n",
    ">    \"credit_amount\": [\"mean\", \"std\"],\n",
    ">    \"duration_months\": [\"mean\", \"max\"]\n",
    ">})\n",
    ">```\n",
    "___\n",
    "> **Example 3 ‚Äî Using Custom Functions Inside `.agg()`**\n",
    ">\n",
    "> You can also define your own functions and use them directly inside `.agg()`.\n",
    ">\n",
    "> This is extremely useful when the standard statistics (`mean`, `median`, etc.) are not enough for your analysis.\n",
    ">\n",
    "> ```python\n",
    "> # Custom function: range = max - min\n",
    "> def value_range(series):\n",
    ">     return series.max() - series.min()\n",
    ">\n",
    "> df.groupby(\"age_group\")[\"credit_amount\"].agg([\n",
    ">     \"mean\",\n",
    ">     \"median\",\n",
    ">     value_range,     # custom function\n",
    "> ])\n",
    "> ```\n",
    ">\n",
    "> **This will return a table containing:**\n",
    "> - the mean  \n",
    "> - the median  \n",
    "> - and your custom-defined \"range\" metric  \n",
    ">\n",
    "> computed separately **for each age group**.\n",
    "___\n",
    ">**This approach is very common because it allows you to summarize multiple variables at once, grouped by a meaningful category**\n",
    "\n",
    "### Q3.15. Multiple Statistics for `credit_amount` per age group\n",
    "\n",
    "- Using `.groupby(\"age_group\")` and `.agg()`, compute the following statistics for `credit_amount` inside each `age_group`:\n",
    "\n",
    "    - mean  \n",
    "    - median\n",
    "    - maximum value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76638874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76a9107",
   "metadata": {},
   "source": [
    "### Q3.16. Aggregate Two Numerical Columns at Once\n",
    "\n",
    "- Using .groupby(\"age_group\"), compute:\n",
    "\n",
    "    - mean and standard deviation of credit_amount\n",
    "\n",
    "    - mean and max of duration_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f0daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb6606d",
   "metadata": {},
   "source": [
    "### Q3.17 Define your own function that computes the range of a numeric variable.\n",
    "- Using `.groupby(\"age_group\")[\"credit_amount\"].agg([...])`, compute:\n",
    "\n",
    "    - mean\n",
    "    - median\n",
    "    - your custom range function\n",
    "\n",
    ">**Example**\n",
    ">```python\n",
    ">   # Custom function: range = max - min\n",
    ">   def value_range(series: pd.Series):\n",
    ">       # your code here\n",
    ">```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0676476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function: range = max - min\n",
    "def value_range(series):\n",
    "    pass\n",
    "    # continue from here removing the pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d33965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your tears here üòä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a495aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your tears here üòä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca45ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your tears here üòä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45048b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your tears here üòä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a756e451",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
