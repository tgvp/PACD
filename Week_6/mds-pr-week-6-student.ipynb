{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c8ca0de4",
      "metadata": {
        "id": "c8ca0de4"
      },
      "source": [
        "## Week 6 â€” Preprocessing Pipelines\n",
        "\n",
        "In previous weeks, we learned how to load data, clean missing values, handle categorical variables,  \n",
        "perform aggregations, and transform the data.\n",
        "\n",
        "Now we move to a **new phase** that is part of modern workflows:\n",
        "\n",
        "> **Preparing the dataset for modeling using feature transformations with preprocessing pipelines.**\n",
        "\n",
        "This includes:\n",
        "- Review of Cleaning Techniques, encoding categorical features and scaling of numerical features\n",
        "- Preprocessing Pipelines using `sklearn.pipeline`  \n",
        "\n",
        "**Dataset Reference:** ðŸ”— https://archive.ics.uci.edu/dataset/2/adult\n",
        "\n",
        "---\n",
        "## SETUP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7553e524",
      "metadata": {
        "id": "7553e524"
      },
      "source": [
        "**Install the required package to load the dataset from UCI repository**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1906b3b0",
      "metadata": {
        "id": "1906b3b0"
      },
      "outputs": [],
      "source": [
        "# uncomment the following line to install the required package\n",
        "#!pip install ucimlrepo\n",
        "#!pip install IPython\n",
        "#!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e02d90cf",
      "metadata": {
        "id": "e02d90cf"
      },
      "source": [
        "**Import necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4e90ab3",
      "metadata": {
        "id": "e4e90ab3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display, HTML\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "adult = fetch_ucirepo(id=2)\n",
        "X = adult.data.features\n",
        "y = adult.data.targets\n",
        "\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6366bd4",
      "metadata": {
        "id": "c6366bd4"
      },
      "source": [
        "**Import pipeline related classes from `sklearn`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b384618a",
      "metadata": {
        "id": "b384618a"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.utils import estimator_html_repr"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e8a4634",
      "metadata": {
        "id": "6e8a4634"
      },
      "source": [
        "## 1. Load the Dataset\n",
        "\n",
        "> When loading the Adult Census Income dataset from the UCI repository, you will notice\n",
        "> that the data is split into **two separate DataFrames**:\n",
        ">\n",
        "> - `X` contains all feature columns (Attribute1 â€¦ Attribute20)\n",
        "> - `y` contains the target variable (`class`)\n",
        ">\n",
        "> This separation is common in Machine Learning libraries because it clearly\n",
        "> distinguishes:\n",
        ">\n",
        "> - **independent variables** â†’ used to make predictions  \n",
        "> - **dependent variable** â†’ the value we want to predict\n",
        ">\n",
        "> However, for **Exploratory Data Analysis (EDA)**, it is usually more convenient\n",
        "> to work with a **single unified table**.\n",
        ">\n",
        "> Having both features and the target in the same DataFrame simplifies:\n",
        ">\n",
        "> - inspecting the overall structure  \n",
        "> - checking distributions  \n",
        "> - computing correlations\n",
        "> - detecting missing values  \n",
        "> - visualizing relationships between variables\n",
        ">\n",
        "> To prepare for EDA, we will **concatenate** the two parts into one unified table.\n",
        ">\n",
        "> ### Concatenating DataFrames\n",
        ">\n",
        "> The simplest way to combine `X` and `y` is with `pd.concat`, which allows us to\n",
        "> join DataFrames **side-by-side** using `axis=1`:\n",
        ">\n",
        "> - `pd.concat([...])` â†’ specifies the DataFrames to combine  \n",
        "> - `axis=1` or `axis='columns'` â†’ concatenate **column-wise**, placing the\n",
        ">   target column next to the features  \n",
        ">\n",
        "> **Example:**\n",
        ">\n",
        "> ```python\n",
        "> df = pd.concat([df_1, df_2], axis=1)\n",
        ">\n",
        "> # or equivalently\n",
        ">\n",
        "> df = pd.concat([df_1, df_2], axis=\"columns\")\n",
        "> ```\n",
        ">\n",
        "> ### What about `axis=0` or `axis='rows'`?\n",
        ">\n",
        "> - This stacks DataFrames **row-wise**, one on top of the other.  \n",
        "> - It requires both DataFrames to have the **same columns**.  \n",
        "> - Therefore it is *not* appropriate for joining `X` and `y`.\n",
        "\n",
        "---\n",
        "\n",
        "### Q1.1 Verify both datasets which are separated DataFrames `X` and `y`, then concatenate them into one using `pd.concat`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fae537f",
      "metadata": {
        "id": "2fae537f"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edd01f8a",
      "metadata": {
        "id": "edd01f8a"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "473af762",
      "metadata": {
        "id": "473af762"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a13525f2",
      "metadata": {
        "id": "a13525f2"
      },
      "source": [
        "## Data Dictionary\n",
        ">\n",
        ">Below is the official data dictionary for the **Adult Census Income** dataset (UCI ML Repository).  \n",
        ">\n",
        ">Unlike the German Credit dataset, the Adult dataset already includes descriptive column names.\n",
        ">However, several columns contain coded categories, ambiguous meanings, or missing values masked as `\" ?\"`,  \n",
        ">which we will address in the cleaning and preprocessing stages.\n",
        ">\n",
        ">| Variable Name     | Role    | Type         | Demographic     | Description                                                                                               | Units | Missing Values |\n",
        ">|-------------------|---------|--------------|-----------------|-----------------------------------------------------------------------------------------------------------|--------|----------------|\n",
        ">| age               | Feature | Integer      | Age             | Age of the individual                                                                                     | years  | no             |\n",
        ">| workclass         | Feature | Categorical  | Income/Employment | Employment status (Private, Self-emp-not-inc, Federal-gov, â€¦ )                                           |        | yes (encoded as `\" ?\"`) |\n",
        ">| fnlwgt            | Feature | Integer      | â€”               | Final sampling weight (used by US Census Bureau)                                                          |        | no             |\n",
        ">| education         | Feature | Categorical  | Education       | Highest level of education achieved (Bachelors, HS-grad, Some-college, â€¦)                                 |        | no             |\n",
        ">| education-num     | Feature | Integer      | Education       | Numerical representation of education level                                                                |        | no             |\n",
        ">| marital-status    | Feature | Categorical  | Other           | Marital status (Married, Divorced, Never-married, â€¦)                                                      |        | no             |\n",
        ">| occupation        | Feature | Categorical  | Employment      | Type of occupation (Tech-support, Sales, Exec-managerial, â€¦)                                              |        | yes (encoded as `\" ?\"`) |\n",
        ">| relationship      | Feature | Categorical  | Other           | Relationship of the individual to their household                                                          |        | no             |\n",
        ">| race              | Feature | Categorical  | Race            | Race group (White, Black, Asian-Pac-Islander, â€¦)                                                           |        | no             |\n",
        ">| sex               | Feature | Binary       | Sex             | Biological sex (Male, Female)                                                                              |        | no             |\n",
        ">| capital-gain      | Feature | Integer      | â€”               | Capital gain from investment income                                                                        | USD    | no             |\n",
        ">| capital-loss      | Feature | Integer      | â€”               | Capital loss from investment income                                                                        | USD    | no             |\n",
        ">| hours-per-week    | Feature | Integer      | Employment      | Working hours per week                                                                                     | hours  | no             |\n",
        ">| native-country    | Feature | Categorical  | Other           | Country of origin                                                                                          |        | yes (encoded as `\" ?\"`) |\n",
        ">| income            | Target  | Binary       | Income          | Income category: `<=50K` or `>50K`                                                                         |        | no             |\n",
        "---\n",
        "### Q1.2 Obtain the `.info()` from the Dataset:\n",
        "\n",
        ">Investigate the datatypes of each column. Are they appropriate?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f0f1b46",
      "metadata": {
        "id": "0f0f1b46"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c686704",
      "metadata": {
        "id": "6c686704"
      },
      "source": [
        "### Q1.3 Obtain descriptive statistics using `.describe()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "181f4b06",
      "metadata": {
        "id": "181f4b06"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fa854d7",
      "metadata": {
        "id": "0fa854d7"
      },
      "source": [
        "### Q1.4 Investigate how many missing values are in each column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad1f9f33",
      "metadata": {
        "id": "ad1f9f33"
      },
      "outputs": [],
      "source": [
        "# you code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c6bed5f",
      "metadata": {
        "id": "2c6bed5f"
      },
      "source": [
        ">## 2. Preprocessing Pipelines\n",
        ">\n",
        ">### Why use preprocessing pipelines?\n",
        ">\n",
        ">In the previous class you cleaned the dataset **manually**, step by step.\n",
        ">\n",
        ">In this section, we will **recreate the same cleaning logic** using scikit-learn pipelines so that:\n",
        ">\n",
        ">- The preprocessing steps are **reusable** and **reproducible**.\n",
        ">- You can apply the **same transformations** to any new data in the future.\n",
        ">- All cleaning logic is kept in **one single object** instead of many scattered lines of code.\n",
        ">\n",
        ">We will use:\n",
        ">\n",
        ">- `Pipeline`\n",
        ">- `ColumnTransformer`\n",
        ">- `SimpleImputer`\n",
        ">- `StandardScaler`\n",
        ">- `OneHotEncoder`\n",
        "---\n",
        ">### Identify numerical and categorical columns for the pipelines\n",
        ">\n",
        ">We first separate the feature names into:\n",
        ">\n",
        ">- `numeric_features` â€“ columns treated as **numerical**.\n",
        ">- `categorical_features` â€“ columns treated as **categorical**.\n",
        ">\n",
        ">**Example:**\n",
        ">\n",
        ">```python\n",
        ">numeric_features = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist() # or use include=np.number\n",
        ">categorical_features = df.select_dtypes(include=[\"object\"]).columns.tolist() # or also include=[\"object\", \"category\", \"bool\"]\n",
        ">```\n",
        "---\n",
        "### Q2.1. Create the `numeric_features` and `categorical_features` lists\n",
        "- use the `.select_dtypes()` method exactly as shown above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a5f8ddd",
      "metadata": {
        "id": "3a5f8ddd"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d32461c",
      "metadata": {
        "id": "9d32461c"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67457685",
      "metadata": {
        "id": "67457685"
      },
      "source": [
        ">### Build the preprocessing pipeline for numerical features\n",
        ">\n",
        ">For numerical columns, a common preprocessing flow is:\n",
        ">\n",
        ">1. **Impute missing values** (e.g. with the median).\n",
        ">2. **Scale the values** to have mean 0 and variance 1.\n",
        ">\n",
        ">We can capture this logic in a `Pipeline`:\n",
        ">\n",
        ">```python\n",
        ">from sklearn.pipeline import Pipeline\n",
        ">from sklearn.impute import SimpleImputer\n",
        ">from sklearn.preprocessing import StandardScaler\n",
        ">\n",
        ">numeric_pipeline = Pipeline([\n",
        ">    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        ">    (\"scaler\", StandardScaler()),\n",
        ">])\n",
        ">```\n",
        "---\n",
        "### Q2.2. Build `numeric_pipeline` using:\n",
        "- median imputation  \n",
        "- standardization with `StandardScaler()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8089d4a",
      "metadata": {
        "id": "b8089d4a"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccf492ed",
      "metadata": {
        "id": "ccf492ed"
      },
      "source": [
        ">### Build the preprocessing pipeline for categorical features\n",
        ">\n",
        ">For categorical columns, a common preprocessing flow is:\n",
        ">\n",
        ">1. **Impute missing values** with the most frequent category.\n",
        ">2. **Encode categories** using one-hot encoding.\n",
        ">\n",
        ">We also ask the encoder to return **dense output** so that the final result can easily be converted to a pandas DataFrame.\n",
        ">\n",
        ">```python\n",
        ">from sklearn.preprocessing import OneHotEncoder\n",
        ">\n",
        ">categorical_pipeline = Pipeline([\n",
        ">    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        ">    (\"encoder\", OneHotEncoder(\n",
        ">        drop=\"first\",\n",
        ">        handle_unknown=\"ignore\",\n",
        ">        sparse_output=False,\n",
        ">    )),\n",
        ">])\n",
        ">```\n",
        ">- `handle_unknown='ignore'`: ensures that if new categories appear in future data, the encoder will not raise an error but will instead create all-zero columns for those unseen categories.\n",
        ">\n",
        ">- `sparse_output=False`: makes sure the output is a dense array, which is easier to convert to a DataFrame.\n",
        ">\n",
        ">   **Example of Dense Output:**\n",
        ">```console\n",
        ">                education_Bachelors  education_Masters  education_PhD  education_Some-college\n",
        ">           0    1                    0                  0              0\n",
        ">           1    0                    0                  0              1\n",
        ">           2    0                    1                  0              0\n",
        ">```\n",
        ">   - Dense arrays are easier to convert to pandas DataFrames, otherwise you get sparse matrix representations that are less intuitive to work with as the example below:\n",
        ">\n",
        ">       **Example of Sparse Output:**\n",
        ">```console\n",
        ">           (0, 2)\t1\n",
        ">           (1, 0)\t1\n",
        ">           (2, 1)\t1\n",
        ">```\n",
        ">\n",
        ">   - Each row shows:\n",
        ">\n",
        ">       - the **position** `(row_index, column_index)`\n",
        ">       - the **value** stored at that position  \n",
        ">       - all other positions not shown are implicitly **zeros**.\n",
        "---\n",
        "### Q2.3. Build the `categorical_pipeline` using:\n",
        "- imputation with `most_frequent`\n",
        "- one-hot encoding (drop first, ignore unknowns, dense output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "870295fb",
      "metadata": {
        "id": "870295fb"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac95e567",
      "metadata": {
        "id": "ac95e567"
      },
      "source": [
        ">---\n",
        ">### Custom Cleaning Stage\n",
        ">\n",
        ">At this point, we have already built:\n",
        ">\n",
        ">- the **numeric preprocessing pipeline**\n",
        ">- the **categorical preprocessing pipeline**\n",
        ">\n",
        ">However, before sending the features into the encoder,  \n",
        ">we must ensure that some columns â€” specifically those in the Adult Income dataset  \n",
        ">that contain formatting inconsistencies â€” are **cleaned first**.\n",
        ">\n",
        ">These issues include:\n",
        ">\n",
        ">- values ending in `\".\"` (e.g., `\">50K.\"`, `\"United-States.\"`)\n",
        ">- values containing `\"?\"` instead of real missing values\n",
        ">- extra spaces, uppercase/lowercase inconsistencies\n",
        ">\n",
        ">This is exactly what we manually fixed in `Week 5`.  \n",
        ">Now we reproduce this using a **custom preprocessing function**.\n",
        ">\n",
        ">---\n",
        ">**Example: Define a custom cleaning function**\n",
        ">\n",
        ">```python\n",
        ">def clean_categorical_values(df):\n",
        ">    \"\"\"\n",
        ">    Applies simple normalization on all object columns:\n",
        ">    - strip whitespace\n",
        ">    - remove trailing periods\n",
        ">    - replace '?' with ''\n",
        ">    - lowercase everything\n",
        ">    \"\"\"\n",
        ">    df = df.copy()\n",
        ">    for col in df.select_dtypes(include=[\"object\"]):\n",
        ">        df[col] = df[col].str.strip()\n",
        ">        df[col] = df[col].str.replace(\".\", \"\", regex=False)\n",
        ">        df[col] = df[col].str.replace(\"?\", \"\", regex=False)\n",
        ">        df[col] = df[col].str.lower()\n",
        ">    return df\n",
        ">```\n",
        ">---\n",
        "### Q2.4. Run the following function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e01f588e",
      "metadata": {
        "id": "e01f588e"
      },
      "outputs": [],
      "source": [
        "def clean_categorical_values(df):\n",
        "    \"\"\"\n",
        "    Applies simple normalization on all object columns:\n",
        "    - strip whitespace\n",
        "    - remove trailing periods\n",
        "    - replace '?' with ''\n",
        "    - lowercase everything\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    for col in df.select_dtypes(include=[\"object\"]):\n",
        "        df[col] = df[col].str.strip()\n",
        "        df[col] = df[col].str.replace(\".\", \"\", regex=False)\n",
        "        df[col] = df[col].str.replace(\"?\", \"\", regex=False)\n",
        "        df[col] = df[col].str.lower()\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "929586fb",
      "metadata": {
        "id": "929586fb"
      },
      "source": [
        "### Q2.5. Identify which columns must pass through the custom cleaning step\n",
        ">\n",
        ">Inspect the categorical variables and create a list named **`custom_features`**  \n",
        ">containing only the columns that have:\n",
        ">\n",
        ">- `\".\"`  \n",
        ">- `\"?\"`  \n",
        ">- leading/trailing spaces  \n",
        ">- inconsistent labels  \n",
        ">\n",
        ">Examples based on Week 5:\n",
        ">\n",
        ">- `\"income\"`  \n",
        ">- `\"native-country\"`  \n",
        ">- `\"occupation\"`  \n",
        ">- `\"workclass\"`  \n",
        ">\n",
        ">**Note**: Make sure that the columns you have selected to pass through this pipeline are not present in the `categorical_features` list anymore. Otherwise they will pass trhough both pipelines. Use `categorical_features.remove('col')` to remove the item from the list.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "100d241e",
      "metadata": {
        "id": "100d241e"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e8f8b65",
      "metadata": {
        "id": "1e8f8b65"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n"
      ],
      "metadata": {
        "id": "tFcGBkfrYpWT"
      },
      "id": "tFcGBkfrYpWT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f12e9d03",
      "metadata": {
        "id": "f12e9d03"
      },
      "source": [
        ">### Q2.6. Wrap your function using `FunctionTransformer`\n",
        ">\n",
        ">```python\n",
        ">from sklearn.preprocessing import FunctionTransformer\n",
        ">\n",
        ">custom_cleaner = FunctionTransformer(clean_categorical_values)\n",
        ">```\n",
        ">---\n",
        ">### Build a pipeline that applies ONLY the custom cleaning\n",
        ">\n",
        ">This pipeline will operate **before** the normal preprocessing.\n",
        ">\n",
        ">```python\n",
        ">custom_pipeline = Pipeline([\n",
        ">    (\"custom_pipeline\", custom_cleaner)\n",
        ">])\n",
        ">```\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adf2d2d6",
      "metadata": {
        "id": "adf2d2d6"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa08943c",
      "metadata": {
        "id": "aa08943c"
      },
      "source": [
        ">### Combine custom, numerical and categorical pipelines with `ColumnTransformer`\n",
        ">\n",
        ">Now we combine both pipelines in a single object that knows:\n",
        ">- which columns need custom processing and use `custom_pipeline`\n",
        ">- which columns are numerical and use `numeric_pipeline`.\n",
        ">- which columns are categorical and use `categorical_pipeline`.\n",
        ">\n",
        ">We also ask the transformer to return a **pandas DataFrame** instead of a NumPy array.\n",
        ">\n",
        ">```python\n",
        ">from sklearn.compose import ColumnTransformer\n",
        ">\n",
        ">full_preprocessor = ColumnTransformer([\n",
        ">    (\"cus\", custom_pipeline, custom_features),\n",
        ">    (\"num\", numeric_pipeline, numeric_features),\n",
        ">    (\"cat\", categorical_pipeline, categorical_features),\n",
        ">])\n",
        ">\n",
        ">full_preprocessor.set_output(transform=\"pandas\")\n",
        ">```\n",
        ">- `full_preprocessor.set_output(transform=\"pandas\")`: works around the default behavior of ColumnTransformer, which is to return a NumPy array.\n",
        "---\n",
        "### Q2.7. Create the `full_preprocessor` using ColumnTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdb49564",
      "metadata": {
        "id": "bdb49564"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ed9d73d",
      "metadata": {
        "id": "6ed9d73d"
      },
      "source": [
        ">### Visualizing the Preprocessing Pipeline (Diagram)\n",
        ">\n",
        ">Scikit-learn allows us to **visualize the entire preprocessing pipeline** using  \n",
        ">`set_output(transform=\"pandas\")` **(already done above) together with  \n",
        ">`sklearn.utils.estimator_html_repr`.**\n",
        ">\n",
        ">This creates an interactive HTML diagram that shows:\n",
        ">\n",
        ">- each step in the pipeline,\n",
        ">- how data flows through numerical and categorical branches,\n",
        ">- how transformations are combined in the `ColumnTransformer`,\n",
        ">- and the final output.\n",
        ">\n",
        ">This is extremely useful for understanding the structure of your preprocessing workflow.\n",
        ">\n",
        ">```python\n",
        ">    from sklearn.utils import estimator_html_repr\n",
        ">    from IPython.display import display, HTML\n",
        ">\n",
        ">    HTML(estimator_html_repr(full_preprocessor))\n",
        ">```\n",
        ">**Example Output**:\n",
        ">\n",
        ">![Pipeline Diagram](https://github.com/tgvp/PACD/blob/main/img/pipeline.png?raw=1)\n",
        "---\n",
        "### Q2.8. Display your pipeline diagram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7099c564",
      "metadata": {
        "id": "7099c564"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f90b3e7",
      "metadata": {
        "id": "1f90b3e7"
      },
      "source": [
        ">### Apply the preprocessing pipeline to the whole dataset\n",
        ">\n",
        ">With a **single call** we now apply:\n",
        ">\n",
        ">- imputation of missing values\n",
        ">- scaling of numerical features\n",
        ">- one-hot encoding of categorical features\n",
        ">\n",
        ">and obtain a fully processed DataFrame.\n",
        ">\n",
        ">```python\n",
        ">df_clean = full_preprocessor.fit_transform(df)\n",
        ">df_clean.head()\n",
        ">```\n",
        "---\n",
        "### Q2.9. Apply the pipeline and create `df_clean`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bdb8e7a",
      "metadata": {
        "id": "2bdb8e7a"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "681ea6fb",
      "metadata": {
        "id": "681ea6fb"
      },
      "source": [
        ">### Export the cleaned dataset produced by the pipeline\n",
        ">\n",
        ">Finally, save the preprocessed dataset for future use (for example, in a Machine Learning course).\n",
        ">\n",
        ">```python\n",
        ">df_clean.to_csv(\"clean_dataset.csv\", index=False)\n",
        ">```\n",
        "---\n",
        "### Q2.10. Save the cleaned dataset as `clean_dataset.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b057d581",
      "metadata": {
        "id": "b057d581"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa906e3e",
      "metadata": {
        "id": "fa906e3e"
      },
      "source": [
        "### Concluding\n",
        ">From now on, instead of repeating all cleaning steps manually, you can:\n",
        ">\n",
        ">- reuse `full_preprocessor` on new data with `full_preprocessor.transform(new_df)`;\n",
        ">- keep all preprocessing logic **centralized and reproducible** in a single object.\n",
        ">\n",
        "> **Note 1**: You could also **improve** the pipeline by **reusing the group-based imputation** we have applied in **Week 5** and create a different pipeline.\n",
        ">\n",
        "> **Note 2**: You could also be more meticulous and remember **when you should apply One-Hot-Encoding** and **when not to apply**. Also ask yourself if it makes sense to apply it in `native-country` column for example.\n",
        ">\n",
        "> **Note 3**: Try to **chain pipelines** when it makes sense.\n",
        ">\n",
        "> **Example**:\n",
        ">```python\n",
        ">cat_preproc = Pipeline([\n",
        ">    (\"cus\", custom_pipeline),\n",
        ">    (\"cat\", categorical_pipeline)\n",
        ">])\n",
        ">\n",
        ">full_preprocessor = ColumnTransformer([\n",
        ">    (\"cat\", cat_preproc, categorical_features),\n",
        ">    (\"num\", numeric_pipeline, numeric_features),\n",
        ">])\n",
        ">\n",
        ">full_preprocessor.set_output(transform=\"pandas\")\n",
        ">```\n",
        ">\n",
        ">Verify the diagram of this alternative mkaing sure to include all features you want in the list.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f43a3e52",
      "metadata": {
        "id": "f43a3e52"
      },
      "source": [
        "## Let's practice a bit more!!!\n",
        "\n",
        "By now you already defined the preprocessing pipeline which could e reused in a differente dataset.\n",
        "\n",
        "### ðŸ“Œ **Bank Marketing Dataset (UCI Machine Learning Repository)**\n",
        "This dataset contains information about clients of a Portuguese bank and whether they subscribed to a term deposit.\n",
        "\n",
        "---\n",
        "\n",
        "**Dataset Reference:** ðŸ”— https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional-full.csv\n",
        "\n",
        "### Data Dictionary\n",
        "> | Variable Name | Role     | Type         | Demographic      | Description                                                                                                                                                                                                                       | Units | Missing Values |\n",
        "> |---------------|----------|--------------|------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------|----------------|\n",
        "> | age           | Feature  | Integer      | Age              | Client age                                                                                                                                                                                                                        |       | no             |\n",
        "> | job           | Feature  | Categorical  | Occupation       | Type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')                                                      |       | no             |\n",
        "> | marital       | Feature  | Categorical  | Marital Status   | Marital status (categorical: 'divorced','married','single','unknown'; *note*: \"divorced\" includes widowed)                                                                                                                       |       | no             |\n",
        "> | education     | Feature  | Categorical  | Education Level  | Education level (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')                                                                                   |       | no             |\n",
        "> | default       | Feature  | Binary       |                  | Has credit in default?                                                                                                                                                                                                            |       | no             |\n",
        "> | balance       | Feature  | Integer      |                  | Average yearly balance                                                                                                                                                                                                            | euros | no             |\n",
        "> | housing       | Feature  | Binary       |                  | Has housing loan?                                                                                                                                                                                                                 |       | no             |\n",
        "> | loan          | Feature  | Binary       |                  | Has personal loan?                                                                                                                                                                                                                |       | no             |\n",
        "> | contact       | Feature  | Categorical  |                  | Contact communication type (categorical: 'cellular','telephone')                                                                                                                                                                  |       | yes            |\n",
        "> | day_of_week   | Feature  | Date         |                  | Last contact day of the week                                                                                                                                                                                                      |       | no             |\n",
        "> | month         | Feature  | Date         |                  | Last contact month (categorical: 'jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec')                                                                                                                         |       | no             |\n",
        "> | duration      | Feature  | Integer      |                  | Last contact duration (seconds). **Important:** heavily affects target. Should be excluded from realistic predictive models. Included only for benchmark comparisons.                                                             | sec   | no             |\n",
        "> | campaign      | Feature  | Integer      |                  | Number of contacts performed during this campaign (includes last contact)                                                                                                                                                         |       | no             |\n",
        "> | pdays         | Feature  | Integer      |                  | Days since last contact from previous campaign (-1 means never contacted)                                                                                                                                                         |       | yes            |\n",
        "> | previous      | Feature  | Integer      |                  | Number of contacts performed before this campaign                                                                                                                                                                                 |       | no             |\n",
        "> | poutcome      | Feature  | Categorical  |                  | Outcome of previous marketing campaign (categorical: 'failure','nonexistent','success')                                                                                                                                           |       | yes            |\n",
        "> | y             | Target   | Binary       |                  | Has the client subscribed a term deposit?                                                                                                                                                                                         |       | no             |\n",
        "---\n",
        "### Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cada850",
      "metadata": {
        "id": "1cada850"
      },
      "outputs": [],
      "source": [
        "# fetch dataset\n",
        "bank_marketing = fetch_ucirepo(id=222)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = bank_marketing.data.features\n",
        "y = bank_marketing.data.targets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e056f23",
      "metadata": {
        "id": "1e056f23"
      },
      "source": [
        "### Now you will:\n",
        "\n",
        "1. **Import the dataset**\n",
        "2. **Inspect its structure**\n",
        "3. **Identify numerical and categorical columns**\n",
        "4. **Identify columns that will require custom processing**\n",
        "5. **Apply the preprocessing pipeline already built**\n",
        "6. **Export the cleaned dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94b85bfe",
      "metadata": {
        "id": "94b85bfe"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7dc34ca",
      "metadata": {
        "id": "f7dc34ca"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc320087",
      "metadata": {
        "id": "dc320087"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87356577",
      "metadata": {
        "id": "87356577"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf4cdb2a",
      "metadata": {
        "id": "bf4cdb2a"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afc4cae3",
      "metadata": {
        "id": "afc4cae3"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b1581d2",
      "metadata": {
        "id": "2b1581d2"
      },
      "outputs": [],
      "source": [
        "# your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "322f4c85",
      "metadata": {
        "id": "322f4c85"
      },
      "outputs": [],
      "source": [
        "# your tears here ðŸ˜Š"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}